{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "#############################################\n",
    "# 1. ë‘ LoRA weight í–‰ë ¬(Wâº, Wâ») ê²°í•© í•¨ìˆ˜ ì •ì˜\n",
    "#############################################\n",
    "def combine_lora_weights(W_plus: np.ndarray, W_minus: np.ndarray, threshold: float = 0.9) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ë‘ LoRA weight í–‰ë ¬ W_plusì™€ W_minusë¥¼ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ weight W_newë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    (W_new = W_plus + (W_minus - P W_minus), PëŠ” ë‘ í–‰ë ¬ì˜ ê³µí†µ subspaceì— ëŒ€í•œ projection)\n",
    "    \n",
    "    Args:\n",
    "        W_plus (np.ndarray): alpaca-gpt4ë¡œ íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì˜ LoRA weight (Wâº)\n",
    "        W_minus (np.ndarray): toxic ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì˜ LoRA weight (Wâ»)\n",
    "        threshold (float): ê³µí†µ subspaceë¥¼ ì„ íƒí•  singular value threshold\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: ê²°í•©ëœ ìƒˆë¡œìš´ weight W_new.\n",
    "    \"\"\"\n",
    "    r = 4  # LoRAì˜ rank (ë‘ ëª¨ë¸ ëª¨ë‘ r=4ë¡œ ê°€ì •)\n",
    "    lamb = 1.0\n",
    "    # W_plusì— ëŒ€í•œ SVD: W_plus = U_plus * S_plus * Vh_plus\n",
    "    U_plus, S_plus, Vh_plus = np.linalg.svd(W_plus, full_matrices=False)\n",
    "    U_plus = U_plus[:, :r]  # (out_features x r)\n",
    "    \n",
    "    # W_minusì— ëŒ€í•œ SVD: W_minus = U_minus * S_minus * Vh_minus\n",
    "    U_minus, S_minus, Vh_minus = np.linalg.svd(W_minus, full_matrices=False)\n",
    "    U_minus = U_minus[:, :r]  # (out_features x r)\n",
    "    \n",
    "    # U_plusì™€ U_minusì˜ ê³µí†µ subspace ì°¾ê¸°:\n",
    "    X = np.dot(U_plus.T, U_minus)   # (r x r)\n",
    "    U_m, singular_values, Vh_m = np.linalg.svd(X)\n",
    "    \n",
    "    # singular valueê°€ threshold ì´ìƒì¸ ë°©í–¥ ì„ íƒ\n",
    "    common_indices = np.where(singular_values >= threshold)[0]\n",
    "    if common_indices.size == 0:\n",
    "        print(\"Threshold ì´ìƒì˜ ê³µí†µ subspaceê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  r ë°©í–¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        common_indices = np.arange(r)\n",
    "        \n",
    "    # U_common: U_plusì˜ ì„ í˜• ì¡°í•©ìœ¼ë¡œ ê³µí†µ basis êµ¬í•˜ê¸°\n",
    "    U_common = np.dot(U_plus, U_m[:, common_indices])  # (out_features x k), k <= r\n",
    "    P = np.dot(U_common, U_common.T)  # Projection matrix onto common subspace\n",
    "    \n",
    "    # W_common: W_minusì˜ ê³µí†µ ì„±ë¶„\n",
    "    W_common = np.dot(P, W_minus)\n",
    "    \n",
    "    # ìµœì¢…ì ìœ¼ë¡œ ê²°í•©í•œ weight\n",
    "    W_new = W_plus - lamb * (W_minus - lamb * W_common)\n",
    "    return W_new\n",
    "\n",
    "#############################################\n",
    "# 2. W_newë¥¼ LoRAì˜ ë‘ í–‰ë ¬(lora_B, lora_A)ë¡œ ë¶„í•´í•˜ëŠ” í•¨ìˆ˜\n",
    "#############################################\n",
    "def factorize_weight(W_new: np.ndarray, r: int, scaling: float):\n",
    "    \"\"\"\n",
    "    W_new(íš¨ê³¼ì ì¸ LoRA ì—…ë°ì´íŠ¸)ë¥¼ lora_Bì™€ lora_Aë¡œ ë¶„í•´í•©ë‹ˆë‹¤.\n",
    "    LoRA ì—…ë°ì´íŠ¸ëŠ” ì›ë˜ (lora_B @ lora_A) * scaling í˜•íƒœë¡œ ì ìš©ë˜ë¯€ë¡œ,\n",
    "    lora_B @ lora_A = W_new / scalingê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    SVDë¥¼ í†µí•´ M = W_new/scaling = U S V^T ë¡œ ë¶„í•´í•œ í›„,\n",
    "    lora_B = U * sqrt(S)   (shape: [out_features, r])\n",
    "    lora_A = sqrt(S) * V^T   (shape: [r, in_features])\n",
    "    \n",
    "    Args:\n",
    "        W_new (np.ndarray): ê²°í•©ëœ effective weight update (out_features x in_features)\n",
    "        r (int): LoRAì˜ rank (ì˜ˆì œì—ì„œëŠ” 4)\n",
    "        scaling (float): lora_alpha / r (ì˜ˆ: 32/4 = 8)\n",
    "        \n",
    "    Returns:\n",
    "        lora_B, lora_A: torch.Tensorë¡œ ë³€í™˜ëœ ë¶„í•´ ê²°ê³¼.\n",
    "    \"\"\"\n",
    "    M = W_new / scaling  # (lora_B @ lora_A = M)\n",
    "    U, S, Vh = np.linalg.svd(M, full_matrices=False)\n",
    "    \n",
    "    U_r = U[:, :r]      # (out_features x r)\n",
    "    S_r = S[:r]         # (r,)\n",
    "    Vh_r = Vh[:r, :]    # (r x in_features)\n",
    "    \n",
    "    sqrt_S = np.sqrt(S_r)\n",
    "    lora_B = U_r * sqrt_S[np.newaxis, :]   # broadcasting, shape: (out_features x r)\n",
    "    lora_A = sqrt_S[:, np.newaxis] * Vh_r    # shape: (r x in_features)\n",
    "    \n",
    "    # torch tensorë¡œ ë³€í™˜ (dtypeì€ ëª¨ë¸ê³¼ ì¼ì¹˜í•˜ë„ë¡)\n",
    "    lora_B = torch.tensor(lora_B, dtype=torch.float16)\n",
    "    lora_A = torch.tensor(lora_A, dtype=torch.float16)\n",
    "    return lora_B, lora_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################\n",
    "# 3. Qwen ëª¨ë¸ ë¡œë“œ ë° LoRA ì ìš© (PEFT ë°©ì‹)\n",
    "#############################################\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# LoRA ì„¤ì • (ì›ë˜ íŒŒì¸íŠœë‹ì— ì‚¬ìš©í–ˆë˜ target module ëª©ë¡)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\", \n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\", \n",
    "        \"mlp.down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# PEFTë¥¼ í†µí•´ ëª¨ë¸ì— LoRA ì–´ëŒ‘í„° ì¶”ê°€\n",
    "peft_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒˆë¡œìš´ LoRA ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ìƒˆë¡œìš´ ê²°í•©ëœ LoRA ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ./qwen-0.5b-unlearned-lora\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "\n",
    "# ğŸŸ¢ Fine-tuned LoRA ëª¨ë¸ ê²½ë¡œ\n",
    "path_plus = \"./qwen-0.5b-lora-finetuned-alpaca-gpt4\"   # W+\n",
    "path_minus = \"./qwen-0.5b-lora-finetuned-toxic\"         # W-\n",
    "\n",
    "# ì›ë³¸ Qwen ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Fine-tuned LoRA ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model_plus = PeftModel.from_pretrained(base_model, path_plus)\n",
    "model_minus = PeftModel.from_pretrained(base_model, path_minus)\n",
    "\n",
    "# ëª¨ë¸ì˜ state_dict ê°€ì ¸ì˜¤ê¸°\n",
    "state_dict_plus = model_plus.state_dict()\n",
    "state_dict_minus = model_minus.state_dict()\n",
    "\n",
    "# ğŸŸ¢ LoRA Target Modules\n",
    "target_modules = [\n",
    "    \"self_attn.q_proj\",\n",
    "    \"self_attn.k_proj\", \n",
    "    \"self_attn.v_proj\",\n",
    "    \"self_attn.o_proj\",\n",
    "    \"mlp.gate_proj\",\n",
    "    \"mlp.up_proj\", \n",
    "    \"mlp.down_proj\"\n",
    "]\n",
    "\n",
    "# LoRA ì„¤ì • (ìƒˆë¡œìš´ ëª¨ë¸ì— ì ìš©)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "# ìƒˆë¡œìš´ PEFT ëª¨ë¸ ìƒì„±\n",
    "new_peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# LoRA scaling factor\n",
    "scaling = lora_config.lora_alpha / lora_config.r  # ì˜ˆ: 32/4 = 8\n",
    "\n",
    "# ğŸŸ¢ LoRA weightë¥¼ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ W_new ìƒì„±\n",
    "for layer_idx in range(24):  # Qwen-0.5BëŠ” 24ê°œì˜ Transformer layerë¥¼ ê°€ì§\n",
    "    for target_module in target_modules:\n",
    "        # LoRA weight í‚¤ ìƒì„± (ê° ë ˆì´ì–´ì— ëŒ€í•´)\n",
    "        key_A = f\"base_model.model.model.layers.{layer_idx}.{target_module}.lora_A.default.weight\"\n",
    "        key_B = f\"base_model.model.model.layers.{layer_idx}.{target_module}.lora_B.default.weight\"\n",
    "\n",
    "        # í‚¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸ ìˆ˜í–‰\n",
    "        if key_A in state_dict_plus and key_B in state_dict_plus:\n",
    "            # ê¸°ì¡´ W+ì™€ W- ë¶ˆëŸ¬ì˜¤ê¸° (torch.Tensor â†’ numpy ë³€í™˜)\n",
    "            W_plus = (state_dict_plus[key_B] @ state_dict_plus[key_A]).cpu().numpy()\n",
    "            W_minus = (state_dict_minus[key_B] @ state_dict_minus[key_A]).cpu().numpy()\n",
    "\n",
    "            # W_new ìƒì„±\n",
    "            W_new = combine_lora_weights(W_plus, W_minus)\n",
    "\n",
    "            # W_newë¥¼ SVD ë¶„í•´í•˜ì—¬ lora_A, lora_Bë¡œ ë³µêµ¬\n",
    "            lora_B, lora_A = factorize_weight(W_new, r=lora_config.r, scaling=scaling)\n",
    "\n",
    "            # ìƒˆë¡œìš´ ëª¨ë¸ì˜ LoRA weight ì—…ë°ì´íŠ¸ (torch.Tensor í˜•íƒœë¡œ ë³€í™˜)\n",
    "            with torch.no_grad():\n",
    "                state_dict_plus[key_A].copy_(lora_A.to(state_dict_plus[key_A].dtype))\n",
    "                state_dict_plus[key_B].copy_(lora_B.to(state_dict_plus[key_B].dtype))\n",
    "\n",
    "print(\"ìƒˆë¡œìš´ LoRA ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# ğŸŸ¢ ìƒˆë¡œìš´ LoRA ëª¨ë¸ ì €ì¥\n",
    "save_path = \"./qwen-0.5b-unlearned-lora\"\n",
    "new_peft_model.save_pretrained(save_path)\n",
    "print(f\"ìƒˆë¡œìš´ ê²°í•©ëœ LoRA ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
