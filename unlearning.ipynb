{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyudan/anaconda3/envs/unlearning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from scipy.linalg import svd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from peft import PeftModel, LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "import numpy as np\n",
    "class LoRAUnlearner:\n",
    "    def __init__(self, alpha=0.5, beta=0.5, lambda_reg=0.1, interp_coef=0.95):\n",
    "        \"\"\"\n",
    "        alpha: ê³µí†µ ì„±ë¶„ ì œê±° ì‹œ ì¶”ê°€ ë³´ì • ê³„ìˆ˜ (í˜„ì¬ ë¯¸ì‚¬ìš©)\n",
    "        beta: unlearning ê°•ë„ (Wâ»ì—ì„œ ë¹¼ì¤„ ì •ë„)\n",
    "        lambda_reg: ì •ê·œí™” ê³„ìˆ˜ (ê³¼ë„í•œ ë³€ê²½ ì–µì œ)\n",
    "        interp_coef: ìµœì¢…ì ìœ¼ë¡œ ì›ë³¸ Wâºì™€ í˜¼í•©í•  ë¹„ìœ¨ (0~1, 1ì´ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ)\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.interp_coef = interp_coef\n",
    "\n",
    "    def svd_decompose(self, W):\n",
    "        \"\"\"SVD ë¶„í•´ ìˆ˜í–‰\"\"\"\n",
    "        U, Sigma, Vt = np.linalg.svd(W, full_matrices=False)\n",
    "        return U, np.diag(Sigma), Vt\n",
    "\n",
    "    def extract_common_weights(self, W_plus, W_minus):\n",
    "        \"\"\"Wâºì™€ Wâ» ì‚¬ì´ì˜ ê³µí†µ ì„±ë¶„ ì¶”ì¶œ\"\"\"\n",
    "        W_cross = np.dot(W_plus, W_minus.T)\n",
    "        U_c, _, _ = self.svd_decompose(W_cross)\n",
    "        W_common = np.dot(U_c, np.dot(U_c.T, (W_plus + W_minus) / 2))\n",
    "        return W_common\n",
    "\n",
    "    def remove_common_weights(self, W_minus, W_common):\n",
    "        \"\"\"Wâ»ì—ì„œ ê³µí†µ ì„±ë¶„ ì œê±°\"\"\"\n",
    "        # ê³µí†µ ì„±ë¶„ì— ëŒ€í•œ projection ê³„ì‚° (ì—­í–‰ë ¬ ì•ˆì •ì„±ì„ ìœ„í•´ ì‘ì€ ê°’ ì¶”ê°€)\n",
    "        projection = np.dot(W_common, np.linalg.inv(np.dot(W_common.T, W_common) + 1e-8 * np.eye(W_common.shape[1])))\n",
    "        projection = np.dot(projection, np.dot(W_common.T, W_minus))\n",
    "        W_minus_pure = W_minus - projection\n",
    "        return W_minus_pure\n",
    "\n",
    "    def compute_fisher_matrix(self, W_plus):\n",
    "        \"\"\"\n",
    "        ê°„ë‹¨í•œ ëŒ€ê° ê·¼ì‚¬(Fisher information)ì˜ surrogateë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "        ê° í–‰ì˜ í‰ê·  ì œê³±ê°’ì„ ê³„ì‚°í•˜ì—¬ ëŒ€ê°í–‰ë ¬ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        (ì‹¤ì œ ê³„ì‚° ì‹œì—ëŠ” ë°ì´í„°ì™€ ì†ì‹¤ í•¨ìˆ˜ì— ê¸°ë°˜í•œ gradient ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.)\n",
    "        \"\"\"\n",
    "        # W_plus: (out_features, in_features)\n",
    "        diag_elements = np.mean(W_plus**2, axis=1)  # ê° í–‰ì˜ í‰ê·  ì œê³±\n",
    "        fisher = np.diag(diag_elements)  # (out_features x out_features)\n",
    "        return fisher\n",
    "\n",
    "    def adjust_weights(self, W_minus_pure, fisher_matrix):\n",
    "        \"\"\"Fisher ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë³´ì • (Fisher matrixëŠ” ëŒ€ê° ê·¼ì‚¬ ì‚¬ìš©)\"\"\"\n",
    "        fisher_inv = np.linalg.inv(fisher_matrix + 1e-8 * np.eye(fisher_matrix.shape[0]))\n",
    "        # ì¤‘ìš” íŒŒë¼ë¯¸í„°ì˜ ë³€í™”ê°€ ì¶•ì†Œë˜ë„ë¡ fisher_invë¥¼ ê³±í•¨\n",
    "        W_minus_adjusted = np.dot(fisher_inv, W_minus_pure)\n",
    "        return W_minus_adjusted\n",
    "\n",
    "    def unlearn_weights(self, W_plus, W_minus):\n",
    "        \"\"\"\n",
    "        ê°œì„ ëœ unlearning ì ˆì°¨:\n",
    "          1. ê³µí†µ ì„±ë¶„ ì¶”ì¶œ ë° ì œê±°\n",
    "          2. Fisher ì •ë³´ë¥¼ í†µí•œ ë³´ì • (ë‚´ë¶€ì—ì„œ ìë™ ê³„ì‚°)\n",
    "          3. Wâºì™€ Wâ» ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ë¶€ë“œëŸ½ê²Œ ë³´ì •í•˜ê³ , ì •ê·œí™” ë° ì›ë³¸ê³¼ ë³´ê°„\n",
    "        \"\"\"\n",
    "        # 1. ê³µí†µ ì„±ë¶„ ì¶”ì¶œ ë° ì œê±°\n",
    "        W_common = self.extract_common_weights(W_plus, W_minus)\n",
    "        W_minus_pure = self.remove_common_weights(W_minus, W_common)\n",
    "        \n",
    "        # 2. Fisher matrix ê³„ì‚° ë° ì ìš©\n",
    "        fisher_matrix = self.compute_fisher_matrix(W_plus)\n",
    "        W_minus_adjusted = self.adjust_weights(W_minus_pure, fisher_matrix)\n",
    "\n",
    "        # 3. Wâºì™€ Wâ»ì˜ ì°¨ì´ì— ê¸°ì´ˆí•´ ì ì§„ì  unlearning ì ìš©\n",
    "        delta = self.beta * (W_plus - W_minus_adjusted)\n",
    "        W_unlearned = W_plus - delta\n",
    "\n",
    "        # ì •ê·œí™” í•­: ê³¼ë„í•œ ë³€ê²½ ì–µì œ (Wâºì— ê°€ê¹ê²Œ ìœ ì§€)\n",
    "        W_unlearned = (1 - self.lambda_reg) * W_unlearned + self.lambda_reg * W_plus\n",
    "\n",
    "        # ìµœì¢… ë³´ê°„: ì›ë˜ Wâº ì •ë³´ì™€ í˜¼í•©\n",
    "        W_unlearned = self.interp_coef * W_plus + (1 - self.interp_coef) * W_unlearned\n",
    "\n",
    "        # ê°’ ë²”ìœ„ í´ë¦¬í•‘ (ì˜ˆ: -1ê³¼ 1 ì‚¬ì´)\n",
    "        W_unlearned = np.clip(W_unlearned, -1, 1)\n",
    "        \n",
    "        return W_unlearned\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 2. W_newë¥¼ LoRAì˜ ë‘ í–‰ë ¬(lora_B, lora_A)ë¡œ ë¶„í•´í•˜ëŠ” í•¨ìˆ˜\n",
    "#############################################\n",
    "def factorize_weight(W_new: np.ndarray, r: int, scaling: float):\n",
    "    \"\"\"\n",
    "    W_new(íš¨ê³¼ì ì¸ LoRA ì—…ë°ì´íŠ¸)ë¥¼ lora_Bì™€ lora_Aë¡œ ë¶„í•´í•©ë‹ˆë‹¤.\n",
    "    LoRA ì—…ë°ì´íŠ¸ëŠ” ì›ë˜ (lora_B @ lora_A) * scaling í˜•íƒœë¡œ ì ìš©ë˜ë¯€ë¡œ,\n",
    "    lora_B @ lora_A = W_new / scalingê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    SVDë¥¼ í†µí•´ M = W_new/scaling = U S V^T ë¡œ ë¶„í•´í•œ í›„,\n",
    "    lora_B = U * sqrt(S)   (shape: [out_features, r])\n",
    "    lora_A = sqrt(S) * V^T   (shape: [r, in_features])\n",
    "    \n",
    "    Args:\n",
    "        W_new (np.ndarray): ê²°í•©ëœ effective weight update (out_features x in_features)\n",
    "        r (int): LoRAì˜ rank (ì˜ˆì œì—ì„œëŠ” 4)\n",
    "        scaling (float): lora_alpha / r (ì˜ˆ: 32/4 = 8)\n",
    "        \n",
    "    Returns:\n",
    "        lora_B, lora_A: torch.Tensorë¡œ ë³€í™˜ëœ ë¶„í•´ ê²°ê³¼.\n",
    "    \"\"\"\n",
    "    M = W_new / scaling  # (lora_B @ lora_A = M)\n",
    "    U, S, Vh = np.linalg.svd(M, full_matrices=False)\n",
    "    \n",
    "    U_r = U[:, :r]      # (out_features x r)\n",
    "    S_r = S[:r]         # (r,)\n",
    "    Vh_r = Vh[:r, :]    # (r x in_features)\n",
    "    \n",
    "    sqrt_S = np.sqrt(S_r)\n",
    "    lora_B = U_r * sqrt_S[np.newaxis, :]   # broadcasting, shape: (out_features x r)\n",
    "    lora_A = sqrt_S[:, np.newaxis] * Vh_r    # shape: (r x in_features)\n",
    "    \n",
    "    # torch tensorë¡œ ë³€í™˜ (dtypeì€ ëª¨ë¸ê³¼ ì¼ì¹˜í•˜ë„ë¡)\n",
    "    lora_B = torch.tensor(lora_B, dtype=torch.float16)\n",
    "    lora_A = torch.tensor(lora_A, dtype=torch.float16)\n",
    "    return lora_B, lora_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#############################################\n",
    "# 3. Qwen ëª¨ë¸ ë¡œë“œ ë° LoRA ì ìš© (PEFT ë°©ì‹)\n",
    "#############################################\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    #attn_implementation=\"eager\"\n",
    "\n",
    ")\n",
    "model.config.sliding_window = None\n",
    "\n",
    "# LoRA ì„¤ì • (ì›ë˜ íŒŒì¸íŠœë‹ì— ì‚¬ìš©í–ˆë˜ target module ëª©ë¡)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\", \n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\", \n",
    "        \"mlp.down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# PEFTë¥¼ í†µí•´ ëª¨ë¸ì— LoRA ì–´ëŒ‘í„° ì¶”ê°€\n",
    "peft_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "\n",
    "path_plus = \"./qwen-0.5b-lora-finetuned-alpaca-gpt4\"   # W+\n",
    "path_minus = \"./qwen-0.5b-lora-finetuned-toxic\"         # W-\n",
    "\n",
    "# ì›ë³¸ Qwen ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Fine-tuned LoRA ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model_plus = PeftModel.from_pretrained(base_model, path_plus)\n",
    "model_minus = PeftModel.from_pretrained(base_model, path_minus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ëª¨ë¸ì˜ state_dict ê°€ì ¸ì˜¤ê¸°\n",
    "state_dict_plus = model_plus.state_dict()\n",
    "state_dict_minus = model_minus.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plus.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 Î”W: 18.04%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 Î”W: 186.23%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 Î”W: 772.46%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 Î”W: 60.76%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 Î”W: 4.35%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 Î”W: 8.21%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 Î”W: 47.00%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 Î”W: 29.23%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 Î”W: 295.43%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 Î”W: 621.66%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 Î”W: 146.42%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 Î”W: 4.26%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 Î”W: 7.48%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 Î”W: 61.74%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 Î”W: 27.85%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 Î”W: 396.06%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 Î”W: 661.84%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 Î”W: 43.91%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 Î”W: 4.04%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 Î”W: 6.24%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 Î”W: 329.41%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 Î”W: 38.11%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 Î”W: 547.20%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 Î”W: 777.50%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 Î”W: 37.03%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 Î”W: 4.25%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 Î”W: 10.70%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 Î”W: 68.44%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 Î”W: 36.51%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 Î”W: 477.32%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 Î”W: 1968.25%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 Î”W: 34.29%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 Î”W: 4.50%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 Î”W: 6.88%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 Î”W: 81.12%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 Î”W: 41.28%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 Î”W: 513.81%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 Î”W: 1161.25%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 Î”W: 38.37%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 Î”W: 4.60%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 Î”W: 11.24%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 Î”W: 47.45%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 Î”W: 43.95%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 Î”W: 358.36%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 Î”W: 1541.98%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 Î”W: 32.75%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 Î”W: 5.44%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 Î”W: 11.04%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 Î”W: 34.64%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 Î”W: 40.62%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 Î”W: 566.91%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 Î”W: 998.71%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 Î”W: 29.40%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 Î”W: 3.82%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 Î”W: 9.46%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 Î”W: 51.03%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 Î”W: 41.89%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 Î”W: 730.16%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 Î”W: 924.46%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 Î”W: 43.21%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 Î”W: 4.38%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 Î”W: 7.16%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 Î”W: 67.42%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 Î”W: 39.57%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 Î”W: 456.96%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 Î”W: 917.20%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 Î”W: 37.42%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 Î”W: 4.58%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 Î”W: 5.87%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 Î”W: 47.09%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 Î”W: 43.99%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 Î”W: 337.82%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 Î”W: 782.94%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 Î”W: 46.92%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 Î”W: 4.40%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 Î”W: 5.53%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 Î”W: 53.88%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 Î”W: 23.06%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 Î”W: 271.46%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 Î”W: 890.99%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 Î”W: 36.40%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 Î”W: 3.31%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 Î”W: 7.42%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 Î”W: 44.39%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 Î”W: 51.46%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 Î”W: 410.65%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 Î”W: 689.17%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 Î”W: 34.72%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 Î”W: 6.14%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 Î”W: 7.61%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 Î”W: 38.66%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 Î”W: 39.20%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 Î”W: 329.48%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 Î”W: 779.66%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 Î”W: 29.80%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 Î”W: 4.19%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 Î”W: 6.51%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 Î”W: 46.03%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 Î”W: 50.02%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 Î”W: 425.10%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 Î”W: 587.99%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 Î”W: 40.95%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 Î”W: 5.04%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 Î”W: 5.61%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 Î”W: 29.97%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 Î”W: 32.15%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 Î”W: 324.18%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 Î”W: 452.29%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 Î”W: 34.67%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 Î”W: 2.43%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 Î”W: 5.67%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 Î”W: 47.01%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 Î”W: 34.29%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 Î”W: 468.16%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 Î”W: 2322.10%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 Î”W: 16.60%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 Î”W: 3.69%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 Î”W: 6.33%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 Î”W: 35.66%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 Î”W: 30.84%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 Î”W: 432.15%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 Î”W: 774.40%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 Î”W: 22.83%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 Î”W: 4.02%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 Î”W: 8.95%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 Î”W: 52.64%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 Î”W: 30.56%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 Î”W: 313.32%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 Î”W: 609.38%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 Î”W: 18.96%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 Î”W: 3.10%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 Î”W: 4.79%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 Î”W: 27.13%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 Î”W: 33.75%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 Î”W: 332.70%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 Î”W: 708.51%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 Î”W: 24.74%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 Î”W: 2.74%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 Î”W: 4.50%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 Î”W: 33.55%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 Î”W: 29.84%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 Î”W: 248.51%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 Î”W: 701.56%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 Î”W: 80.10%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 Î”W: 4.09%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 Î”W: 3.64%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 Î”W: 26.52%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 Î”W: 35.56%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 Î”W: 421.14%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 Î”W: 488.02%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 Î”W: 35.01%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 Î”W: 1.93%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 Î”W: 2.59%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 Î”W: 66.67%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 Î”W: 29.97%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 Î”W: 269.82%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 Î”W: 776.80%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 Î”W: 25.34%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 Î”W: 1.70%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 Î”W: 2.72%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 Î”W: 21.79%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 Î”W: 27.62%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 Î”W: 297.28%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 Î”W: 316.01%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 Î”W: 33.87%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 Î”W: 2.55%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 Î”W: 3.94%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 Î”W: 81.61%\n",
      "ìƒˆë¡œìš´ ê²°í•©ëœ LoRA ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ./qwen-0.5b-unlearned-lora\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# ğŸŸ¢ LoRA Target Modules\n",
    "target_modules = [\n",
    "    \"self_attn.q_proj\",\n",
    "    \"self_attn.k_proj\", \n",
    "    \"self_attn.v_proj\",\n",
    "    \"self_attn.o_proj\",\n",
    "    \"mlp.gate_proj\",\n",
    "    \"mlp.up_proj\", \n",
    "    \"mlp.down_proj\"\n",
    "]\n",
    "\n",
    "# LoRA ì„¤ì • (ìƒˆë¡œìš´ ëª¨ë¸ì— ì ìš©)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "# ìƒˆë¡œìš´ PEFT ëª¨ë¸ ìƒì„±\n",
    "new_peft_model = get_peft_model(base_model, lora_config)\n",
    "new_state_dict = new_peft_model.state_dict()\n",
    "\n",
    "# LoRA scaling factor\n",
    "scaling = lora_config.lora_alpha / lora_config.r  # ì˜ˆ: 32/4 = 8\n",
    "\n",
    "for layer_idx in range(24):  # Qwen-0.5BëŠ” 24ê°œì˜ Transformer layerë¥¼ ê°€ì§\n",
    "    for target_module in target_modules:\n",
    "        # LoRA weight í‚¤ ìƒì„± (ê° ë ˆì´ì–´ì— ëŒ€í•´)\n",
    "        key_A = f\"base_model.model.model.layers.{layer_idx}.{target_module}.lora_A.default.weight\"\n",
    "        key_B = f\"base_model.model.model.layers.{layer_idx}.{target_module}.lora_B.default.weight\"\n",
    "\n",
    "        # í‚¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸ ìˆ˜í–‰\n",
    "        if key_A in state_dict_plus and key_B in state_dict_plus:\n",
    "            print(f\"bringing W+ and W-{layer_idx}\")\n",
    "            # ê¸°ì¡´ W+ì™€ W- ë¶ˆëŸ¬ì˜¤ê¸° (torch.Tensor â†’ numpy ë³€í™˜)\n",
    "            W_plus = (state_dict_plus[key_B] @ state_dict_plus[key_A]).cpu().numpy()\n",
    "            W_minus = (state_dict_minus[key_B] @ state_dict_minus[key_A]).cpu().numpy()\n",
    "            print(\"combining lora weights\")\n",
    "            # W_new ìƒì„±\n",
    "            # rank_common < LoRA rank\n",
    "            unlearner = LoRAUnlearner(alpha=0.5, beta=0.1, lambda_reg=0.5, interp_coef=0.90)\n",
    "            W_new = unlearner.unlearn_weights(W_plus, W_minus)\n",
    "\n",
    "            delta = np.linalg.norm(W_new - W_plus) / np.linalg.norm(W_plus)\n",
    "            print(f\"Layer {layer_idx} Î”W: {delta:.2%}\")\n",
    "            # W_newë¥¼ lora_A, lora_Bë¡œ ë³µêµ¬ (í•¨ìˆ˜ í™•ì¸í•´ì•¼ë¨.)\n",
    "            lora_B, lora_A = factorize_weight(W_new, r=lora_config.r, scaling=scaling)\n",
    "\n",
    "            # ìƒˆë¡œìš´ ëª¨ë¸ì˜ LoRA weight ì—…ë°ì´íŠ¸ (torch.Tensor í˜•íƒœë¡œ ë³€í™˜) (ìŒ ì§„ì§œ?)\n",
    "            with torch.no_grad():\n",
    "                new_state_dict[key_A].copy_(lora_A.to(new_state_dict[key_A].dtype))\n",
    "                new_state_dict[key_B].copy_(lora_B.to(new_state_dict[key_B].dtype))\n",
    "                \n",
    "new_peft_model.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "\n",
    "save_path = \"./qwen-0.5b-unlearned-lora\"\n",
    "new_peft_model.save_pretrained(save_path)\n",
    "print(f\"ìƒˆë¡œìš´ ê²°í•©ëœ LoRA ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "i=114\n",
    "# JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open('output/toxic_eval_unlearned_2025-02-21.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "with open('output/toxic_eval_alpaca-gpt4_finetuned_results.json', 'r', encoding='utf-8') as file:\n",
    "    dataalpaca = json.load(file)\n",
    "\n",
    "print(\"unlearned\")\n",
    "print(data[i]['instruction'])\n",
    "print(data[i]['output'])\n",
    "\n",
    "print(\"alpaca\")\n",
    "print(dataalpaca[i]['instruction'])\n",
    "print(dataalpaca[i]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
