{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyudan/anaconda3/envs/unlearning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from scipy.linalg import svd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from peft import PeftModel, LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "import numpy as np\n",
    "class LoRAUnlearner:\n",
    "    def __init__(self, alpha=0.5, beta=0.5, lambda_reg=0.1, interp_coef=0.95):\n",
    "        \"\"\"\n",
    "        alpha: 공통 성분 제거 시 추가 보정 계수 (현재 미사용)\n",
    "        beta: unlearning 강도 (W⁻에서 빼줄 정도)\n",
    "        lambda_reg: 정규화 계수 (과도한 변경 억제)\n",
    "        interp_coef: 최종적으로 원본 W⁺와 혼합할 비율 (0~1, 1이면 원본 그대로)\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.interp_coef = interp_coef\n",
    "\n",
    "    def svd_decompose(self, W):\n",
    "        \"\"\"SVD 분해 수행\"\"\"\n",
    "        U, Sigma, Vt = np.linalg.svd(W, full_matrices=False)\n",
    "        return U, np.diag(Sigma), Vt\n",
    "\n",
    "    def extract_common_weights(self, W_plus, W_minus):\n",
    "        \"\"\"W⁺와 W⁻ 사이의 공통 성분 추출\"\"\"\n",
    "        W_cross = np.dot(W_plus, W_minus.T)\n",
    "        U_c, _, _ = self.svd_decompose(W_cross)\n",
    "        W_common = np.dot(U_c, np.dot(U_c.T, (W_plus + W_minus) / 2))\n",
    "        return W_common\n",
    "\n",
    "    def remove_common_weights(self, W_minus, W_common):\n",
    "        \"\"\"W⁻에서 공통 성분 제거\"\"\"\n",
    "        # 공통 성분에 대한 projection 계산 (역행렬 안정성을 위해 작은 값 추가)\n",
    "        projection = np.dot(W_common, np.linalg.inv(np.dot(W_common.T, W_common) + 1e-8 * np.eye(W_common.shape[1])))\n",
    "        projection = np.dot(projection, np.dot(W_common.T, W_minus))\n",
    "        W_minus_pure = W_minus - projection\n",
    "        return W_minus_pure\n",
    "\n",
    "    def compute_fisher_matrix(self, W_plus):\n",
    "        \"\"\"\n",
    "        간단한 대각 근사(Fisher information)의 surrogate를 계산합니다.\n",
    "        각 행의 평균 제곱값을 계산하여 대각행렬을 생성합니다.\n",
    "        (실제 계산 시에는 데이터와 손실 함수에 기반한 gradient 정보가 필요합니다.)\n",
    "        \"\"\"\n",
    "        # W_plus: (out_features, in_features)\n",
    "        diag_elements = np.mean(W_plus**2, axis=1)  # 각 행의 평균 제곱\n",
    "        fisher = np.diag(diag_elements)  # (out_features x out_features)\n",
    "        return fisher\n",
    "\n",
    "    def adjust_weights(self, W_minus_pure, fisher_matrix):\n",
    "        \"\"\"Fisher 정보를 활용하여 보정 (Fisher matrix는 대각 근사 사용)\"\"\"\n",
    "        fisher_inv = np.linalg.inv(fisher_matrix + 1e-8 * np.eye(fisher_matrix.shape[0]))\n",
    "        # 중요 파라미터의 변화가 축소되도록 fisher_inv를 곱함\n",
    "        W_minus_adjusted = np.dot(fisher_inv, W_minus_pure)\n",
    "        return W_minus_adjusted\n",
    "\n",
    "    def unlearn_weights(self, W_plus, W_minus):\n",
    "        \"\"\"\n",
    "        개선된 unlearning 절차:\n",
    "          1. 공통 성분 추출 및 제거\n",
    "          2. Fisher 정보를 통한 보정 (내부에서 자동 계산)\n",
    "          3. W⁺와 W⁻ 사이의 차이를 부드럽게 보정하고, 정규화 및 원본과 보간\n",
    "        \"\"\"\n",
    "        # 1. 공통 성분 추출 및 제거\n",
    "        W_common = self.extract_common_weights(W_plus, W_minus)\n",
    "        W_minus_pure = self.remove_common_weights(W_minus, W_common)\n",
    "        \n",
    "        # 2. Fisher matrix 계산 및 적용\n",
    "        fisher_matrix = self.compute_fisher_matrix(W_plus)\n",
    "        W_minus_adjusted = self.adjust_weights(W_minus_pure, fisher_matrix)\n",
    "\n",
    "        # 3. W⁺와 W⁻의 차이에 기초해 점진적 unlearning 적용\n",
    "        delta = self.beta * (W_plus - W_minus_adjusted)\n",
    "        W_unlearned = W_plus - delta\n",
    "\n",
    "        # 정규화 항: 과도한 변경 억제 (W⁺에 가깝게 유지)\n",
    "        W_unlearned = (1 - self.lambda_reg) * W_unlearned + self.lambda_reg * W_plus\n",
    "\n",
    "        # 최종 보간: 원래 W⁺ 정보와 혼합\n",
    "        W_unlearned = self.interp_coef * W_plus + (1 - self.interp_coef) * W_unlearned\n",
    "\n",
    "        # 값 범위 클리핑 (예: -1과 1 사이)\n",
    "        W_unlearned = np.clip(W_unlearned, -1, 1)\n",
    "        \n",
    "        return W_unlearned\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 2. W_new를 LoRA의 두 행렬(lora_B, lora_A)로 분해하는 함수\n",
    "#############################################\n",
    "def factorize_weight(W_new: np.ndarray, r: int, scaling: float):\n",
    "    \"\"\"\n",
    "    W_new(효과적인 LoRA 업데이트)를 lora_B와 lora_A로 분해합니다.\n",
    "    LoRA 업데이트는 원래 (lora_B @ lora_A) * scaling 형태로 적용되므로,\n",
    "    lora_B @ lora_A = W_new / scaling가 되어야 합니다.\n",
    "    \n",
    "    SVD를 통해 M = W_new/scaling = U S V^T 로 분해한 후,\n",
    "    lora_B = U * sqrt(S)   (shape: [out_features, r])\n",
    "    lora_A = sqrt(S) * V^T   (shape: [r, in_features])\n",
    "    \n",
    "    Args:\n",
    "        W_new (np.ndarray): 결합된 effective weight update (out_features x in_features)\n",
    "        r (int): LoRA의 rank (예제에서는 4)\n",
    "        scaling (float): lora_alpha / r (예: 32/4 = 8)\n",
    "        \n",
    "    Returns:\n",
    "        lora_B, lora_A: torch.Tensor로 변환된 분해 결과.\n",
    "    \"\"\"\n",
    "    M = W_new / scaling  # (lora_B @ lora_A = M)\n",
    "    U, S, Vh = np.linalg.svd(M, full_matrices=False)\n",
    "    \n",
    "    U_r = U[:, :r]      # (out_features x r)\n",
    "    S_r = S[:r]         # (r,)\n",
    "    Vh_r = Vh[:r, :]    # (r x in_features)\n",
    "    \n",
    "    sqrt_S = np.sqrt(S_r)\n",
    "    lora_B = U_r * sqrt_S[np.newaxis, :]   # broadcasting, shape: (out_features x r)\n",
    "    lora_A = sqrt_S[:, np.newaxis] * Vh_r    # shape: (r x in_features)\n",
    "    \n",
    "    # torch tensor로 변환 (dtype은 모델과 일치하도록)\n",
    "    lora_B = torch.tensor(lora_B, dtype=torch.float16)\n",
    "    lora_A = torch.tensor(lora_A, dtype=torch.float16)\n",
    "    return lora_B, lora_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#############################################\n",
    "# 3. Qwen 모델 로드 및 LoRA 적용 (PEFT 방식)\n",
    "#############################################\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    #attn_implementation=\"eager\"\n",
    "\n",
    ")\n",
    "model.config.sliding_window = None\n",
    "\n",
    "# LoRA 설정 (원래 파인튜닝에 사용했던 target module 목록)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\", \n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\", \n",
    "        \"mlp.down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# PEFT를 통해 모델에 LoRA 어댑터 추가\n",
    "peft_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "\n",
    "path_plus = \"./qwen-0.5b-lora-finetuned-alpaca-gpt4\"   # W+\n",
    "path_minus = \"./qwen-0.5b-lora-finetuned-toxic\"         # W-\n",
    "\n",
    "# 원본 Qwen 모델 로드\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Fine-tuned LoRA 모델 불러오기\n",
    "model_plus = PeftModel.from_pretrained(base_model, path_plus)\n",
    "model_minus = PeftModel.from_pretrained(base_model, path_minus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 모델의 state_dict 가져오기\n",
    "state_dict_plus = model_plus.state_dict()\n",
    "state_dict_minus = model_minus.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plus.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 ΔW: 18.04%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 ΔW: 186.23%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 ΔW: 772.46%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 ΔW: 60.76%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 ΔW: 4.35%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 ΔW: 8.21%\n",
      "bringing W+ and W-0\n",
      "combining lora weights\n",
      "Layer 0 ΔW: 47.00%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 ΔW: 29.23%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 ΔW: 295.43%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 ΔW: 621.66%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 ΔW: 146.42%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 ΔW: 4.26%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 ΔW: 7.48%\n",
      "bringing W+ and W-1\n",
      "combining lora weights\n",
      "Layer 1 ΔW: 61.74%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 ΔW: 27.85%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 ΔW: 396.06%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 ΔW: 661.84%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 ΔW: 43.91%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 ΔW: 4.04%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 ΔW: 6.24%\n",
      "bringing W+ and W-2\n",
      "combining lora weights\n",
      "Layer 2 ΔW: 329.41%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 ΔW: 38.11%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 ΔW: 547.20%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 ΔW: 777.50%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 ΔW: 37.03%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 ΔW: 4.25%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 ΔW: 10.70%\n",
      "bringing W+ and W-3\n",
      "combining lora weights\n",
      "Layer 3 ΔW: 68.44%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 ΔW: 36.51%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 ΔW: 477.32%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 ΔW: 1968.25%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 ΔW: 34.29%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 ΔW: 4.50%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 ΔW: 6.88%\n",
      "bringing W+ and W-4\n",
      "combining lora weights\n",
      "Layer 4 ΔW: 81.12%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 ΔW: 41.28%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 ΔW: 513.81%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 ΔW: 1161.25%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 ΔW: 38.37%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 ΔW: 4.60%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 ΔW: 11.24%\n",
      "bringing W+ and W-5\n",
      "combining lora weights\n",
      "Layer 5 ΔW: 47.45%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 ΔW: 43.95%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 ΔW: 358.36%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 ΔW: 1541.98%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 ΔW: 32.75%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 ΔW: 5.44%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 ΔW: 11.04%\n",
      "bringing W+ and W-6\n",
      "combining lora weights\n",
      "Layer 6 ΔW: 34.64%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 ΔW: 40.62%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 ΔW: 566.91%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 ΔW: 998.71%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 ΔW: 29.40%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 ΔW: 3.82%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 ΔW: 9.46%\n",
      "bringing W+ and W-7\n",
      "combining lora weights\n",
      "Layer 7 ΔW: 51.03%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 ΔW: 41.89%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 ΔW: 730.16%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 ΔW: 924.46%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 ΔW: 43.21%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 ΔW: 4.38%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 ΔW: 7.16%\n",
      "bringing W+ and W-8\n",
      "combining lora weights\n",
      "Layer 8 ΔW: 67.42%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 ΔW: 39.57%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 ΔW: 456.96%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 ΔW: 917.20%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 ΔW: 37.42%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 ΔW: 4.58%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 ΔW: 5.87%\n",
      "bringing W+ and W-9\n",
      "combining lora weights\n",
      "Layer 9 ΔW: 47.09%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 ΔW: 43.99%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 ΔW: 337.82%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 ΔW: 782.94%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 ΔW: 46.92%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 ΔW: 4.40%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 ΔW: 5.53%\n",
      "bringing W+ and W-10\n",
      "combining lora weights\n",
      "Layer 10 ΔW: 53.88%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 ΔW: 23.06%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 ΔW: 271.46%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 ΔW: 890.99%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 ΔW: 36.40%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 ΔW: 3.31%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 ΔW: 7.42%\n",
      "bringing W+ and W-11\n",
      "combining lora weights\n",
      "Layer 11 ΔW: 44.39%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 ΔW: 51.46%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 ΔW: 410.65%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 ΔW: 689.17%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 ΔW: 34.72%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 ΔW: 6.14%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 ΔW: 7.61%\n",
      "bringing W+ and W-12\n",
      "combining lora weights\n",
      "Layer 12 ΔW: 38.66%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 ΔW: 39.20%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 ΔW: 329.48%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 ΔW: 779.66%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 ΔW: 29.80%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 ΔW: 4.19%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 ΔW: 6.51%\n",
      "bringing W+ and W-13\n",
      "combining lora weights\n",
      "Layer 13 ΔW: 46.03%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 ΔW: 50.02%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 ΔW: 425.10%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 ΔW: 587.99%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 ΔW: 40.95%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 ΔW: 5.04%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 ΔW: 5.61%\n",
      "bringing W+ and W-14\n",
      "combining lora weights\n",
      "Layer 14 ΔW: 29.97%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 ΔW: 32.15%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 ΔW: 324.18%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 ΔW: 452.29%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 ΔW: 34.67%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 ΔW: 2.43%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 ΔW: 5.67%\n",
      "bringing W+ and W-15\n",
      "combining lora weights\n",
      "Layer 15 ΔW: 47.01%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 ΔW: 34.29%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 ΔW: 468.16%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 ΔW: 2322.10%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 ΔW: 16.60%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 ΔW: 3.69%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 ΔW: 6.33%\n",
      "bringing W+ and W-16\n",
      "combining lora weights\n",
      "Layer 16 ΔW: 35.66%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 ΔW: 30.84%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 ΔW: 432.15%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 ΔW: 774.40%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 ΔW: 22.83%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 ΔW: 4.02%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 ΔW: 8.95%\n",
      "bringing W+ and W-17\n",
      "combining lora weights\n",
      "Layer 17 ΔW: 52.64%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 ΔW: 30.56%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 ΔW: 313.32%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 ΔW: 609.38%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 ΔW: 18.96%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 ΔW: 3.10%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 ΔW: 4.79%\n",
      "bringing W+ and W-18\n",
      "combining lora weights\n",
      "Layer 18 ΔW: 27.13%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 ΔW: 33.75%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 ΔW: 332.70%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 ΔW: 708.51%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 ΔW: 24.74%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 ΔW: 2.74%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 ΔW: 4.50%\n",
      "bringing W+ and W-19\n",
      "combining lora weights\n",
      "Layer 19 ΔW: 33.55%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 ΔW: 29.84%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 ΔW: 248.51%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 ΔW: 701.56%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 ΔW: 80.10%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 ΔW: 4.09%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 ΔW: 3.64%\n",
      "bringing W+ and W-20\n",
      "combining lora weights\n",
      "Layer 20 ΔW: 26.52%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 ΔW: 35.56%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 ΔW: 421.14%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 ΔW: 488.02%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 ΔW: 35.01%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 ΔW: 1.93%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 ΔW: 2.59%\n",
      "bringing W+ and W-21\n",
      "combining lora weights\n",
      "Layer 21 ΔW: 66.67%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 ΔW: 29.97%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 ΔW: 269.82%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 ΔW: 776.80%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 ΔW: 25.34%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 ΔW: 1.70%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 ΔW: 2.72%\n",
      "bringing W+ and W-22\n",
      "combining lora weights\n",
      "Layer 22 ΔW: 21.79%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 ΔW: 27.62%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 ΔW: 297.28%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 ΔW: 316.01%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 ΔW: 33.87%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 ΔW: 2.55%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 ΔW: 3.94%\n",
      "bringing W+ and W-23\n",
      "combining lora weights\n",
      "Layer 23 ΔW: 81.61%\n",
      "새로운 결합된 LoRA 모델이 저장되었습니다: ./qwen-0.5b-unlearned-lora\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# 🟢 LoRA Target Modules\n",
    "target_modules = [\n",
    "    \"self_attn.q_proj\",\n",
    "    \"self_attn.k_proj\", \n",
    "    \"self_attn.v_proj\",\n",
    "    \"self_attn.o_proj\",\n",
    "    \"mlp.gate_proj\",\n",
    "    \"mlp.up_proj\", \n",
    "    \"mlp.down_proj\"\n",
    "]\n",
    "\n",
    "# LoRA 설정 (새로운 모델에 적용)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "# 새로운 PEFT 모델 생성\n",
    "new_peft_model = get_peft_model(base_model, lora_config)\n",
    "new_state_dict = new_peft_model.state_dict()\n",
    "\n",
    "# LoRA scaling factor\n",
    "scaling = lora_config.lora_alpha / lora_config.r  # 예: 32/4 = 8\n",
    "\n",
    "for layer_idx in range(24):  # Qwen-0.5B는 24개의 Transformer layer를 가짐\n",
    "    for target_module in target_modules:\n",
    "        # LoRA weight 키 생성 (각 레이어에 대해)\n",
    "        key_A = f\"base_model.model.model.layers.{layer_idx}.{target_module}.lora_A.default.weight\"\n",
    "        key_B = f\"base_model.model.model.layers.{layer_idx}.{target_module}.lora_B.default.weight\"\n",
    "\n",
    "        # 키가 존재하는 경우에만 업데이트 수행\n",
    "        if key_A in state_dict_plus and key_B in state_dict_plus:\n",
    "            print(f\"bringing W+ and W-{layer_idx}\")\n",
    "            # 기존 W+와 W- 불러오기 (torch.Tensor → numpy 변환)\n",
    "            W_plus = (state_dict_plus[key_B] @ state_dict_plus[key_A]).cpu().numpy()\n",
    "            W_minus = (state_dict_minus[key_B] @ state_dict_minus[key_A]).cpu().numpy()\n",
    "            print(\"combining lora weights\")\n",
    "            # W_new 생성\n",
    "            # rank_common < LoRA rank\n",
    "            unlearner = LoRAUnlearner(alpha=0.5, beta=0.1, lambda_reg=0.5, interp_coef=0.90)\n",
    "            W_new = unlearner.unlearn_weights(W_plus, W_minus)\n",
    "\n",
    "            delta = np.linalg.norm(W_new - W_plus) / np.linalg.norm(W_plus)\n",
    "            print(f\"Layer {layer_idx} ΔW: {delta:.2%}\")\n",
    "            # W_new를 lora_A, lora_B로 복구 (함수 확인해야됨.)\n",
    "            lora_B, lora_A = factorize_weight(W_new, r=lora_config.r, scaling=scaling)\n",
    "\n",
    "            # 새로운 모델의 LoRA weight 업데이트 (torch.Tensor 형태로 변환) (음 진짜?)\n",
    "            with torch.no_grad():\n",
    "                new_state_dict[key_A].copy_(lora_A.to(new_state_dict[key_A].dtype))\n",
    "                new_state_dict[key_B].copy_(lora_B.to(new_state_dict[key_B].dtype))\n",
    "                \n",
    "new_peft_model.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "\n",
    "save_path = \"./qwen-0.5b-unlearned-lora\"\n",
    "new_peft_model.save_pretrained(save_path)\n",
    "print(f\"새로운 결합된 LoRA 모델이 저장되었습니다: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "i=114\n",
    "# JSON 파일 불러오기\n",
    "with open('output/toxic_eval_unlearned_2025-02-21.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "with open('output/toxic_eval_alpaca-gpt4_finetuned_results.json', 'r', encoding='utf-8') as file:\n",
    "    dataalpaca = json.load(file)\n",
    "\n",
    "print(\"unlearned\")\n",
    "print(data[i]['instruction'])\n",
    "print(data[i]['output'])\n",
    "\n",
    "print(\"alpaca\")\n",
    "print(dataalpaca[i]['instruction'])\n",
    "print(dataalpaca[i]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
