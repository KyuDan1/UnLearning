{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "#############################################\n",
    "# 1. 두 LoRA weight 행렬(W⁺, W⁻) 결합 함수 정의\n",
    "#############################################\n",
    "def combine_lora_weights(W_plus: np.ndarray, W_minus: np.ndarray, threshold: float = 0.9) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    두 LoRA weight 행렬 W_plus와 W_minus를 결합하여 새로운 weight W_new를 계산합니다.\n",
    "    (W_new = W_plus + (W_minus - P W_minus), P는 두 행렬의 공통 subspace에 대한 projection)\n",
    "    \n",
    "    Args:\n",
    "        W_plus (np.ndarray): alpaca-gpt4로 파인튜닝한 모델의 LoRA weight (W⁺)\n",
    "        W_minus (np.ndarray): toxic 데이터셋으로 파인튜닝한 모델의 LoRA weight (W⁻)\n",
    "        threshold (float): 공통 subspace를 선택할 singular value threshold\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 결합된 새로운 weight W_new.\n",
    "    \"\"\"\n",
    "    r = 4  # LoRA의 rank (두 모델 모두 r=4로 가정)\n",
    "    lamb = 1.0\n",
    "    # W_plus에 대한 SVD: W_plus = U_plus * S_plus * Vh_plus\n",
    "    U_plus, S_plus, Vh_plus = np.linalg.svd(W_plus, full_matrices=False)\n",
    "    U_plus = U_plus[:, :r]  # (out_features x r)\n",
    "    \n",
    "    # W_minus에 대한 SVD: W_minus = U_minus * S_minus * Vh_minus\n",
    "    U_minus, S_minus, Vh_minus = np.linalg.svd(W_minus, full_matrices=False)\n",
    "    U_minus = U_minus[:, :r]  # (out_features x r)\n",
    "    \n",
    "    # U_plus와 U_minus의 공통 subspace 찾기:\n",
    "    X = np.dot(U_plus.T, U_minus)   # (r x r)\n",
    "    U_m, singular_values, Vh_m = np.linalg.svd(X)\n",
    "    \n",
    "    # singular value가 threshold 이상인 방향 선택\n",
    "    common_indices = np.where(singular_values >= threshold)[0]\n",
    "    if common_indices.size == 0:\n",
    "        print(\"Threshold 이상의 공통 subspace가 없습니다. 모든 r 방향을 사용합니다.\")\n",
    "        common_indices = np.arange(r)\n",
    "        \n",
    "    # U_common: U_plus의 선형 조합으로 공통 basis 구하기\n",
    "    U_common = np.dot(U_plus, U_m[:, common_indices])  # (out_features x k), k <= r\n",
    "    P = np.dot(U_common, U_common.T)  # Projection matrix onto common subspace\n",
    "    \n",
    "    # W_common: W_minus의 공통 성분\n",
    "    W_common = np.dot(P, W_minus)\n",
    "    \n",
    "    # 최종적으로 결합한 weight\n",
    "    W_new = W_plus - lamb * (W_minus - lamb * W_common)\n",
    "    return W_new\n",
    "\n",
    "#############################################\n",
    "# 2. W_new를 LoRA의 두 행렬(lora_B, lora_A)로 분해하는 함수\n",
    "#############################################\n",
    "def factorize_weight(W_new: np.ndarray, r: int, scaling: float):\n",
    "    \"\"\"\n",
    "    W_new(효과적인 LoRA 업데이트)를 lora_B와 lora_A로 분해합니다.\n",
    "    LoRA 업데이트는 원래 (lora_B @ lora_A) * scaling 형태로 적용되므로,\n",
    "    lora_B @ lora_A = W_new / scaling가 되어야 합니다.\n",
    "    \n",
    "    SVD를 통해 M = W_new/scaling = U S V^T 로 분해한 후,\n",
    "    lora_B = U * sqrt(S)   (shape: [out_features, r])\n",
    "    lora_A = sqrt(S) * V^T   (shape: [r, in_features])\n",
    "    \n",
    "    Args:\n",
    "        W_new (np.ndarray): 결합된 effective weight update (out_features x in_features)\n",
    "        r (int): LoRA의 rank (예제에서는 4)\n",
    "        scaling (float): lora_alpha / r (예: 32/4 = 8)\n",
    "        \n",
    "    Returns:\n",
    "        lora_B, lora_A: torch.Tensor로 변환된 분해 결과.\n",
    "    \"\"\"\n",
    "    M = W_new / scaling  # (lora_B @ lora_A = M)\n",
    "    U, S, Vh = np.linalg.svd(M, full_matrices=False)\n",
    "    \n",
    "    U_r = U[:, :r]      # (out_features x r)\n",
    "    S_r = S[:r]         # (r,)\n",
    "    Vh_r = Vh[:r, :]    # (r x in_features)\n",
    "    \n",
    "    sqrt_S = np.sqrt(S_r)\n",
    "    lora_B = U_r * sqrt_S[np.newaxis, :]   # broadcasting, shape: (out_features x r)\n",
    "    lora_A = sqrt_S[:, np.newaxis] * Vh_r    # shape: (r x in_features)\n",
    "    \n",
    "    # torch tensor로 변환 (dtype은 모델과 일치하도록)\n",
    "    lora_B = torch.tensor(lora_B, dtype=torch.float16)\n",
    "    lora_A = torch.tensor(lora_A, dtype=torch.float16)\n",
    "    return lora_B, lora_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################\n",
    "# 3. Qwen 모델 로드 및 LoRA 적용 (PEFT 방식)\n",
    "#############################################\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# LoRA 설정 (원래 파인튜닝에 사용했던 target module 목록)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\", \n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\", \n",
    "        \"mlp.down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# PEFT를 통해 모델에 LoRA 어댑터 추가\n",
    "peft_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 LoRA 모델이 정상적으로 적용되었습니다!\n",
      "새로운 결합된 LoRA 모델이 저장되었습니다: ./qwen-0.5b-unlearned-lora\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "\n",
    "# 🟢 Fine-tuned LoRA 모델 경로\n",
    "path_plus = \"./qwen-0.5b-lora-finetuned-alpaca-gpt4\"   # W+\n",
    "path_minus = \"./qwen-0.5b-lora-finetuned-toxic\"         # W-\n",
    "\n",
    "# 원본 Qwen 모델 로드\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Fine-tuned LoRA 모델 불러오기\n",
    "model_plus = PeftModel.from_pretrained(base_model, path_plus)\n",
    "model_minus = PeftModel.from_pretrained(base_model, path_minus)\n",
    "\n",
    "# 모델의 state_dict 가져오기\n",
    "state_dict_plus = model_plus.state_dict()\n",
    "state_dict_minus = model_minus.state_dict()\n",
    "\n",
    "# 🟢 LoRA Target Modules\n",
    "target_modules = [\n",
    "    \"self_attn.q_proj\",\n",
    "    \"self_attn.k_proj\", \n",
    "    \"self_attn.v_proj\",\n",
    "    \"self_attn.o_proj\",\n",
    "    \"mlp.gate_proj\",\n",
    "    \"mlp.up_proj\", \n",
    "    \"mlp.down_proj\"\n",
    "]\n",
    "\n",
    "# LoRA 설정 (새로운 모델에 적용)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "# 새로운 PEFT 모델 생성\n",
    "new_peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# LoRA scaling factor\n",
    "scaling = lora_config.lora_alpha / lora_config.r  # 예: 32/4 = 8\n",
    "\n",
    "# 🟢 LoRA weight를 결합하여 새로운 W_new 생성\n",
    "for layer_idx in range(24):  # Qwen-0.5B는 24개의 Transformer layer를 가짐\n",
    "    for target_module in target_modules:\n",
    "        # LoRA weight 키 생성 (각 레이어에 대해)\n",
    "        key_A = f\"base_model.model.model.layers.{layer_idx}.{target_module}.lora_A.default.weight\"\n",
    "        key_B = f\"base_model.model.model.layers.{layer_idx}.{target_module}.lora_B.default.weight\"\n",
    "\n",
    "        # 키가 존재하는 경우에만 업데이트 수행\n",
    "        if key_A in state_dict_plus and key_B in state_dict_plus:\n",
    "            # 기존 W+와 W- 불러오기 (torch.Tensor → numpy 변환)\n",
    "            W_plus = (state_dict_plus[key_B] @ state_dict_plus[key_A]).cpu().numpy()\n",
    "            W_minus = (state_dict_minus[key_B] @ state_dict_minus[key_A]).cpu().numpy()\n",
    "\n",
    "            # W_new 생성\n",
    "            W_new = combine_lora_weights(W_plus, W_minus)\n",
    "\n",
    "            # W_new를 SVD 분해하여 lora_A, lora_B로 복구\n",
    "            lora_B, lora_A = factorize_weight(W_new, r=lora_config.r, scaling=scaling)\n",
    "\n",
    "            # 새로운 모델의 LoRA weight 업데이트 (torch.Tensor 형태로 변환)\n",
    "            with torch.no_grad():\n",
    "                state_dict_plus[key_A].copy_(lora_A.to(state_dict_plus[key_A].dtype))\n",
    "                state_dict_plus[key_B].copy_(lora_B.to(state_dict_plus[key_B].dtype))\n",
    "\n",
    "print(\"새로운 LoRA 모델이 정상적으로 적용되었습니다!\")\n",
    "\n",
    "# 🟢 새로운 LoRA 모델 저장\n",
    "save_path = \"./qwen-0.5b-unlearned-lora\"\n",
    "new_peft_model.save_pretrained(save_path)\n",
    "print(f\"새로운 결합된 LoRA 모델이 저장되었습니다: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
