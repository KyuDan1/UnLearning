{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9997014034040012,
  "eval_steps": 500,
  "global_step": 1674,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005971931919976112,
      "grad_norm": 150.50726318359375,
      "learning_rate": 8.333333333333334e-06,
      "loss": 14.68,
      "step": 10
    },
    {
      "epoch": 0.011943863839952225,
      "grad_norm": 222.8882598876953,
      "learning_rate": 1.9047619047619046e-05,
      "loss": 10.4631,
      "step": 20
    },
    {
      "epoch": 0.017915795759928337,
      "grad_norm": 2.5456202030181885,
      "learning_rate": 3.095238095238095e-05,
      "loss": 1.7257,
      "step": 30
    },
    {
      "epoch": 0.02388772767990445,
      "grad_norm": 0.6856053471565247,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 0.5183,
      "step": 40
    },
    {
      "epoch": 0.029859659599880562,
      "grad_norm": 0.49762794375419617,
      "learning_rate": 5.4761904761904766e-05,
      "loss": 0.5324,
      "step": 50
    },
    {
      "epoch": 0.035831591519856675,
      "grad_norm": 0.4563083350658417,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.5097,
      "step": 60
    },
    {
      "epoch": 0.04180352343983278,
      "grad_norm": 0.4627855122089386,
      "learning_rate": 7.857142857142858e-05,
      "loss": 0.4942,
      "step": 70
    },
    {
      "epoch": 0.0477754553598089,
      "grad_norm": 0.4405980110168457,
      "learning_rate": 9.047619047619048e-05,
      "loss": 0.4878,
      "step": 80
    },
    {
      "epoch": 0.05374738727978501,
      "grad_norm": 0.3872614800930023,
      "learning_rate": 0.00010238095238095237,
      "loss": 0.4845,
      "step": 90
    },
    {
      "epoch": 0.059719319199761124,
      "grad_norm": 0.40151581168174744,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.4813,
      "step": 100
    },
    {
      "epoch": 0.06569125111973724,
      "grad_norm": 0.3905043303966522,
      "learning_rate": 0.0001261904761904762,
      "loss": 0.4641,
      "step": 110
    },
    {
      "epoch": 0.07166318303971335,
      "grad_norm": 0.43501052260398865,
      "learning_rate": 0.0001380952380952381,
      "loss": 0.4856,
      "step": 120
    },
    {
      "epoch": 0.07763511495968946,
      "grad_norm": 0.39994725584983826,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.4895,
      "step": 130
    },
    {
      "epoch": 0.08360704687966557,
      "grad_norm": 0.39407825469970703,
      "learning_rate": 0.00016190476190476192,
      "loss": 0.4343,
      "step": 140
    },
    {
      "epoch": 0.08957897879964169,
      "grad_norm": 0.39342018961906433,
      "learning_rate": 0.00017380952380952383,
      "loss": 0.4485,
      "step": 150
    },
    {
      "epoch": 0.0955509107196178,
      "grad_norm": 0.3729061484336853,
      "learning_rate": 0.00018571428571428572,
      "loss": 0.5008,
      "step": 160
    },
    {
      "epoch": 0.10152284263959391,
      "grad_norm": 0.40832221508026123,
      "learning_rate": 0.00019761904761904763,
      "loss": 0.4439,
      "step": 170
    },
    {
      "epoch": 0.10749477455957002,
      "grad_norm": 0.39069032669067383,
      "learning_rate": 0.00019893758300132804,
      "loss": 0.4835,
      "step": 180
    },
    {
      "epoch": 0.11346670647954614,
      "grad_norm": 0.40626877546310425,
      "learning_rate": 0.00019760956175298805,
      "loss": 0.4991,
      "step": 190
    },
    {
      "epoch": 0.11943863839952225,
      "grad_norm": 0.3809158504009247,
      "learning_rate": 0.0001962815405046481,
      "loss": 0.4137,
      "step": 200
    },
    {
      "epoch": 0.12541057031949837,
      "grad_norm": 0.33558934926986694,
      "learning_rate": 0.0001949535192563081,
      "loss": 0.4579,
      "step": 210
    },
    {
      "epoch": 0.13138250223947448,
      "grad_norm": 0.34306949377059937,
      "learning_rate": 0.00019362549800796815,
      "loss": 0.458,
      "step": 220
    },
    {
      "epoch": 0.1373544341594506,
      "grad_norm": 0.31331560015678406,
      "learning_rate": 0.00019229747675962816,
      "loss": 0.4405,
      "step": 230
    },
    {
      "epoch": 0.1433263660794267,
      "grad_norm": 0.34082093834877014,
      "learning_rate": 0.0001909694555112882,
      "loss": 0.4469,
      "step": 240
    },
    {
      "epoch": 0.1492982979994028,
      "grad_norm": 0.3707633316516876,
      "learning_rate": 0.00018964143426294822,
      "loss": 0.5003,
      "step": 250
    },
    {
      "epoch": 0.15527022991937892,
      "grad_norm": 0.34048599004745483,
      "learning_rate": 0.00018831341301460826,
      "loss": 0.4679,
      "step": 260
    },
    {
      "epoch": 0.16124216183935502,
      "grad_norm": 0.3304387032985687,
      "learning_rate": 0.00018698539176626827,
      "loss": 0.4503,
      "step": 270
    },
    {
      "epoch": 0.16721409375933113,
      "grad_norm": 0.3485371470451355,
      "learning_rate": 0.00018565737051792828,
      "loss": 0.4702,
      "step": 280
    },
    {
      "epoch": 0.17318602567930724,
      "grad_norm": 0.3654024600982666,
      "learning_rate": 0.00018432934926958832,
      "loss": 0.4989,
      "step": 290
    },
    {
      "epoch": 0.17915795759928338,
      "grad_norm": 0.30477234721183777,
      "learning_rate": 0.00018300132802124834,
      "loss": 0.4748,
      "step": 300
    },
    {
      "epoch": 0.1851298895192595,
      "grad_norm": 0.34044790267944336,
      "learning_rate": 0.00018167330677290838,
      "loss": 0.4765,
      "step": 310
    },
    {
      "epoch": 0.1911018214392356,
      "grad_norm": 0.33542245626449585,
      "learning_rate": 0.0001803452855245684,
      "loss": 0.4855,
      "step": 320
    },
    {
      "epoch": 0.1970737533592117,
      "grad_norm": 0.2856369614601135,
      "learning_rate": 0.00017901726427622843,
      "loss": 0.5007,
      "step": 330
    },
    {
      "epoch": 0.20304568527918782,
      "grad_norm": 0.3508654236793518,
      "learning_rate": 0.00017768924302788845,
      "loss": 0.4368,
      "step": 340
    },
    {
      "epoch": 0.20901761719916392,
      "grad_norm": 0.3387034833431244,
      "learning_rate": 0.0001763612217795485,
      "loss": 0.4734,
      "step": 350
    },
    {
      "epoch": 0.21498954911914003,
      "grad_norm": 0.3267536759376526,
      "learning_rate": 0.0001750332005312085,
      "loss": 0.468,
      "step": 360
    },
    {
      "epoch": 0.22096148103911614,
      "grad_norm": 0.39946794509887695,
      "learning_rate": 0.00017370517928286854,
      "loss": 0.497,
      "step": 370
    },
    {
      "epoch": 0.22693341295909228,
      "grad_norm": 0.3365913927555084,
      "learning_rate": 0.00017237715803452856,
      "loss": 0.456,
      "step": 380
    },
    {
      "epoch": 0.2329053448790684,
      "grad_norm": 0.37483564019203186,
      "learning_rate": 0.0001710491367861886,
      "loss": 0.4969,
      "step": 390
    },
    {
      "epoch": 0.2388772767990445,
      "grad_norm": 0.3901427090167999,
      "learning_rate": 0.0001697211155378486,
      "loss": 0.4758,
      "step": 400
    },
    {
      "epoch": 0.2448492087190206,
      "grad_norm": 0.33307111263275146,
      "learning_rate": 0.00016839309428950865,
      "loss": 0.4785,
      "step": 410
    },
    {
      "epoch": 0.25082114063899674,
      "grad_norm": 0.3488765358924866,
      "learning_rate": 0.00016706507304116867,
      "loss": 0.5024,
      "step": 420
    },
    {
      "epoch": 0.2567930725589728,
      "grad_norm": 0.35506463050842285,
      "learning_rate": 0.0001657370517928287,
      "loss": 0.464,
      "step": 430
    },
    {
      "epoch": 0.26276500447894896,
      "grad_norm": 0.33117520809173584,
      "learning_rate": 0.00016440903054448872,
      "loss": 0.4809,
      "step": 440
    },
    {
      "epoch": 0.26873693639892504,
      "grad_norm": 0.31057828664779663,
      "learning_rate": 0.00016308100929614876,
      "loss": 0.4711,
      "step": 450
    },
    {
      "epoch": 0.2747088683189012,
      "grad_norm": 0.3547170162200928,
      "learning_rate": 0.00016175298804780875,
      "loss": 0.5076,
      "step": 460
    },
    {
      "epoch": 0.28068080023887726,
      "grad_norm": 0.3108370304107666,
      "learning_rate": 0.0001604249667994688,
      "loss": 0.4345,
      "step": 470
    },
    {
      "epoch": 0.2866527321588534,
      "grad_norm": 0.3935929536819458,
      "learning_rate": 0.0001590969455511288,
      "loss": 0.4978,
      "step": 480
    },
    {
      "epoch": 0.2926246640788295,
      "grad_norm": 0.28076809644699097,
      "learning_rate": 0.00015776892430278885,
      "loss": 0.492,
      "step": 490
    },
    {
      "epoch": 0.2985965959988056,
      "grad_norm": 0.3356407880783081,
      "learning_rate": 0.00015644090305444886,
      "loss": 0.4597,
      "step": 500
    },
    {
      "epoch": 0.30456852791878175,
      "grad_norm": 0.3033050000667572,
      "learning_rate": 0.0001551128818061089,
      "loss": 0.4584,
      "step": 510
    },
    {
      "epoch": 0.31054045983875783,
      "grad_norm": 0.2620345652103424,
      "learning_rate": 0.00015378486055776891,
      "loss": 0.4213,
      "step": 520
    },
    {
      "epoch": 0.31651239175873397,
      "grad_norm": 0.3146626651287079,
      "learning_rate": 0.00015245683930942896,
      "loss": 0.4656,
      "step": 530
    },
    {
      "epoch": 0.32248432367871005,
      "grad_norm": 0.3576095998287201,
      "learning_rate": 0.000151128818061089,
      "loss": 0.4377,
      "step": 540
    },
    {
      "epoch": 0.3284562555986862,
      "grad_norm": 0.30313190817832947,
      "learning_rate": 0.000149800796812749,
      "loss": 0.4475,
      "step": 550
    },
    {
      "epoch": 0.33442818751866227,
      "grad_norm": 0.3296883702278137,
      "learning_rate": 0.00014847277556440905,
      "loss": 0.4317,
      "step": 560
    },
    {
      "epoch": 0.3404001194386384,
      "grad_norm": 0.292310893535614,
      "learning_rate": 0.00014714475431606907,
      "loss": 0.4757,
      "step": 570
    },
    {
      "epoch": 0.3463720513586145,
      "grad_norm": 0.314487099647522,
      "learning_rate": 0.0001458167330677291,
      "loss": 0.4391,
      "step": 580
    },
    {
      "epoch": 0.3523439832785906,
      "grad_norm": 0.2999584376811981,
      "learning_rate": 0.00014448871181938912,
      "loss": 0.4596,
      "step": 590
    },
    {
      "epoch": 0.35831591519856676,
      "grad_norm": 0.3364800214767456,
      "learning_rate": 0.00014316069057104916,
      "loss": 0.5013,
      "step": 600
    },
    {
      "epoch": 0.36428784711854284,
      "grad_norm": 0.34989601373672485,
      "learning_rate": 0.00014183266932270918,
      "loss": 0.4825,
      "step": 610
    },
    {
      "epoch": 0.370259779038519,
      "grad_norm": 0.34458544850349426,
      "learning_rate": 0.00014050464807436922,
      "loss": 0.5199,
      "step": 620
    },
    {
      "epoch": 0.37623171095849506,
      "grad_norm": 0.2999364137649536,
      "learning_rate": 0.00013917662682602923,
      "loss": 0.4868,
      "step": 630
    },
    {
      "epoch": 0.3822036428784712,
      "grad_norm": 0.36524906754493713,
      "learning_rate": 0.00013784860557768927,
      "loss": 0.505,
      "step": 640
    },
    {
      "epoch": 0.3881755747984473,
      "grad_norm": 0.35044658184051514,
      "learning_rate": 0.00013652058432934926,
      "loss": 0.466,
      "step": 650
    },
    {
      "epoch": 0.3941475067184234,
      "grad_norm": 0.29094743728637695,
      "learning_rate": 0.0001351925630810093,
      "loss": 0.4763,
      "step": 660
    },
    {
      "epoch": 0.40011943863839955,
      "grad_norm": 0.3443804383277893,
      "learning_rate": 0.0001338645418326693,
      "loss": 0.4807,
      "step": 670
    },
    {
      "epoch": 0.40609137055837563,
      "grad_norm": 0.2920614182949066,
      "learning_rate": 0.00013253652058432935,
      "loss": 0.4406,
      "step": 680
    },
    {
      "epoch": 0.41206330247835177,
      "grad_norm": 0.24723079800605774,
      "learning_rate": 0.00013120849933598937,
      "loss": 0.4898,
      "step": 690
    },
    {
      "epoch": 0.41803523439832785,
      "grad_norm": 0.2886427640914917,
      "learning_rate": 0.0001298804780876494,
      "loss": 0.4742,
      "step": 700
    },
    {
      "epoch": 0.424007166318304,
      "grad_norm": 0.3378130793571472,
      "learning_rate": 0.00012855245683930942,
      "loss": 0.4742,
      "step": 710
    },
    {
      "epoch": 0.42997909823828007,
      "grad_norm": 0.28247058391571045,
      "learning_rate": 0.00012722443559096946,
      "loss": 0.4572,
      "step": 720
    },
    {
      "epoch": 0.4359510301582562,
      "grad_norm": 0.3197522759437561,
      "learning_rate": 0.00012589641434262948,
      "loss": 0.4804,
      "step": 730
    },
    {
      "epoch": 0.4419229620782323,
      "grad_norm": 0.3542722165584564,
      "learning_rate": 0.00012456839309428952,
      "loss": 0.4551,
      "step": 740
    },
    {
      "epoch": 0.4478948939982084,
      "grad_norm": 0.3083886504173279,
      "learning_rate": 0.00012324037184594953,
      "loss": 0.44,
      "step": 750
    },
    {
      "epoch": 0.45386682591818456,
      "grad_norm": 0.32869309186935425,
      "learning_rate": 0.00012191235059760957,
      "loss": 0.4796,
      "step": 760
    },
    {
      "epoch": 0.45983875783816064,
      "grad_norm": 0.3117542266845703,
      "learning_rate": 0.0001205843293492696,
      "loss": 0.4488,
      "step": 770
    },
    {
      "epoch": 0.4658106897581368,
      "grad_norm": 0.28525060415267944,
      "learning_rate": 0.00011925630810092963,
      "loss": 0.4238,
      "step": 780
    },
    {
      "epoch": 0.47178262167811286,
      "grad_norm": 0.2568216323852539,
      "learning_rate": 0.00011792828685258966,
      "loss": 0.4903,
      "step": 790
    },
    {
      "epoch": 0.477754553598089,
      "grad_norm": 0.3421175181865692,
      "learning_rate": 0.00011660026560424968,
      "loss": 0.4963,
      "step": 800
    },
    {
      "epoch": 0.4837264855180651,
      "grad_norm": 0.2747792601585388,
      "learning_rate": 0.00011527224435590971,
      "loss": 0.4897,
      "step": 810
    },
    {
      "epoch": 0.4896984174380412,
      "grad_norm": 0.2905268669128418,
      "learning_rate": 0.00011394422310756974,
      "loss": 0.497,
      "step": 820
    },
    {
      "epoch": 0.4956703493580173,
      "grad_norm": 0.33245882391929626,
      "learning_rate": 0.00011261620185922976,
      "loss": 0.4633,
      "step": 830
    },
    {
      "epoch": 0.5016422812779935,
      "grad_norm": 0.30797064304351807,
      "learning_rate": 0.00011128818061088976,
      "loss": 0.4531,
      "step": 840
    },
    {
      "epoch": 0.5076142131979695,
      "grad_norm": 0.2643960118293762,
      "learning_rate": 0.00010996015936254979,
      "loss": 0.4918,
      "step": 850
    },
    {
      "epoch": 0.5135861451179456,
      "grad_norm": 0.34538742899894714,
      "learning_rate": 0.00010863213811420982,
      "loss": 0.4378,
      "step": 860
    },
    {
      "epoch": 0.5195580770379218,
      "grad_norm": 0.33463355898857117,
      "learning_rate": 0.00010730411686586985,
      "loss": 0.4697,
      "step": 870
    },
    {
      "epoch": 0.5255300089578979,
      "grad_norm": 0.34438958764076233,
      "learning_rate": 0.00010597609561752987,
      "loss": 0.4303,
      "step": 880
    },
    {
      "epoch": 0.531501940877874,
      "grad_norm": 0.32181069254875183,
      "learning_rate": 0.0001046480743691899,
      "loss": 0.4948,
      "step": 890
    },
    {
      "epoch": 0.5374738727978501,
      "grad_norm": 0.32172441482543945,
      "learning_rate": 0.00010332005312084993,
      "loss": 0.4525,
      "step": 900
    },
    {
      "epoch": 0.5434458047178262,
      "grad_norm": 0.31274038553237915,
      "learning_rate": 0.00010199203187250996,
      "loss": 0.4428,
      "step": 910
    },
    {
      "epoch": 0.5494177366378024,
      "grad_norm": 0.3692021667957306,
      "learning_rate": 0.00010066401062416998,
      "loss": 0.4712,
      "step": 920
    },
    {
      "epoch": 0.5553896685577785,
      "grad_norm": 0.338919073343277,
      "learning_rate": 9.933598937583003e-05,
      "loss": 0.4399,
      "step": 930
    },
    {
      "epoch": 0.5613616004777545,
      "grad_norm": 0.30170419812202454,
      "learning_rate": 9.800796812749005e-05,
      "loss": 0.4431,
      "step": 940
    },
    {
      "epoch": 0.5673335323977307,
      "grad_norm": 0.2633068561553955,
      "learning_rate": 9.667994687915008e-05,
      "loss": 0.4489,
      "step": 950
    },
    {
      "epoch": 0.5733054643177068,
      "grad_norm": 0.2970050275325775,
      "learning_rate": 9.535192563081011e-05,
      "loss": 0.4853,
      "step": 960
    },
    {
      "epoch": 0.5792773962376829,
      "grad_norm": 0.27671122550964355,
      "learning_rate": 9.402390438247013e-05,
      "loss": 0.4755,
      "step": 970
    },
    {
      "epoch": 0.585249328157659,
      "grad_norm": 0.3309197425842285,
      "learning_rate": 9.269588313413015e-05,
      "loss": 0.4453,
      "step": 980
    },
    {
      "epoch": 0.5912212600776351,
      "grad_norm": 0.33566221594810486,
      "learning_rate": 9.136786188579018e-05,
      "loss": 0.4305,
      "step": 990
    },
    {
      "epoch": 0.5971931919976112,
      "grad_norm": 0.279891699552536,
      "learning_rate": 9.00398406374502e-05,
      "loss": 0.4487,
      "step": 1000
    },
    {
      "epoch": 0.6031651239175874,
      "grad_norm": 0.2883315682411194,
      "learning_rate": 8.871181938911023e-05,
      "loss": 0.4651,
      "step": 1010
    },
    {
      "epoch": 0.6091370558375635,
      "grad_norm": 0.31850871443748474,
      "learning_rate": 8.738379814077026e-05,
      "loss": 0.4703,
      "step": 1020
    },
    {
      "epoch": 0.6151089877575395,
      "grad_norm": 0.2822771668434143,
      "learning_rate": 8.605577689243029e-05,
      "loss": 0.4608,
      "step": 1030
    },
    {
      "epoch": 0.6210809196775157,
      "grad_norm": 0.27402085065841675,
      "learning_rate": 8.472775564409031e-05,
      "loss": 0.4428,
      "step": 1040
    },
    {
      "epoch": 0.6270528515974918,
      "grad_norm": 0.33210402727127075,
      "learning_rate": 8.339973439575034e-05,
      "loss": 0.4592,
      "step": 1050
    },
    {
      "epoch": 0.6330247835174679,
      "grad_norm": 0.32555076479911804,
      "learning_rate": 8.207171314741037e-05,
      "loss": 0.4579,
      "step": 1060
    },
    {
      "epoch": 0.638996715437444,
      "grad_norm": 0.28544822335243225,
      "learning_rate": 8.074369189907038e-05,
      "loss": 0.4812,
      "step": 1070
    },
    {
      "epoch": 0.6449686473574201,
      "grad_norm": 0.2604288160800934,
      "learning_rate": 7.941567065073041e-05,
      "loss": 0.4225,
      "step": 1080
    },
    {
      "epoch": 0.6509405792773962,
      "grad_norm": 0.3315622806549072,
      "learning_rate": 7.808764940239044e-05,
      "loss": 0.4791,
      "step": 1090
    },
    {
      "epoch": 0.6569125111973724,
      "grad_norm": 0.33338621258735657,
      "learning_rate": 7.675962815405046e-05,
      "loss": 0.4659,
      "step": 1100
    },
    {
      "epoch": 0.6628844431173485,
      "grad_norm": 0.2929788827896118,
      "learning_rate": 7.543160690571049e-05,
      "loss": 0.4157,
      "step": 1110
    },
    {
      "epoch": 0.6688563750373245,
      "grad_norm": 0.32037070393562317,
      "learning_rate": 7.410358565737052e-05,
      "loss": 0.4511,
      "step": 1120
    },
    {
      "epoch": 0.6748283069573007,
      "grad_norm": 0.3052944242954254,
      "learning_rate": 7.277556440903055e-05,
      "loss": 0.4816,
      "step": 1130
    },
    {
      "epoch": 0.6808002388772768,
      "grad_norm": 0.2878150939941406,
      "learning_rate": 7.144754316069057e-05,
      "loss": 0.4419,
      "step": 1140
    },
    {
      "epoch": 0.686772170797253,
      "grad_norm": 0.30404984951019287,
      "learning_rate": 7.01195219123506e-05,
      "loss": 0.462,
      "step": 1150
    },
    {
      "epoch": 0.692744102717229,
      "grad_norm": 0.303451269865036,
      "learning_rate": 6.879150066401063e-05,
      "loss": 0.4631,
      "step": 1160
    },
    {
      "epoch": 0.6987160346372051,
      "grad_norm": 0.27817559242248535,
      "learning_rate": 6.746347941567066e-05,
      "loss": 0.4099,
      "step": 1170
    },
    {
      "epoch": 0.7046879665571812,
      "grad_norm": 0.3416375517845154,
      "learning_rate": 6.613545816733068e-05,
      "loss": 0.4473,
      "step": 1180
    },
    {
      "epoch": 0.7106598984771574,
      "grad_norm": 0.31136736273765564,
      "learning_rate": 6.480743691899071e-05,
      "loss": 0.4834,
      "step": 1190
    },
    {
      "epoch": 0.7166318303971335,
      "grad_norm": 0.29333555698394775,
      "learning_rate": 6.347941567065074e-05,
      "loss": 0.4825,
      "step": 1200
    },
    {
      "epoch": 0.7226037623171095,
      "grad_norm": 0.3149971067905426,
      "learning_rate": 6.215139442231077e-05,
      "loss": 0.4911,
      "step": 1210
    },
    {
      "epoch": 0.7285756942370857,
      "grad_norm": 0.31835949420928955,
      "learning_rate": 6.0823373173970786e-05,
      "loss": 0.398,
      "step": 1220
    },
    {
      "epoch": 0.7345476261570618,
      "grad_norm": 0.32234975695610046,
      "learning_rate": 5.9495351925630814e-05,
      "loss": 0.4268,
      "step": 1230
    },
    {
      "epoch": 0.740519558077038,
      "grad_norm": 0.27742141485214233,
      "learning_rate": 5.816733067729084e-05,
      "loss": 0.4458,
      "step": 1240
    },
    {
      "epoch": 0.746491489997014,
      "grad_norm": 0.3022800385951996,
      "learning_rate": 5.683930942895087e-05,
      "loss": 0.476,
      "step": 1250
    },
    {
      "epoch": 0.7524634219169901,
      "grad_norm": 0.3169964551925659,
      "learning_rate": 5.551128818061089e-05,
      "loss": 0.4759,
      "step": 1260
    },
    {
      "epoch": 0.7584353538369663,
      "grad_norm": 0.310273140668869,
      "learning_rate": 5.418326693227092e-05,
      "loss": 0.4662,
      "step": 1270
    },
    {
      "epoch": 0.7644072857569424,
      "grad_norm": 0.3818434178829193,
      "learning_rate": 5.2855245683930944e-05,
      "loss": 0.4985,
      "step": 1280
    },
    {
      "epoch": 0.7703792176769185,
      "grad_norm": 0.3858293294906616,
      "learning_rate": 5.152722443559097e-05,
      "loss": 0.4552,
      "step": 1290
    },
    {
      "epoch": 0.7763511495968946,
      "grad_norm": 0.2855969965457916,
      "learning_rate": 5.0199203187251e-05,
      "loss": 0.4565,
      "step": 1300
    },
    {
      "epoch": 0.7823230815168707,
      "grad_norm": 0.31481263041496277,
      "learning_rate": 4.8871181938911026e-05,
      "loss": 0.4564,
      "step": 1310
    },
    {
      "epoch": 0.7882950134368468,
      "grad_norm": 0.315727174282074,
      "learning_rate": 4.7543160690571054e-05,
      "loss": 0.4946,
      "step": 1320
    },
    {
      "epoch": 0.794266945356823,
      "grad_norm": 0.2956857979297638,
      "learning_rate": 4.6215139442231074e-05,
      "loss": 0.4222,
      "step": 1330
    },
    {
      "epoch": 0.8002388772767991,
      "grad_norm": 0.32416483759880066,
      "learning_rate": 4.48871181938911e-05,
      "loss": 0.4725,
      "step": 1340
    },
    {
      "epoch": 0.8062108091967751,
      "grad_norm": 0.2828001081943512,
      "learning_rate": 4.355909694555113e-05,
      "loss": 0.445,
      "step": 1350
    },
    {
      "epoch": 0.8121827411167513,
      "grad_norm": 0.32465410232543945,
      "learning_rate": 4.223107569721116e-05,
      "loss": 0.4621,
      "step": 1360
    },
    {
      "epoch": 0.8181546730367274,
      "grad_norm": 0.2961518168449402,
      "learning_rate": 4.0903054448871184e-05,
      "loss": 0.487,
      "step": 1370
    },
    {
      "epoch": 0.8241266049567035,
      "grad_norm": 0.2780632972717285,
      "learning_rate": 3.957503320053121e-05,
      "loss": 0.4903,
      "step": 1380
    },
    {
      "epoch": 0.8300985368766796,
      "grad_norm": 0.31393542885780334,
      "learning_rate": 3.824701195219124e-05,
      "loss": 0.4843,
      "step": 1390
    },
    {
      "epoch": 0.8360704687966557,
      "grad_norm": 0.3603900671005249,
      "learning_rate": 3.6918990703851266e-05,
      "loss": 0.4412,
      "step": 1400
    },
    {
      "epoch": 0.8420424007166318,
      "grad_norm": 0.27677518129348755,
      "learning_rate": 3.5590969455511294e-05,
      "loss": 0.499,
      "step": 1410
    },
    {
      "epoch": 0.848014332636608,
      "grad_norm": 0.2989685833454132,
      "learning_rate": 3.4262948207171314e-05,
      "loss": 0.3949,
      "step": 1420
    },
    {
      "epoch": 0.8539862645565841,
      "grad_norm": 0.256459504365921,
      "learning_rate": 3.293492695883134e-05,
      "loss": 0.4558,
      "step": 1430
    },
    {
      "epoch": 0.8599581964765601,
      "grad_norm": 0.3373161852359772,
      "learning_rate": 3.160690571049137e-05,
      "loss": 0.426,
      "step": 1440
    },
    {
      "epoch": 0.8659301283965363,
      "grad_norm": 0.28815698623657227,
      "learning_rate": 3.0278884462151397e-05,
      "loss": 0.4106,
      "step": 1450
    },
    {
      "epoch": 0.8719020603165124,
      "grad_norm": 0.3105207681655884,
      "learning_rate": 2.8950863213811424e-05,
      "loss": 0.4839,
      "step": 1460
    },
    {
      "epoch": 0.8778739922364885,
      "grad_norm": 0.3213988244533539,
      "learning_rate": 2.7622841965471448e-05,
      "loss": 0.4866,
      "step": 1470
    },
    {
      "epoch": 0.8838459241564646,
      "grad_norm": 0.3050647974014282,
      "learning_rate": 2.6294820717131475e-05,
      "loss": 0.439,
      "step": 1480
    },
    {
      "epoch": 0.8898178560764407,
      "grad_norm": 0.335662841796875,
      "learning_rate": 2.4966799468791503e-05,
      "loss": 0.4624,
      "step": 1490
    },
    {
      "epoch": 0.8957897879964168,
      "grad_norm": 0.3260025680065155,
      "learning_rate": 2.363877822045153e-05,
      "loss": 0.4466,
      "step": 1500
    },
    {
      "epoch": 0.901761719916393,
      "grad_norm": 0.24345038831233978,
      "learning_rate": 2.2310756972111554e-05,
      "loss": 0.4216,
      "step": 1510
    },
    {
      "epoch": 0.9077336518363691,
      "grad_norm": 0.3630465865135193,
      "learning_rate": 2.0982735723771582e-05,
      "loss": 0.4627,
      "step": 1520
    },
    {
      "epoch": 0.9137055837563451,
      "grad_norm": 0.2740810811519623,
      "learning_rate": 1.9654714475431606e-05,
      "loss": 0.4439,
      "step": 1530
    },
    {
      "epoch": 0.9196775156763213,
      "grad_norm": 0.27710890769958496,
      "learning_rate": 1.8326693227091633e-05,
      "loss": 0.4156,
      "step": 1540
    },
    {
      "epoch": 0.9256494475962974,
      "grad_norm": 0.36119401454925537,
      "learning_rate": 1.699867197875166e-05,
      "loss": 0.4625,
      "step": 1550
    },
    {
      "epoch": 0.9316213795162736,
      "grad_norm": 0.30007731914520264,
      "learning_rate": 1.5670650730411688e-05,
      "loss": 0.4268,
      "step": 1560
    },
    {
      "epoch": 0.9375933114362496,
      "grad_norm": 0.302439421415329,
      "learning_rate": 1.4342629482071715e-05,
      "loss": 0.4362,
      "step": 1570
    },
    {
      "epoch": 0.9435652433562257,
      "grad_norm": 0.31534987688064575,
      "learning_rate": 1.301460823373174e-05,
      "loss": 0.4673,
      "step": 1580
    },
    {
      "epoch": 0.9495371752762019,
      "grad_norm": 0.38080036640167236,
      "learning_rate": 1.1686586985391767e-05,
      "loss": 0.4501,
      "step": 1590
    },
    {
      "epoch": 0.955509107196178,
      "grad_norm": 0.360572874546051,
      "learning_rate": 1.0358565737051794e-05,
      "loss": 0.5157,
      "step": 1600
    },
    {
      "epoch": 0.9614810391161541,
      "grad_norm": 0.30405786633491516,
      "learning_rate": 9.03054448871182e-06,
      "loss": 0.4813,
      "step": 1610
    },
    {
      "epoch": 0.9674529710361301,
      "grad_norm": 0.31677931547164917,
      "learning_rate": 7.702523240371846e-06,
      "loss": 0.4552,
      "step": 1620
    },
    {
      "epoch": 0.9734249029561063,
      "grad_norm": 0.31593966484069824,
      "learning_rate": 6.374501992031872e-06,
      "loss": 0.4329,
      "step": 1630
    },
    {
      "epoch": 0.9793968348760824,
      "grad_norm": 0.25822505354881287,
      "learning_rate": 5.046480743691899e-06,
      "loss": 0.5042,
      "step": 1640
    },
    {
      "epoch": 0.9853687667960586,
      "grad_norm": 0.32853594422340393,
      "learning_rate": 3.718459495351926e-06,
      "loss": 0.4204,
      "step": 1650
    },
    {
      "epoch": 0.9913406987160346,
      "grad_norm": 0.33927327394485474,
      "learning_rate": 2.3904382470119524e-06,
      "loss": 0.4499,
      "step": 1660
    },
    {
      "epoch": 0.9973126306360107,
      "grad_norm": 0.30112534761428833,
      "learning_rate": 1.0624169986719788e-06,
      "loss": 0.5071,
      "step": 1670
    }
  ],
  "logging_steps": 10,
  "max_steps": 1674,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.962899711767347e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
