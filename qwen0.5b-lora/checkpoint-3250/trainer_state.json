{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9999230828397816,
  "eval_steps": 500,
  "global_step": 3250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003076686408737789,
      "grad_norm": 120.74886322021484,
      "learning_rate": 4.3076923076923076e-06,
      "loss": 12.4369,
      "step": 10
    },
    {
      "epoch": 0.006153372817475578,
      "grad_norm": 180.07386779785156,
      "learning_rate": 9.846153846153846e-06,
      "loss": 9.8436,
      "step": 20
    },
    {
      "epoch": 0.009230059226213368,
      "grad_norm": 96.82559967041016,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 4.5838,
      "step": 30
    },
    {
      "epoch": 0.012306745634951157,
      "grad_norm": 1.2527142763137817,
      "learning_rate": 2.2153846153846154e-05,
      "loss": 0.6636,
      "step": 40
    },
    {
      "epoch": 0.015383432043688947,
      "grad_norm": 0.8452781438827515,
      "learning_rate": 2.8307692307692306e-05,
      "loss": 0.5072,
      "step": 50
    },
    {
      "epoch": 0.018460118452426736,
      "grad_norm": 0.5381007790565491,
      "learning_rate": 3.446153846153846e-05,
      "loss": 0.4516,
      "step": 60
    },
    {
      "epoch": 0.021536804861164525,
      "grad_norm": 0.4847935438156128,
      "learning_rate": 4.0615384615384615e-05,
      "loss": 0.5009,
      "step": 70
    },
    {
      "epoch": 0.024613491269902314,
      "grad_norm": 0.4446413516998291,
      "learning_rate": 4.676923076923077e-05,
      "loss": 0.4471,
      "step": 80
    },
    {
      "epoch": 0.027690177678640106,
      "grad_norm": 0.4872961938381195,
      "learning_rate": 5.292307692307693e-05,
      "loss": 0.4509,
      "step": 90
    },
    {
      "epoch": 0.030766864087377895,
      "grad_norm": 0.5191770792007446,
      "learning_rate": 5.907692307692309e-05,
      "loss": 0.4292,
      "step": 100
    },
    {
      "epoch": 0.03384355049611568,
      "grad_norm": 0.5253692269325256,
      "learning_rate": 6.523076923076923e-05,
      "loss": 0.4579,
      "step": 110
    },
    {
      "epoch": 0.03692023690485347,
      "grad_norm": 0.4574761986732483,
      "learning_rate": 7.138461538461538e-05,
      "loss": 0.4316,
      "step": 120
    },
    {
      "epoch": 0.03999692331359126,
      "grad_norm": 0.413616418838501,
      "learning_rate": 7.753846153846153e-05,
      "loss": 0.4269,
      "step": 130
    },
    {
      "epoch": 0.04307360972232905,
      "grad_norm": 0.46569666266441345,
      "learning_rate": 8.369230769230769e-05,
      "loss": 0.3917,
      "step": 140
    },
    {
      "epoch": 0.04615029613106684,
      "grad_norm": 0.5550094842910767,
      "learning_rate": 8.984615384615384e-05,
      "loss": 0.4313,
      "step": 150
    },
    {
      "epoch": 0.04922698253980463,
      "grad_norm": 0.47100672125816345,
      "learning_rate": 9.6e-05,
      "loss": 0.3968,
      "step": 160
    },
    {
      "epoch": 0.05230366894854242,
      "grad_norm": 0.5088377594947815,
      "learning_rate": 0.00010215384615384615,
      "loss": 0.4242,
      "step": 170
    },
    {
      "epoch": 0.05538035535728021,
      "grad_norm": 0.45173293352127075,
      "learning_rate": 0.00010830769230769231,
      "loss": 0.4478,
      "step": 180
    },
    {
      "epoch": 0.058457041766018,
      "grad_norm": 0.5405610799789429,
      "learning_rate": 0.00011446153846153846,
      "loss": 0.496,
      "step": 190
    },
    {
      "epoch": 0.06153372817475579,
      "grad_norm": 0.4546821415424347,
      "learning_rate": 0.00012061538461538462,
      "loss": 0.4771,
      "step": 200
    },
    {
      "epoch": 0.06461041458349358,
      "grad_norm": 0.5480431914329529,
      "learning_rate": 0.00012676923076923078,
      "loss": 0.4369,
      "step": 210
    },
    {
      "epoch": 0.06768710099223137,
      "grad_norm": 0.4879983961582184,
      "learning_rate": 0.00013292307692307692,
      "loss": 0.4458,
      "step": 220
    },
    {
      "epoch": 0.07076378740096916,
      "grad_norm": 0.46667253971099854,
      "learning_rate": 0.00013907692307692308,
      "loss": 0.4607,
      "step": 230
    },
    {
      "epoch": 0.07384047380970694,
      "grad_norm": 0.4551909267902374,
      "learning_rate": 0.00014523076923076924,
      "loss": 0.4313,
      "step": 240
    },
    {
      "epoch": 0.07691716021844473,
      "grad_norm": 0.45894956588745117,
      "learning_rate": 0.0001513846153846154,
      "loss": 0.4703,
      "step": 250
    },
    {
      "epoch": 0.07999384662718252,
      "grad_norm": 0.4730789065361023,
      "learning_rate": 0.00015753846153846154,
      "loss": 0.4459,
      "step": 260
    },
    {
      "epoch": 0.08307053303592031,
      "grad_norm": 0.4377472698688507,
      "learning_rate": 0.0001636923076923077,
      "loss": 0.4084,
      "step": 270
    },
    {
      "epoch": 0.0861472194446581,
      "grad_norm": 0.4747014045715332,
      "learning_rate": 0.00016984615384615386,
      "loss": 0.4758,
      "step": 280
    },
    {
      "epoch": 0.08922390585339589,
      "grad_norm": 0.35840946435928345,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.3797,
      "step": 290
    },
    {
      "epoch": 0.09230059226213368,
      "grad_norm": 0.5252364873886108,
      "learning_rate": 0.00018215384615384616,
      "loss": 0.4757,
      "step": 300
    },
    {
      "epoch": 0.09537727867087147,
      "grad_norm": 0.34567147493362427,
      "learning_rate": 0.00018830769230769232,
      "loss": 0.4619,
      "step": 310
    },
    {
      "epoch": 0.09845396507960925,
      "grad_norm": 0.4011288285255432,
      "learning_rate": 0.00019446153846153848,
      "loss": 0.4534,
      "step": 320
    },
    {
      "epoch": 0.10153065148834706,
      "grad_norm": 0.3955449163913727,
      "learning_rate": 0.00019993162393162396,
      "loss": 0.4156,
      "step": 330
    },
    {
      "epoch": 0.10460733789708485,
      "grad_norm": 0.3772217929363251,
      "learning_rate": 0.00019924786324786327,
      "loss": 0.4448,
      "step": 340
    },
    {
      "epoch": 0.10768402430582263,
      "grad_norm": 0.4383673369884491,
      "learning_rate": 0.0001985641025641026,
      "loss": 0.4517,
      "step": 350
    },
    {
      "epoch": 0.11076071071456042,
      "grad_norm": 0.36705705523490906,
      "learning_rate": 0.00019788034188034188,
      "loss": 0.4111,
      "step": 360
    },
    {
      "epoch": 0.11383739712329821,
      "grad_norm": 0.4732646942138672,
      "learning_rate": 0.0001971965811965812,
      "loss": 0.4876,
      "step": 370
    },
    {
      "epoch": 0.116914083532036,
      "grad_norm": 0.40494683384895325,
      "learning_rate": 0.0001965128205128205,
      "loss": 0.4418,
      "step": 380
    },
    {
      "epoch": 0.11999076994077379,
      "grad_norm": 0.31891217827796936,
      "learning_rate": 0.00019582905982905982,
      "loss": 0.4661,
      "step": 390
    },
    {
      "epoch": 0.12306745634951158,
      "grad_norm": 0.38369542360305786,
      "learning_rate": 0.00019514529914529916,
      "loss": 0.4552,
      "step": 400
    },
    {
      "epoch": 0.12614414275824937,
      "grad_norm": 0.3550228178501129,
      "learning_rate": 0.00019446153846153848,
      "loss": 0.4279,
      "step": 410
    },
    {
      "epoch": 0.12922082916698716,
      "grad_norm": 0.4344315230846405,
      "learning_rate": 0.0001937777777777778,
      "loss": 0.4163,
      "step": 420
    },
    {
      "epoch": 0.13229751557572494,
      "grad_norm": 0.34611308574676514,
      "learning_rate": 0.0001930940170940171,
      "loss": 0.4095,
      "step": 430
    },
    {
      "epoch": 0.13537420198446273,
      "grad_norm": 0.4009982943534851,
      "learning_rate": 0.00019241025641025643,
      "loss": 0.4565,
      "step": 440
    },
    {
      "epoch": 0.13845088839320052,
      "grad_norm": 0.4199123680591583,
      "learning_rate": 0.00019172649572649571,
      "loss": 0.3791,
      "step": 450
    },
    {
      "epoch": 0.1415275748019383,
      "grad_norm": 0.5771690607070923,
      "learning_rate": 0.00019104273504273506,
      "loss": 0.5083,
      "step": 460
    },
    {
      "epoch": 0.1446042612106761,
      "grad_norm": 0.3338198959827423,
      "learning_rate": 0.00019035897435897437,
      "loss": 0.4423,
      "step": 470
    },
    {
      "epoch": 0.1476809476194139,
      "grad_norm": 0.34340402483940125,
      "learning_rate": 0.0001896752136752137,
      "loss": 0.4161,
      "step": 480
    },
    {
      "epoch": 0.15075763402815168,
      "grad_norm": 0.3484652042388916,
      "learning_rate": 0.000188991452991453,
      "loss": 0.4386,
      "step": 490
    },
    {
      "epoch": 0.15383432043688947,
      "grad_norm": 0.3519359230995178,
      "learning_rate": 0.00018830769230769232,
      "loss": 0.4428,
      "step": 500
    },
    {
      "epoch": 0.15691100684562725,
      "grad_norm": 0.37358811497688293,
      "learning_rate": 0.00018762393162393163,
      "loss": 0.4196,
      "step": 510
    },
    {
      "epoch": 0.15998769325436504,
      "grad_norm": 0.3936792314052582,
      "learning_rate": 0.00018694017094017095,
      "loss": 0.4222,
      "step": 520
    },
    {
      "epoch": 0.16306437966310283,
      "grad_norm": 0.38426387310028076,
      "learning_rate": 0.00018625641025641026,
      "loss": 0.4544,
      "step": 530
    },
    {
      "epoch": 0.16614106607184062,
      "grad_norm": 0.3963180482387543,
      "learning_rate": 0.00018557264957264958,
      "loss": 0.4543,
      "step": 540
    },
    {
      "epoch": 0.1692177524805784,
      "grad_norm": 0.4152660071849823,
      "learning_rate": 0.0001848888888888889,
      "loss": 0.4612,
      "step": 550
    },
    {
      "epoch": 0.1722944388893162,
      "grad_norm": 0.39149704575538635,
      "learning_rate": 0.0001842051282051282,
      "loss": 0.4278,
      "step": 560
    },
    {
      "epoch": 0.175371125298054,
      "grad_norm": 0.43974319100379944,
      "learning_rate": 0.00018352136752136753,
      "loss": 0.4793,
      "step": 570
    },
    {
      "epoch": 0.17844781170679178,
      "grad_norm": 0.40668970346450806,
      "learning_rate": 0.00018283760683760684,
      "loss": 0.4312,
      "step": 580
    },
    {
      "epoch": 0.18152449811552956,
      "grad_norm": 0.3654879033565521,
      "learning_rate": 0.00018215384615384616,
      "loss": 0.4578,
      "step": 590
    },
    {
      "epoch": 0.18460118452426735,
      "grad_norm": 0.40481820702552795,
      "learning_rate": 0.00018147008547008547,
      "loss": 0.4696,
      "step": 600
    },
    {
      "epoch": 0.18767787093300514,
      "grad_norm": 0.3251613974571228,
      "learning_rate": 0.0001807863247863248,
      "loss": 0.4105,
      "step": 610
    },
    {
      "epoch": 0.19075455734174293,
      "grad_norm": 0.35225793719291687,
      "learning_rate": 0.00018010256410256413,
      "loss": 0.4038,
      "step": 620
    },
    {
      "epoch": 0.19383124375048072,
      "grad_norm": 0.3708897829055786,
      "learning_rate": 0.00017941880341880345,
      "loss": 0.4521,
      "step": 630
    },
    {
      "epoch": 0.1969079301592185,
      "grad_norm": 0.4001448452472687,
      "learning_rate": 0.00017873504273504273,
      "loss": 0.4335,
      "step": 640
    },
    {
      "epoch": 0.19998461656795632,
      "grad_norm": 0.35732921957969666,
      "learning_rate": 0.00017805128205128205,
      "loss": 0.4329,
      "step": 650
    },
    {
      "epoch": 0.2030613029766941,
      "grad_norm": 0.39901575446128845,
      "learning_rate": 0.00017736752136752136,
      "loss": 0.5054,
      "step": 660
    },
    {
      "epoch": 0.2061379893854319,
      "grad_norm": 0.34610748291015625,
      "learning_rate": 0.00017668376068376068,
      "loss": 0.4587,
      "step": 670
    },
    {
      "epoch": 0.2092146757941697,
      "grad_norm": 0.320634663105011,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.3896,
      "step": 680
    },
    {
      "epoch": 0.21229136220290748,
      "grad_norm": 0.3793400228023529,
      "learning_rate": 0.00017531623931623934,
      "loss": 0.4366,
      "step": 690
    },
    {
      "epoch": 0.21536804861164527,
      "grad_norm": 0.3242475986480713,
      "learning_rate": 0.00017463247863247865,
      "loss": 0.4224,
      "step": 700
    },
    {
      "epoch": 0.21844473502038306,
      "grad_norm": 0.4221375286579132,
      "learning_rate": 0.00017394871794871797,
      "loss": 0.4714,
      "step": 710
    },
    {
      "epoch": 0.22152142142912085,
      "grad_norm": 0.3683077096939087,
      "learning_rate": 0.00017326495726495728,
      "loss": 0.4945,
      "step": 720
    },
    {
      "epoch": 0.22459810783785863,
      "grad_norm": 0.32813936471939087,
      "learning_rate": 0.00017258119658119657,
      "loss": 0.4416,
      "step": 730
    },
    {
      "epoch": 0.22767479424659642,
      "grad_norm": 0.31129640340805054,
      "learning_rate": 0.0001718974358974359,
      "loss": 0.4149,
      "step": 740
    },
    {
      "epoch": 0.2307514806553342,
      "grad_norm": 0.3856489360332489,
      "learning_rate": 0.00017121367521367523,
      "loss": 0.4064,
      "step": 750
    },
    {
      "epoch": 0.233828167064072,
      "grad_norm": 0.35621944069862366,
      "learning_rate": 0.00017052991452991455,
      "loss": 0.4976,
      "step": 760
    },
    {
      "epoch": 0.2369048534728098,
      "grad_norm": 0.36542001366615295,
      "learning_rate": 0.00016984615384615386,
      "loss": 0.4785,
      "step": 770
    },
    {
      "epoch": 0.23998153988154758,
      "grad_norm": 0.3509456217288971,
      "learning_rate": 0.00016916239316239318,
      "loss": 0.4122,
      "step": 780
    },
    {
      "epoch": 0.24305822629028537,
      "grad_norm": 0.40409061312675476,
      "learning_rate": 0.0001684786324786325,
      "loss": 0.4384,
      "step": 790
    },
    {
      "epoch": 0.24613491269902316,
      "grad_norm": 0.36316996812820435,
      "learning_rate": 0.0001677948717948718,
      "loss": 0.4107,
      "step": 800
    },
    {
      "epoch": 0.24921159910776094,
      "grad_norm": 0.3222857415676117,
      "learning_rate": 0.00016711111111111112,
      "loss": 0.4097,
      "step": 810
    },
    {
      "epoch": 0.25228828551649873,
      "grad_norm": 0.3759448826313019,
      "learning_rate": 0.00016642735042735044,
      "loss": 0.4797,
      "step": 820
    },
    {
      "epoch": 0.2553649719252365,
      "grad_norm": 0.3361353278160095,
      "learning_rate": 0.00016574358974358975,
      "loss": 0.4491,
      "step": 830
    },
    {
      "epoch": 0.2584416583339743,
      "grad_norm": 0.3655068278312683,
      "learning_rate": 0.00016505982905982907,
      "loss": 0.4428,
      "step": 840
    },
    {
      "epoch": 0.26151834474271207,
      "grad_norm": 0.3463532030582428,
      "learning_rate": 0.00016437606837606838,
      "loss": 0.421,
      "step": 850
    },
    {
      "epoch": 0.2645950311514499,
      "grad_norm": 0.3692874610424042,
      "learning_rate": 0.0001636923076923077,
      "loss": 0.3788,
      "step": 860
    },
    {
      "epoch": 0.2676717175601877,
      "grad_norm": 0.2506142258644104,
      "learning_rate": 0.00016300854700854701,
      "loss": 0.4205,
      "step": 870
    },
    {
      "epoch": 0.27074840396892547,
      "grad_norm": 0.369428426027298,
      "learning_rate": 0.00016232478632478633,
      "loss": 0.4197,
      "step": 880
    },
    {
      "epoch": 0.2738250903776633,
      "grad_norm": 0.35067513585090637,
      "learning_rate": 0.00016164102564102565,
      "loss": 0.4076,
      "step": 890
    },
    {
      "epoch": 0.27690177678640104,
      "grad_norm": 0.3140096068382263,
      "learning_rate": 0.000160957264957265,
      "loss": 0.3983,
      "step": 900
    },
    {
      "epoch": 0.27997846319513886,
      "grad_norm": 0.36275172233581543,
      "learning_rate": 0.00016027350427350428,
      "loss": 0.4009,
      "step": 910
    },
    {
      "epoch": 0.2830551496038766,
      "grad_norm": 0.3734900653362274,
      "learning_rate": 0.0001595897435897436,
      "loss": 0.4463,
      "step": 920
    },
    {
      "epoch": 0.28613183601261444,
      "grad_norm": 0.3011917471885681,
      "learning_rate": 0.0001589059829059829,
      "loss": 0.3778,
      "step": 930
    },
    {
      "epoch": 0.2892085224213522,
      "grad_norm": 0.3325687646865845,
      "learning_rate": 0.00015822222222222222,
      "loss": 0.4299,
      "step": 940
    },
    {
      "epoch": 0.29228520883009,
      "grad_norm": 0.3434043228626251,
      "learning_rate": 0.00015753846153846154,
      "loss": 0.3948,
      "step": 950
    },
    {
      "epoch": 0.2953618952388278,
      "grad_norm": 0.33001628518104553,
      "learning_rate": 0.00015685470085470085,
      "loss": 0.4423,
      "step": 960
    },
    {
      "epoch": 0.2984385816475656,
      "grad_norm": 0.38783887028694153,
      "learning_rate": 0.0001561709401709402,
      "loss": 0.4374,
      "step": 970
    },
    {
      "epoch": 0.30151526805630335,
      "grad_norm": 0.3817467987537384,
      "learning_rate": 0.0001554871794871795,
      "loss": 0.4693,
      "step": 980
    },
    {
      "epoch": 0.30459195446504117,
      "grad_norm": 0.33016321063041687,
      "learning_rate": 0.00015480341880341883,
      "loss": 0.4322,
      "step": 990
    },
    {
      "epoch": 0.30766864087377893,
      "grad_norm": 0.3874586522579193,
      "learning_rate": 0.00015411965811965811,
      "loss": 0.4181,
      "step": 1000
    },
    {
      "epoch": 0.31074532728251675,
      "grad_norm": 0.41581231355667114,
      "learning_rate": 0.00015343589743589743,
      "loss": 0.4779,
      "step": 1010
    },
    {
      "epoch": 0.3138220136912545,
      "grad_norm": 0.37142568826675415,
      "learning_rate": 0.00015275213675213675,
      "loss": 0.4457,
      "step": 1020
    },
    {
      "epoch": 0.3168987000999923,
      "grad_norm": 0.3774667978286743,
      "learning_rate": 0.00015206837606837606,
      "loss": 0.422,
      "step": 1030
    },
    {
      "epoch": 0.3199753865087301,
      "grad_norm": 0.3163091838359833,
      "learning_rate": 0.0001513846153846154,
      "loss": 0.4381,
      "step": 1040
    },
    {
      "epoch": 0.3230520729174679,
      "grad_norm": 0.370370477437973,
      "learning_rate": 0.00015070085470085472,
      "loss": 0.427,
      "step": 1050
    },
    {
      "epoch": 0.32612875932620566,
      "grad_norm": 0.3497225344181061,
      "learning_rate": 0.00015001709401709403,
      "loss": 0.4319,
      "step": 1060
    },
    {
      "epoch": 0.3292054457349435,
      "grad_norm": 0.2727712392807007,
      "learning_rate": 0.00014933333333333335,
      "loss": 0.3864,
      "step": 1070
    },
    {
      "epoch": 0.33228213214368124,
      "grad_norm": 0.4396885633468628,
      "learning_rate": 0.00014864957264957266,
      "loss": 0.4707,
      "step": 1080
    },
    {
      "epoch": 0.33535881855241906,
      "grad_norm": 0.38429713249206543,
      "learning_rate": 0.00014796581196581195,
      "loss": 0.4412,
      "step": 1090
    },
    {
      "epoch": 0.3384355049611568,
      "grad_norm": 0.3675699830055237,
      "learning_rate": 0.00014728205128205127,
      "loss": 0.425,
      "step": 1100
    },
    {
      "epoch": 0.34151219136989464,
      "grad_norm": 0.33042964339256287,
      "learning_rate": 0.0001465982905982906,
      "loss": 0.4476,
      "step": 1110
    },
    {
      "epoch": 0.3445888777786324,
      "grad_norm": 0.32851850986480713,
      "learning_rate": 0.00014591452991452993,
      "loss": 0.407,
      "step": 1120
    },
    {
      "epoch": 0.3476655641873702,
      "grad_norm": 0.33445873856544495,
      "learning_rate": 0.00014523076923076924,
      "loss": 0.4217,
      "step": 1130
    },
    {
      "epoch": 0.350742250596108,
      "grad_norm": 0.3523169159889221,
      "learning_rate": 0.00014454700854700856,
      "loss": 0.4336,
      "step": 1140
    },
    {
      "epoch": 0.3538189370048458,
      "grad_norm": 0.30471983551979065,
      "learning_rate": 0.00014386324786324787,
      "loss": 0.4957,
      "step": 1150
    },
    {
      "epoch": 0.35689562341358355,
      "grad_norm": 0.3550202548503876,
      "learning_rate": 0.0001431794871794872,
      "loss": 0.463,
      "step": 1160
    },
    {
      "epoch": 0.35997230982232137,
      "grad_norm": 0.4044046401977539,
      "learning_rate": 0.0001424957264957265,
      "loss": 0.4555,
      "step": 1170
    },
    {
      "epoch": 0.36304899623105913,
      "grad_norm": 0.3452017903327942,
      "learning_rate": 0.00014181196581196582,
      "loss": 0.4461,
      "step": 1180
    },
    {
      "epoch": 0.36612568263979695,
      "grad_norm": 0.31203627586364746,
      "learning_rate": 0.00014112820512820513,
      "loss": 0.4269,
      "step": 1190
    },
    {
      "epoch": 0.3692023690485347,
      "grad_norm": 0.32118943333625793,
      "learning_rate": 0.00014044444444444445,
      "loss": 0.4131,
      "step": 1200
    },
    {
      "epoch": 0.3722790554572725,
      "grad_norm": 0.34128040075302124,
      "learning_rate": 0.00013976068376068376,
      "loss": 0.4188,
      "step": 1210
    },
    {
      "epoch": 0.3753557418660103,
      "grad_norm": 0.34387460350990295,
      "learning_rate": 0.00013907692307692308,
      "loss": 0.4373,
      "step": 1220
    },
    {
      "epoch": 0.3784324282747481,
      "grad_norm": 0.39229294657707214,
      "learning_rate": 0.0001383931623931624,
      "loss": 0.4152,
      "step": 1230
    },
    {
      "epoch": 0.38150911468348586,
      "grad_norm": 0.42430898547172546,
      "learning_rate": 0.0001377094017094017,
      "loss": 0.4213,
      "step": 1240
    },
    {
      "epoch": 0.3845858010922237,
      "grad_norm": 0.38399335741996765,
      "learning_rate": 0.00013702564102564103,
      "loss": 0.4516,
      "step": 1250
    },
    {
      "epoch": 0.38766248750096144,
      "grad_norm": 0.2797500491142273,
      "learning_rate": 0.00013634188034188037,
      "loss": 0.4225,
      "step": 1260
    },
    {
      "epoch": 0.39073917390969926,
      "grad_norm": 0.3407635986804962,
      "learning_rate": 0.00013565811965811968,
      "loss": 0.4263,
      "step": 1270
    },
    {
      "epoch": 0.393815860318437,
      "grad_norm": 0.3066199719905853,
      "learning_rate": 0.00013497435897435897,
      "loss": 0.4651,
      "step": 1280
    },
    {
      "epoch": 0.39689254672717483,
      "grad_norm": 0.34886977076530457,
      "learning_rate": 0.0001342905982905983,
      "loss": 0.4419,
      "step": 1290
    },
    {
      "epoch": 0.39996923313591265,
      "grad_norm": 0.30868539214134216,
      "learning_rate": 0.0001336068376068376,
      "loss": 0.4167,
      "step": 1300
    },
    {
      "epoch": 0.4030459195446504,
      "grad_norm": 0.3036375045776367,
      "learning_rate": 0.00013292307692307692,
      "loss": 0.4354,
      "step": 1310
    },
    {
      "epoch": 0.4061226059533882,
      "grad_norm": 0.38962921500205994,
      "learning_rate": 0.00013223931623931623,
      "loss": 0.4581,
      "step": 1320
    },
    {
      "epoch": 0.409199292362126,
      "grad_norm": 0.3121672570705414,
      "learning_rate": 0.00013155555555555558,
      "loss": 0.4404,
      "step": 1330
    },
    {
      "epoch": 0.4122759787708638,
      "grad_norm": 0.38057586550712585,
      "learning_rate": 0.0001308717948717949,
      "loss": 0.4574,
      "step": 1340
    },
    {
      "epoch": 0.41535266517960157,
      "grad_norm": 0.35652032494544983,
      "learning_rate": 0.0001301880341880342,
      "loss": 0.4206,
      "step": 1350
    },
    {
      "epoch": 0.4184293515883394,
      "grad_norm": 0.38355979323387146,
      "learning_rate": 0.00012950427350427352,
      "loss": 0.4524,
      "step": 1360
    },
    {
      "epoch": 0.42150603799707714,
      "grad_norm": 0.3817201554775238,
      "learning_rate": 0.0001288205128205128,
      "loss": 0.4274,
      "step": 1370
    },
    {
      "epoch": 0.42458272440581496,
      "grad_norm": 0.4155656099319458,
      "learning_rate": 0.00012813675213675213,
      "loss": 0.4163,
      "step": 1380
    },
    {
      "epoch": 0.4276594108145527,
      "grad_norm": 0.31180688738822937,
      "learning_rate": 0.00012745299145299147,
      "loss": 0.4134,
      "step": 1390
    },
    {
      "epoch": 0.43073609722329054,
      "grad_norm": 0.3652452528476715,
      "learning_rate": 0.00012676923076923078,
      "loss": 0.4657,
      "step": 1400
    },
    {
      "epoch": 0.4338127836320283,
      "grad_norm": 0.3065323233604431,
      "learning_rate": 0.0001260854700854701,
      "loss": 0.4176,
      "step": 1410
    },
    {
      "epoch": 0.4368894700407661,
      "grad_norm": 0.3667019009590149,
      "learning_rate": 0.00012540170940170941,
      "loss": 0.447,
      "step": 1420
    },
    {
      "epoch": 0.4399661564495039,
      "grad_norm": 0.3341561257839203,
      "learning_rate": 0.00012471794871794873,
      "loss": 0.4298,
      "step": 1430
    },
    {
      "epoch": 0.4430428428582417,
      "grad_norm": 0.3707675337791443,
      "learning_rate": 0.00012403418803418805,
      "loss": 0.4135,
      "step": 1440
    },
    {
      "epoch": 0.44611952926697945,
      "grad_norm": 0.3287374973297119,
      "learning_rate": 0.00012335042735042736,
      "loss": 0.4678,
      "step": 1450
    },
    {
      "epoch": 0.44919621567571727,
      "grad_norm": 0.3259797692298889,
      "learning_rate": 0.00012266666666666668,
      "loss": 0.4329,
      "step": 1460
    },
    {
      "epoch": 0.45227290208445503,
      "grad_norm": 0.25938576459884644,
      "learning_rate": 0.00012198290598290598,
      "loss": 0.4103,
      "step": 1470
    },
    {
      "epoch": 0.45534958849319285,
      "grad_norm": 0.37207624316215515,
      "learning_rate": 0.0001212991452991453,
      "loss": 0.476,
      "step": 1480
    },
    {
      "epoch": 0.4584262749019306,
      "grad_norm": 0.35610222816467285,
      "learning_rate": 0.00012061538461538462,
      "loss": 0.4702,
      "step": 1490
    },
    {
      "epoch": 0.4615029613106684,
      "grad_norm": 0.3430788218975067,
      "learning_rate": 0.00011993162393162394,
      "loss": 0.4486,
      "step": 1500
    },
    {
      "epoch": 0.4645796477194062,
      "grad_norm": 0.3552122116088867,
      "learning_rate": 0.00011924786324786325,
      "loss": 0.3785,
      "step": 1510
    },
    {
      "epoch": 0.467656334128144,
      "grad_norm": 0.28891873359680176,
      "learning_rate": 0.00011856410256410258,
      "loss": 0.4276,
      "step": 1520
    },
    {
      "epoch": 0.47073302053688176,
      "grad_norm": 0.36344239115715027,
      "learning_rate": 0.0001178803418803419,
      "loss": 0.4527,
      "step": 1530
    },
    {
      "epoch": 0.4738097069456196,
      "grad_norm": 0.39363864064216614,
      "learning_rate": 0.00011719658119658121,
      "loss": 0.4532,
      "step": 1540
    },
    {
      "epoch": 0.47688639335435734,
      "grad_norm": 0.3933519124984741,
      "learning_rate": 0.00011651282051282051,
      "loss": 0.4689,
      "step": 1550
    },
    {
      "epoch": 0.47996307976309516,
      "grad_norm": 0.3290753662586212,
      "learning_rate": 0.00011582905982905983,
      "loss": 0.3804,
      "step": 1560
    },
    {
      "epoch": 0.4830397661718329,
      "grad_norm": 0.3830421566963196,
      "learning_rate": 0.00011514529914529914,
      "loss": 0.4725,
      "step": 1570
    },
    {
      "epoch": 0.48611645258057073,
      "grad_norm": 0.3615044355392456,
      "learning_rate": 0.00011446153846153846,
      "loss": 0.4056,
      "step": 1580
    },
    {
      "epoch": 0.4891931389893085,
      "grad_norm": 0.32144999504089355,
      "learning_rate": 0.00011377777777777779,
      "loss": 0.4135,
      "step": 1590
    },
    {
      "epoch": 0.4922698253980463,
      "grad_norm": 0.32507187128067017,
      "learning_rate": 0.0001130940170940171,
      "loss": 0.4329,
      "step": 1600
    },
    {
      "epoch": 0.4953465118067841,
      "grad_norm": 0.3696187436580658,
      "learning_rate": 0.00011241025641025642,
      "loss": 0.4073,
      "step": 1610
    },
    {
      "epoch": 0.4984231982155219,
      "grad_norm": 0.3024081587791443,
      "learning_rate": 0.00011172649572649574,
      "loss": 0.4203,
      "step": 1620
    },
    {
      "epoch": 0.5014998846242597,
      "grad_norm": 0.2537505030632019,
      "learning_rate": 0.00011104273504273506,
      "loss": 0.4235,
      "step": 1630
    },
    {
      "epoch": 0.5045765710329975,
      "grad_norm": 0.3123423755168915,
      "learning_rate": 0.00011035897435897435,
      "loss": 0.4294,
      "step": 1640
    },
    {
      "epoch": 0.5076532574417353,
      "grad_norm": 0.3590226471424103,
      "learning_rate": 0.00010967521367521367,
      "loss": 0.4722,
      "step": 1650
    },
    {
      "epoch": 0.510729943850473,
      "grad_norm": 0.3425900936126709,
      "learning_rate": 0.000108991452991453,
      "loss": 0.4808,
      "step": 1660
    },
    {
      "epoch": 0.5138066302592108,
      "grad_norm": 0.2928889989852905,
      "learning_rate": 0.00010830769230769231,
      "loss": 0.4057,
      "step": 1670
    },
    {
      "epoch": 0.5168833166679486,
      "grad_norm": 0.2722553312778473,
      "learning_rate": 0.00010762393162393163,
      "loss": 0.4183,
      "step": 1680
    },
    {
      "epoch": 0.5199600030766864,
      "grad_norm": 0.36563652753829956,
      "learning_rate": 0.00010694017094017094,
      "loss": 0.4391,
      "step": 1690
    },
    {
      "epoch": 0.5230366894854241,
      "grad_norm": 0.3992033898830414,
      "learning_rate": 0.00010625641025641027,
      "loss": 0.4082,
      "step": 1700
    },
    {
      "epoch": 0.526113375894162,
      "grad_norm": 0.34471824765205383,
      "learning_rate": 0.00010557264957264959,
      "loss": 0.4089,
      "step": 1710
    },
    {
      "epoch": 0.5291900623028998,
      "grad_norm": 0.3419985771179199,
      "learning_rate": 0.0001048888888888889,
      "loss": 0.4259,
      "step": 1720
    },
    {
      "epoch": 0.5322667487116376,
      "grad_norm": 0.3151775896549225,
      "learning_rate": 0.0001042051282051282,
      "loss": 0.4632,
      "step": 1730
    },
    {
      "epoch": 0.5353434351203754,
      "grad_norm": 0.3163243234157562,
      "learning_rate": 0.00010352136752136752,
      "loss": 0.4422,
      "step": 1740
    },
    {
      "epoch": 0.5384201215291131,
      "grad_norm": 0.36181262135505676,
      "learning_rate": 0.00010283760683760684,
      "loss": 0.4615,
      "step": 1750
    },
    {
      "epoch": 0.5414968079378509,
      "grad_norm": 0.34331586956977844,
      "learning_rate": 0.00010215384615384615,
      "loss": 0.4397,
      "step": 1760
    },
    {
      "epoch": 0.5445734943465887,
      "grad_norm": 0.3496076464653015,
      "learning_rate": 0.00010147008547008548,
      "loss": 0.4474,
      "step": 1770
    },
    {
      "epoch": 0.5476501807553266,
      "grad_norm": 0.3681075870990753,
      "learning_rate": 0.0001007863247863248,
      "loss": 0.4334,
      "step": 1780
    },
    {
      "epoch": 0.5507268671640643,
      "grad_norm": 0.31244176626205444,
      "learning_rate": 0.00010010256410256411,
      "loss": 0.4288,
      "step": 1790
    },
    {
      "epoch": 0.5538035535728021,
      "grad_norm": 0.33446377515792847,
      "learning_rate": 9.941880341880343e-05,
      "loss": 0.3938,
      "step": 1800
    },
    {
      "epoch": 0.5568802399815399,
      "grad_norm": 0.3884088099002838,
      "learning_rate": 9.873504273504274e-05,
      "loss": 0.4432,
      "step": 1810
    },
    {
      "epoch": 0.5599569263902777,
      "grad_norm": 0.32492607831954956,
      "learning_rate": 9.805128205128206e-05,
      "loss": 0.4501,
      "step": 1820
    },
    {
      "epoch": 0.5630336127990154,
      "grad_norm": 0.3300338089466095,
      "learning_rate": 9.736752136752137e-05,
      "loss": 0.4212,
      "step": 1830
    },
    {
      "epoch": 0.5661102992077532,
      "grad_norm": 0.35738542675971985,
      "learning_rate": 9.668376068376069e-05,
      "loss": 0.4382,
      "step": 1840
    },
    {
      "epoch": 0.5691869856164911,
      "grad_norm": 0.3351295590400696,
      "learning_rate": 9.6e-05,
      "loss": 0.4221,
      "step": 1850
    },
    {
      "epoch": 0.5722636720252289,
      "grad_norm": 0.355238676071167,
      "learning_rate": 9.531623931623932e-05,
      "loss": 0.4561,
      "step": 1860
    },
    {
      "epoch": 0.5753403584339666,
      "grad_norm": 0.2936447858810425,
      "learning_rate": 9.463247863247863e-05,
      "loss": 0.4362,
      "step": 1870
    },
    {
      "epoch": 0.5784170448427044,
      "grad_norm": 0.33588913083076477,
      "learning_rate": 9.394871794871796e-05,
      "loss": 0.4179,
      "step": 1880
    },
    {
      "epoch": 0.5814937312514422,
      "grad_norm": 0.2866966724395752,
      "learning_rate": 9.326495726495726e-05,
      "loss": 0.4338,
      "step": 1890
    },
    {
      "epoch": 0.58457041766018,
      "grad_norm": 0.3437983989715576,
      "learning_rate": 9.258119658119658e-05,
      "loss": 0.4322,
      "step": 1900
    },
    {
      "epoch": 0.5876471040689177,
      "grad_norm": 0.3364243507385254,
      "learning_rate": 9.189743589743591e-05,
      "loss": 0.4253,
      "step": 1910
    },
    {
      "epoch": 0.5907237904776556,
      "grad_norm": 0.3435441553592682,
      "learning_rate": 9.121367521367522e-05,
      "loss": 0.4183,
      "step": 1920
    },
    {
      "epoch": 0.5938004768863934,
      "grad_norm": 0.3319183588027954,
      "learning_rate": 9.052991452991454e-05,
      "loss": 0.4346,
      "step": 1930
    },
    {
      "epoch": 0.5968771632951312,
      "grad_norm": 0.3784755766391754,
      "learning_rate": 8.984615384615384e-05,
      "loss": 0.4476,
      "step": 1940
    },
    {
      "epoch": 0.5999538497038689,
      "grad_norm": 0.35713648796081543,
      "learning_rate": 8.916239316239317e-05,
      "loss": 0.4487,
      "step": 1950
    },
    {
      "epoch": 0.6030305361126067,
      "grad_norm": 0.3166540563106537,
      "learning_rate": 8.847863247863249e-05,
      "loss": 0.3655,
      "step": 1960
    },
    {
      "epoch": 0.6061072225213445,
      "grad_norm": 0.3341945707798004,
      "learning_rate": 8.77948717948718e-05,
      "loss": 0.4602,
      "step": 1970
    },
    {
      "epoch": 0.6091839089300823,
      "grad_norm": 0.2642599940299988,
      "learning_rate": 8.711111111111112e-05,
      "loss": 0.4759,
      "step": 1980
    },
    {
      "epoch": 0.61226059533882,
      "grad_norm": 0.34994804859161377,
      "learning_rate": 8.642735042735043e-05,
      "loss": 0.4392,
      "step": 1990
    },
    {
      "epoch": 0.6153372817475579,
      "grad_norm": 0.375568687915802,
      "learning_rate": 8.574358974358975e-05,
      "loss": 0.4138,
      "step": 2000
    },
    {
      "epoch": 0.6184139681562957,
      "grad_norm": 0.3197311460971832,
      "learning_rate": 8.505982905982906e-05,
      "loss": 0.4423,
      "step": 2010
    },
    {
      "epoch": 0.6214906545650335,
      "grad_norm": 0.3590933084487915,
      "learning_rate": 8.437606837606839e-05,
      "loss": 0.4244,
      "step": 2020
    },
    {
      "epoch": 0.6245673409737712,
      "grad_norm": 0.30618253350257874,
      "learning_rate": 8.369230769230769e-05,
      "loss": 0.4065,
      "step": 2030
    },
    {
      "epoch": 0.627644027382509,
      "grad_norm": 0.31575942039489746,
      "learning_rate": 8.300854700854701e-05,
      "loss": 0.4392,
      "step": 2040
    },
    {
      "epoch": 0.6307207137912468,
      "grad_norm": 0.36704933643341064,
      "learning_rate": 8.232478632478632e-05,
      "loss": 0.4092,
      "step": 2050
    },
    {
      "epoch": 0.6337974001999847,
      "grad_norm": 0.37017250061035156,
      "learning_rate": 8.164102564102565e-05,
      "loss": 0.47,
      "step": 2060
    },
    {
      "epoch": 0.6368740866087224,
      "grad_norm": 0.3473303020000458,
      "learning_rate": 8.095726495726497e-05,
      "loss": 0.4714,
      "step": 2070
    },
    {
      "epoch": 0.6399507730174602,
      "grad_norm": 0.287188857793808,
      "learning_rate": 8.027350427350427e-05,
      "loss": 0.3528,
      "step": 2080
    },
    {
      "epoch": 0.643027459426198,
      "grad_norm": 0.28711920976638794,
      "learning_rate": 7.95897435897436e-05,
      "loss": 0.4186,
      "step": 2090
    },
    {
      "epoch": 0.6461041458349358,
      "grad_norm": 0.2975853383541107,
      "learning_rate": 7.890598290598291e-05,
      "loss": 0.3932,
      "step": 2100
    },
    {
      "epoch": 0.6491808322436735,
      "grad_norm": 0.3408222496509552,
      "learning_rate": 7.822222222222223e-05,
      "loss": 0.4075,
      "step": 2110
    },
    {
      "epoch": 0.6522575186524113,
      "grad_norm": 0.3358583450317383,
      "learning_rate": 7.753846153846153e-05,
      "loss": 0.4516,
      "step": 2120
    },
    {
      "epoch": 0.6553342050611491,
      "grad_norm": 0.38185665011405945,
      "learning_rate": 7.685470085470086e-05,
      "loss": 0.4416,
      "step": 2130
    },
    {
      "epoch": 0.658410891469887,
      "grad_norm": 0.33621394634246826,
      "learning_rate": 7.617094017094018e-05,
      "loss": 0.4331,
      "step": 2140
    },
    {
      "epoch": 0.6614875778786247,
      "grad_norm": 0.34308287501335144,
      "learning_rate": 7.548717948717949e-05,
      "loss": 0.444,
      "step": 2150
    },
    {
      "epoch": 0.6645642642873625,
      "grad_norm": 0.3283780515193939,
      "learning_rate": 7.48034188034188e-05,
      "loss": 0.4437,
      "step": 2160
    },
    {
      "epoch": 0.6676409506961003,
      "grad_norm": 0.3517468571662903,
      "learning_rate": 7.411965811965812e-05,
      "loss": 0.4463,
      "step": 2170
    },
    {
      "epoch": 0.6707176371048381,
      "grad_norm": 0.3111878037452698,
      "learning_rate": 7.343589743589744e-05,
      "loss": 0.4583,
      "step": 2180
    },
    {
      "epoch": 0.6737943235135759,
      "grad_norm": 0.35871943831443787,
      "learning_rate": 7.275213675213675e-05,
      "loss": 0.4116,
      "step": 2190
    },
    {
      "epoch": 0.6768710099223136,
      "grad_norm": 0.36255818605422974,
      "learning_rate": 7.206837606837608e-05,
      "loss": 0.4963,
      "step": 2200
    },
    {
      "epoch": 0.6799476963310515,
      "grad_norm": 0.25302794575691223,
      "learning_rate": 7.138461538461538e-05,
      "loss": 0.4339,
      "step": 2210
    },
    {
      "epoch": 0.6830243827397893,
      "grad_norm": 0.3087162673473358,
      "learning_rate": 7.07008547008547e-05,
      "loss": 0.4254,
      "step": 2220
    },
    {
      "epoch": 0.6861010691485271,
      "grad_norm": 0.3101164400577545,
      "learning_rate": 7.001709401709401e-05,
      "loss": 0.4115,
      "step": 2230
    },
    {
      "epoch": 0.6891777555572648,
      "grad_norm": 0.31172624230384827,
      "learning_rate": 6.933333333333334e-05,
      "loss": 0.4369,
      "step": 2240
    },
    {
      "epoch": 0.6922544419660026,
      "grad_norm": 0.34179165959358215,
      "learning_rate": 6.864957264957266e-05,
      "loss": 0.4788,
      "step": 2250
    },
    {
      "epoch": 0.6953311283747404,
      "grad_norm": 0.32767969369888306,
      "learning_rate": 6.796581196581196e-05,
      "loss": 0.3916,
      "step": 2260
    },
    {
      "epoch": 0.6984078147834782,
      "grad_norm": 0.36351242661476135,
      "learning_rate": 6.728205128205129e-05,
      "loss": 0.4453,
      "step": 2270
    },
    {
      "epoch": 0.701484501192216,
      "grad_norm": 0.32532843947410583,
      "learning_rate": 6.65982905982906e-05,
      "loss": 0.4524,
      "step": 2280
    },
    {
      "epoch": 0.7045611876009538,
      "grad_norm": 0.38976842164993286,
      "learning_rate": 6.591452991452992e-05,
      "loss": 0.4241,
      "step": 2290
    },
    {
      "epoch": 0.7076378740096916,
      "grad_norm": 0.28040555119514465,
      "learning_rate": 6.523076923076923e-05,
      "loss": 0.4246,
      "step": 2300
    },
    {
      "epoch": 0.7107145604184294,
      "grad_norm": 0.3479476571083069,
      "learning_rate": 6.454700854700855e-05,
      "loss": 0.4354,
      "step": 2310
    },
    {
      "epoch": 0.7137912468271671,
      "grad_norm": 0.40148937702178955,
      "learning_rate": 6.386324786324787e-05,
      "loss": 0.4139,
      "step": 2320
    },
    {
      "epoch": 0.7168679332359049,
      "grad_norm": 0.3563120663166046,
      "learning_rate": 6.317948717948718e-05,
      "loss": 0.465,
      "step": 2330
    },
    {
      "epoch": 0.7199446196446427,
      "grad_norm": 0.28557252883911133,
      "learning_rate": 6.249572649572651e-05,
      "loss": 0.4045,
      "step": 2340
    },
    {
      "epoch": 0.7230213060533806,
      "grad_norm": 0.3158312141895294,
      "learning_rate": 6.181196581196581e-05,
      "loss": 0.4199,
      "step": 2350
    },
    {
      "epoch": 0.7260979924621183,
      "grad_norm": 0.33504706621170044,
      "learning_rate": 6.112820512820513e-05,
      "loss": 0.4148,
      "step": 2360
    },
    {
      "epoch": 0.7291746788708561,
      "grad_norm": 0.3289089500904083,
      "learning_rate": 6.044444444444445e-05,
      "loss": 0.462,
      "step": 2370
    },
    {
      "epoch": 0.7322513652795939,
      "grad_norm": 0.365115225315094,
      "learning_rate": 5.9760683760683765e-05,
      "loss": 0.4563,
      "step": 2380
    },
    {
      "epoch": 0.7353280516883317,
      "grad_norm": 0.44653767347335815,
      "learning_rate": 5.907692307692309e-05,
      "loss": 0.4177,
      "step": 2390
    },
    {
      "epoch": 0.7384047380970694,
      "grad_norm": 0.30306461453437805,
      "learning_rate": 5.8393162393162395e-05,
      "loss": 0.3806,
      "step": 2400
    },
    {
      "epoch": 0.7414814245058072,
      "grad_norm": 0.3778373599052429,
      "learning_rate": 5.770940170940171e-05,
      "loss": 0.4265,
      "step": 2410
    },
    {
      "epoch": 0.744558110914545,
      "grad_norm": 0.3266451060771942,
      "learning_rate": 5.702564102564103e-05,
      "loss": 0.4216,
      "step": 2420
    },
    {
      "epoch": 0.7476347973232829,
      "grad_norm": 0.3594154715538025,
      "learning_rate": 5.634188034188035e-05,
      "loss": 0.4344,
      "step": 2430
    },
    {
      "epoch": 0.7507114837320206,
      "grad_norm": 0.27765092253685,
      "learning_rate": 5.565811965811966e-05,
      "loss": 0.4246,
      "step": 2440
    },
    {
      "epoch": 0.7537881701407584,
      "grad_norm": 0.3178212642669678,
      "learning_rate": 5.497435897435897e-05,
      "loss": 0.4599,
      "step": 2450
    },
    {
      "epoch": 0.7568648565494962,
      "grad_norm": 0.3350626826286316,
      "learning_rate": 5.4290598290598294e-05,
      "loss": 0.4184,
      "step": 2460
    },
    {
      "epoch": 0.759941542958234,
      "grad_norm": 0.3176921010017395,
      "learning_rate": 5.360683760683761e-05,
      "loss": 0.4651,
      "step": 2470
    },
    {
      "epoch": 0.7630182293669717,
      "grad_norm": 0.2962225079536438,
      "learning_rate": 5.292307692307693e-05,
      "loss": 0.4659,
      "step": 2480
    },
    {
      "epoch": 0.7660949157757095,
      "grad_norm": 0.33372998237609863,
      "learning_rate": 5.223931623931624e-05,
      "loss": 0.4181,
      "step": 2490
    },
    {
      "epoch": 0.7691716021844474,
      "grad_norm": 0.34311291575431824,
      "learning_rate": 5.1555555555555556e-05,
      "loss": 0.4268,
      "step": 2500
    },
    {
      "epoch": 0.7722482885931852,
      "grad_norm": 0.3093731105327606,
      "learning_rate": 5.087179487179488e-05,
      "loss": 0.4762,
      "step": 2510
    },
    {
      "epoch": 0.7753249750019229,
      "grad_norm": 0.3079461455345154,
      "learning_rate": 5.018803418803419e-05,
      "loss": 0.4505,
      "step": 2520
    },
    {
      "epoch": 0.7784016614106607,
      "grad_norm": 0.4060865044593811,
      "learning_rate": 4.950427350427351e-05,
      "loss": 0.4777,
      "step": 2530
    },
    {
      "epoch": 0.7814783478193985,
      "grad_norm": 0.3448980152606964,
      "learning_rate": 4.882051282051282e-05,
      "loss": 0.4956,
      "step": 2540
    },
    {
      "epoch": 0.7845550342281363,
      "grad_norm": 0.39771321415901184,
      "learning_rate": 4.813675213675214e-05,
      "loss": 0.4403,
      "step": 2550
    },
    {
      "epoch": 0.787631720636874,
      "grad_norm": 0.3269808292388916,
      "learning_rate": 4.7452991452991455e-05,
      "loss": 0.4644,
      "step": 2560
    },
    {
      "epoch": 0.7907084070456118,
      "grad_norm": 0.3434365391731262,
      "learning_rate": 4.676923076923077e-05,
      "loss": 0.4587,
      "step": 2570
    },
    {
      "epoch": 0.7937850934543497,
      "grad_norm": 0.3258511424064636,
      "learning_rate": 4.608547008547009e-05,
      "loss": 0.4006,
      "step": 2580
    },
    {
      "epoch": 0.7968617798630875,
      "grad_norm": 0.31425437331199646,
      "learning_rate": 4.54017094017094e-05,
      "loss": 0.4423,
      "step": 2590
    },
    {
      "epoch": 0.7999384662718253,
      "grad_norm": 0.28315433859825134,
      "learning_rate": 4.471794871794872e-05,
      "loss": 0.4374,
      "step": 2600
    },
    {
      "epoch": 0.803015152680563,
      "grad_norm": 0.32586759328842163,
      "learning_rate": 4.403418803418803e-05,
      "loss": 0.4239,
      "step": 2610
    },
    {
      "epoch": 0.8060918390893008,
      "grad_norm": 0.3151816725730896,
      "learning_rate": 4.3350427350427354e-05,
      "loss": 0.4193,
      "step": 2620
    },
    {
      "epoch": 0.8091685254980386,
      "grad_norm": 0.3657534122467041,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.4561,
      "step": 2630
    },
    {
      "epoch": 0.8122452119067765,
      "grad_norm": 0.35795360803604126,
      "learning_rate": 4.1982905982905985e-05,
      "loss": 0.4874,
      "step": 2640
    },
    {
      "epoch": 0.8153218983155142,
      "grad_norm": 0.30978867411613464,
      "learning_rate": 4.12991452991453e-05,
      "loss": 0.4144,
      "step": 2650
    },
    {
      "epoch": 0.818398584724252,
      "grad_norm": 0.3208770155906677,
      "learning_rate": 4.0615384615384615e-05,
      "loss": 0.4493,
      "step": 2660
    },
    {
      "epoch": 0.8214752711329898,
      "grad_norm": 0.3380218744277954,
      "learning_rate": 3.993162393162394e-05,
      "loss": 0.4434,
      "step": 2670
    },
    {
      "epoch": 0.8245519575417276,
      "grad_norm": 0.287397563457489,
      "learning_rate": 3.9247863247863246e-05,
      "loss": 0.4063,
      "step": 2680
    },
    {
      "epoch": 0.8276286439504653,
      "grad_norm": 0.33158302307128906,
      "learning_rate": 3.856410256410257e-05,
      "loss": 0.4524,
      "step": 2690
    },
    {
      "epoch": 0.8307053303592031,
      "grad_norm": 0.3708432614803314,
      "learning_rate": 3.788034188034188e-05,
      "loss": 0.4912,
      "step": 2700
    },
    {
      "epoch": 0.833782016767941,
      "grad_norm": 0.35499894618988037,
      "learning_rate": 3.71965811965812e-05,
      "loss": 0.4555,
      "step": 2710
    },
    {
      "epoch": 0.8368587031766788,
      "grad_norm": 0.31400439143180847,
      "learning_rate": 3.6512820512820514e-05,
      "loss": 0.4153,
      "step": 2720
    },
    {
      "epoch": 0.8399353895854165,
      "grad_norm": 0.3575758934020996,
      "learning_rate": 3.582905982905983e-05,
      "loss": 0.4516,
      "step": 2730
    },
    {
      "epoch": 0.8430120759941543,
      "grad_norm": 0.30440831184387207,
      "learning_rate": 3.514529914529915e-05,
      "loss": 0.4473,
      "step": 2740
    },
    {
      "epoch": 0.8460887624028921,
      "grad_norm": 0.377488911151886,
      "learning_rate": 3.446153846153846e-05,
      "loss": 0.4463,
      "step": 2750
    },
    {
      "epoch": 0.8491654488116299,
      "grad_norm": 0.22540104389190674,
      "learning_rate": 3.377777777777778e-05,
      "loss": 0.4462,
      "step": 2760
    },
    {
      "epoch": 0.8522421352203676,
      "grad_norm": 0.367383748292923,
      "learning_rate": 3.309401709401709e-05,
      "loss": 0.4352,
      "step": 2770
    },
    {
      "epoch": 0.8553188216291054,
      "grad_norm": 0.3560245931148529,
      "learning_rate": 3.2410256410256413e-05,
      "loss": 0.4369,
      "step": 2780
    },
    {
      "epoch": 0.8583955080378433,
      "grad_norm": 0.3471319377422333,
      "learning_rate": 3.172649572649573e-05,
      "loss": 0.458,
      "step": 2790
    },
    {
      "epoch": 0.8614721944465811,
      "grad_norm": 0.28947487473487854,
      "learning_rate": 3.1042735042735044e-05,
      "loss": 0.4639,
      "step": 2800
    },
    {
      "epoch": 0.8645488808553188,
      "grad_norm": 0.28389084339141846,
      "learning_rate": 3.0358974358974363e-05,
      "loss": 0.4013,
      "step": 2810
    },
    {
      "epoch": 0.8676255672640566,
      "grad_norm": 0.31342798471450806,
      "learning_rate": 2.9675213675213675e-05,
      "loss": 0.4484,
      "step": 2820
    },
    {
      "epoch": 0.8707022536727944,
      "grad_norm": 0.33013176918029785,
      "learning_rate": 2.8991452991452994e-05,
      "loss": 0.3946,
      "step": 2830
    },
    {
      "epoch": 0.8737789400815322,
      "grad_norm": 0.32736119627952576,
      "learning_rate": 2.8307692307692306e-05,
      "loss": 0.4411,
      "step": 2840
    },
    {
      "epoch": 0.8768556264902699,
      "grad_norm": 0.38898682594299316,
      "learning_rate": 2.7623931623931624e-05,
      "loss": 0.4617,
      "step": 2850
    },
    {
      "epoch": 0.8799323128990078,
      "grad_norm": 0.3223898112773895,
      "learning_rate": 2.694017094017094e-05,
      "loss": 0.404,
      "step": 2860
    },
    {
      "epoch": 0.8830089993077456,
      "grad_norm": 0.2954946458339691,
      "learning_rate": 2.625641025641026e-05,
      "loss": 0.4253,
      "step": 2870
    },
    {
      "epoch": 0.8860856857164834,
      "grad_norm": 0.30676984786987305,
      "learning_rate": 2.5572649572649577e-05,
      "loss": 0.4035,
      "step": 2880
    },
    {
      "epoch": 0.8891623721252211,
      "grad_norm": 0.27394646406173706,
      "learning_rate": 2.488888888888889e-05,
      "loss": 0.4071,
      "step": 2890
    },
    {
      "epoch": 0.8922390585339589,
      "grad_norm": 0.27806296944618225,
      "learning_rate": 2.4205128205128205e-05,
      "loss": 0.3817,
      "step": 2900
    },
    {
      "epoch": 0.8953157449426967,
      "grad_norm": 0.3097676634788513,
      "learning_rate": 2.3521367521367523e-05,
      "loss": 0.4247,
      "step": 2910
    },
    {
      "epoch": 0.8983924313514345,
      "grad_norm": 0.31225401163101196,
      "learning_rate": 2.283760683760684e-05,
      "loss": 0.4054,
      "step": 2920
    },
    {
      "epoch": 0.9014691177601722,
      "grad_norm": 0.2808218002319336,
      "learning_rate": 2.2153846153846154e-05,
      "loss": 0.4496,
      "step": 2930
    },
    {
      "epoch": 0.9045458041689101,
      "grad_norm": 0.304386705160141,
      "learning_rate": 2.1470085470085473e-05,
      "loss": 0.3957,
      "step": 2940
    },
    {
      "epoch": 0.9076224905776479,
      "grad_norm": 0.3429705798625946,
      "learning_rate": 2.0786324786324788e-05,
      "loss": 0.436,
      "step": 2950
    },
    {
      "epoch": 0.9106991769863857,
      "grad_norm": 0.3117181956768036,
      "learning_rate": 2.0102564102564104e-05,
      "loss": 0.4091,
      "step": 2960
    },
    {
      "epoch": 0.9137758633951234,
      "grad_norm": 0.3487018048763275,
      "learning_rate": 1.941880341880342e-05,
      "loss": 0.4465,
      "step": 2970
    },
    {
      "epoch": 0.9168525498038612,
      "grad_norm": 0.2871949374675751,
      "learning_rate": 1.8735042735042734e-05,
      "loss": 0.4259,
      "step": 2980
    },
    {
      "epoch": 0.919929236212599,
      "grad_norm": 0.32327166199684143,
      "learning_rate": 1.8051282051282053e-05,
      "loss": 0.4508,
      "step": 2990
    },
    {
      "epoch": 0.9230059226213368,
      "grad_norm": 0.2686401605606079,
      "learning_rate": 1.736752136752137e-05,
      "loss": 0.419,
      "step": 3000
    },
    {
      "epoch": 0.9260826090300746,
      "grad_norm": 0.32157719135284424,
      "learning_rate": 1.6683760683760684e-05,
      "loss": 0.3822,
      "step": 3010
    },
    {
      "epoch": 0.9291592954388124,
      "grad_norm": 0.31225574016571045,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.4155,
      "step": 3020
    },
    {
      "epoch": 0.9322359818475502,
      "grad_norm": 0.3488611578941345,
      "learning_rate": 1.5316239316239318e-05,
      "loss": 0.4709,
      "step": 3030
    },
    {
      "epoch": 0.935312668256288,
      "grad_norm": 0.3197360932826996,
      "learning_rate": 1.4632478632478633e-05,
      "loss": 0.4525,
      "step": 3040
    },
    {
      "epoch": 0.9383893546650258,
      "grad_norm": 0.3151988685131073,
      "learning_rate": 1.3948717948717949e-05,
      "loss": 0.4238,
      "step": 3050
    },
    {
      "epoch": 0.9414660410737635,
      "grad_norm": 0.32141754031181335,
      "learning_rate": 1.3264957264957264e-05,
      "loss": 0.4192,
      "step": 3060
    },
    {
      "epoch": 0.9445427274825013,
      "grad_norm": 0.32520654797554016,
      "learning_rate": 1.2581196581196583e-05,
      "loss": 0.3828,
      "step": 3070
    },
    {
      "epoch": 0.9476194138912392,
      "grad_norm": 0.34685733914375305,
      "learning_rate": 1.1897435897435898e-05,
      "loss": 0.4643,
      "step": 3080
    },
    {
      "epoch": 0.950696100299977,
      "grad_norm": 0.3544566333293915,
      "learning_rate": 1.1213675213675215e-05,
      "loss": 0.461,
      "step": 3090
    },
    {
      "epoch": 0.9537727867087147,
      "grad_norm": 0.3931926190853119,
      "learning_rate": 1.052991452991453e-05,
      "loss": 0.4539,
      "step": 3100
    },
    {
      "epoch": 0.9568494731174525,
      "grad_norm": 0.31455597281455994,
      "learning_rate": 9.846153846153846e-06,
      "loss": 0.4348,
      "step": 3110
    },
    {
      "epoch": 0.9599261595261903,
      "grad_norm": 0.3484104573726654,
      "learning_rate": 9.162393162393163e-06,
      "loss": 0.4343,
      "step": 3120
    },
    {
      "epoch": 0.9630028459349281,
      "grad_norm": 0.24566331505775452,
      "learning_rate": 8.47863247863248e-06,
      "loss": 0.4234,
      "step": 3130
    },
    {
      "epoch": 0.9660795323436658,
      "grad_norm": 0.31342613697052,
      "learning_rate": 7.794871794871796e-06,
      "loss": 0.458,
      "step": 3140
    },
    {
      "epoch": 0.9691562187524037,
      "grad_norm": 0.31927239894866943,
      "learning_rate": 7.111111111111112e-06,
      "loss": 0.4316,
      "step": 3150
    },
    {
      "epoch": 0.9722329051611415,
      "grad_norm": 0.3108937740325928,
      "learning_rate": 6.427350427350427e-06,
      "loss": 0.4129,
      "step": 3160
    },
    {
      "epoch": 0.9753095915698793,
      "grad_norm": 0.4001432955265045,
      "learning_rate": 5.743589743589744e-06,
      "loss": 0.432,
      "step": 3170
    },
    {
      "epoch": 0.978386277978617,
      "grad_norm": 0.3869822919368744,
      "learning_rate": 5.0598290598290605e-06,
      "loss": 0.4858,
      "step": 3180
    },
    {
      "epoch": 0.9814629643873548,
      "grad_norm": 0.28539857268333435,
      "learning_rate": 4.376068376068376e-06,
      "loss": 0.4186,
      "step": 3190
    },
    {
      "epoch": 0.9845396507960926,
      "grad_norm": 0.2926487326622009,
      "learning_rate": 3.692307692307693e-06,
      "loss": 0.4487,
      "step": 3200
    },
    {
      "epoch": 0.9876163372048304,
      "grad_norm": 0.334739625453949,
      "learning_rate": 3.0085470085470087e-06,
      "loss": 0.422,
      "step": 3210
    },
    {
      "epoch": 0.9906930236135681,
      "grad_norm": 0.3100178837776184,
      "learning_rate": 2.324786324786325e-06,
      "loss": 0.3767,
      "step": 3220
    },
    {
      "epoch": 0.993769710022306,
      "grad_norm": 0.3368287980556488,
      "learning_rate": 1.6410256410256412e-06,
      "loss": 0.4994,
      "step": 3230
    },
    {
      "epoch": 0.9968463964310438,
      "grad_norm": 0.29529625177383423,
      "learning_rate": 9.572649572649574e-07,
      "loss": 0.4308,
      "step": 3240
    },
    {
      "epoch": 0.9999230828397816,
      "grad_norm": 0.3603084087371826,
      "learning_rate": 2.735042735042735e-07,
      "loss": 0.4749,
      "step": 3250
    }
  ],
  "logging_steps": 10,
  "max_steps": 3250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.7523441238016e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
