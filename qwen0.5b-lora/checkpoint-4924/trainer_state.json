{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9998477080054825,
  "eval_steps": 500,
  "global_step": 4924,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002030559926899843,
      "grad_norm": 93.93461608886719,
      "learning_rate": 2.8397565922920896e-06,
      "loss": 12.9427,
      "step": 10
    },
    {
      "epoch": 0.004061119853799686,
      "grad_norm": 117.16838836669922,
      "learning_rate": 6.490872210953347e-06,
      "loss": 11.7524,
      "step": 20
    },
    {
      "epoch": 0.006091679780699528,
      "grad_norm": 178.20445251464844,
      "learning_rate": 1.054766734279919e-05,
      "loss": 7.7891,
      "step": 30
    },
    {
      "epoch": 0.008122239707599371,
      "grad_norm": 19.77775764465332,
      "learning_rate": 1.460446247464503e-05,
      "loss": 2.0784,
      "step": 40
    },
    {
      "epoch": 0.010152799634499212,
      "grad_norm": 1.2140568494796753,
      "learning_rate": 1.8661257606490873e-05,
      "loss": 0.6032,
      "step": 50
    },
    {
      "epoch": 0.012183359561399055,
      "grad_norm": 0.5636104941368103,
      "learning_rate": 2.2718052738336716e-05,
      "loss": 0.4914,
      "step": 60
    },
    {
      "epoch": 0.014213919488298898,
      "grad_norm": 0.5297414064407349,
      "learning_rate": 2.6774847870182556e-05,
      "loss": 0.4528,
      "step": 70
    },
    {
      "epoch": 0.016244479415198743,
      "grad_norm": 0.47310662269592285,
      "learning_rate": 3.0831643002028396e-05,
      "loss": 0.4676,
      "step": 80
    },
    {
      "epoch": 0.018275039342098582,
      "grad_norm": 0.5023465752601624,
      "learning_rate": 3.488843813387424e-05,
      "loss": 0.4899,
      "step": 90
    },
    {
      "epoch": 0.020305599268998425,
      "grad_norm": 0.45002835988998413,
      "learning_rate": 3.894523326572008e-05,
      "loss": 0.4968,
      "step": 100
    },
    {
      "epoch": 0.022336159195898268,
      "grad_norm": 0.4638700485229492,
      "learning_rate": 4.300202839756592e-05,
      "loss": 0.4957,
      "step": 110
    },
    {
      "epoch": 0.02436671912279811,
      "grad_norm": 0.46254223585128784,
      "learning_rate": 4.705882352941177e-05,
      "loss": 0.4118,
      "step": 120
    },
    {
      "epoch": 0.026397279049697953,
      "grad_norm": 0.6288833618164062,
      "learning_rate": 5.1115618661257614e-05,
      "loss": 0.5024,
      "step": 130
    },
    {
      "epoch": 0.028427838976597796,
      "grad_norm": 0.5554554462432861,
      "learning_rate": 5.517241379310345e-05,
      "loss": 0.4492,
      "step": 140
    },
    {
      "epoch": 0.03045839890349764,
      "grad_norm": 0.4494868218898773,
      "learning_rate": 5.92292089249493e-05,
      "loss": 0.4644,
      "step": 150
    },
    {
      "epoch": 0.032488958830397485,
      "grad_norm": 0.5674687623977661,
      "learning_rate": 6.328600405679513e-05,
      "loss": 0.4487,
      "step": 160
    },
    {
      "epoch": 0.03451951875729733,
      "grad_norm": 0.47803667187690735,
      "learning_rate": 6.734279918864099e-05,
      "loss": 0.4639,
      "step": 170
    },
    {
      "epoch": 0.036550078684197164,
      "grad_norm": 0.5309122800827026,
      "learning_rate": 7.139959432048681e-05,
      "loss": 0.4275,
      "step": 180
    },
    {
      "epoch": 0.03858063861109701,
      "grad_norm": 0.5880763530731201,
      "learning_rate": 7.545638945233266e-05,
      "loss": 0.4521,
      "step": 190
    },
    {
      "epoch": 0.04061119853799685,
      "grad_norm": 0.4697794020175934,
      "learning_rate": 7.95131845841785e-05,
      "loss": 0.4839,
      "step": 200
    },
    {
      "epoch": 0.04264175846489669,
      "grad_norm": 0.5862203240394592,
      "learning_rate": 8.356997971602434e-05,
      "loss": 0.5366,
      "step": 210
    },
    {
      "epoch": 0.044672318391796535,
      "grad_norm": 0.48828208446502686,
      "learning_rate": 8.762677484787018e-05,
      "loss": 0.4357,
      "step": 220
    },
    {
      "epoch": 0.04670287831869638,
      "grad_norm": 0.5677282214164734,
      "learning_rate": 9.168356997971604e-05,
      "loss": 0.4638,
      "step": 230
    },
    {
      "epoch": 0.04873343824559622,
      "grad_norm": 0.43192094564437866,
      "learning_rate": 9.574036511156188e-05,
      "loss": 0.4474,
      "step": 240
    },
    {
      "epoch": 0.050763998172496064,
      "grad_norm": 0.5208849906921387,
      "learning_rate": 9.979716024340772e-05,
      "loss": 0.4982,
      "step": 250
    },
    {
      "epoch": 0.05279455809939591,
      "grad_norm": 0.5192389488220215,
      "learning_rate": 0.00010385395537525356,
      "loss": 0.4249,
      "step": 260
    },
    {
      "epoch": 0.05482511802629575,
      "grad_norm": 0.530368447303772,
      "learning_rate": 0.0001079107505070994,
      "loss": 0.4775,
      "step": 270
    },
    {
      "epoch": 0.05685567795319559,
      "grad_norm": 0.5009120106697083,
      "learning_rate": 0.00011196754563894525,
      "loss": 0.4234,
      "step": 280
    },
    {
      "epoch": 0.058886237880095435,
      "grad_norm": 0.48269814252853394,
      "learning_rate": 0.00011602434077079107,
      "loss": 0.4411,
      "step": 290
    },
    {
      "epoch": 0.06091679780699528,
      "grad_norm": 0.5305854678153992,
      "learning_rate": 0.00012008113590263693,
      "loss": 0.4935,
      "step": 300
    },
    {
      "epoch": 0.06294735773389512,
      "grad_norm": 0.5016182661056519,
      "learning_rate": 0.00012413793103448277,
      "loss": 0.464,
      "step": 310
    },
    {
      "epoch": 0.06497791766079497,
      "grad_norm": 0.4653405249118805,
      "learning_rate": 0.0001281947261663286,
      "loss": 0.4249,
      "step": 320
    },
    {
      "epoch": 0.0670084775876948,
      "grad_norm": 0.4424412250518799,
      "learning_rate": 0.00013225152129817445,
      "loss": 0.4457,
      "step": 330
    },
    {
      "epoch": 0.06903903751459466,
      "grad_norm": 0.4780416488647461,
      "learning_rate": 0.00013630831643002029,
      "loss": 0.4788,
      "step": 340
    },
    {
      "epoch": 0.07106959744149449,
      "grad_norm": 0.4445948302745819,
      "learning_rate": 0.00014036511156186612,
      "loss": 0.5136,
      "step": 350
    },
    {
      "epoch": 0.07310015736839433,
      "grad_norm": 0.40976524353027344,
      "learning_rate": 0.000144421906693712,
      "loss": 0.4482,
      "step": 360
    },
    {
      "epoch": 0.07513071729529418,
      "grad_norm": 0.5119131207466125,
      "learning_rate": 0.0001484787018255578,
      "loss": 0.4356,
      "step": 370
    },
    {
      "epoch": 0.07716127722219401,
      "grad_norm": 0.38377419114112854,
      "learning_rate": 0.00015253549695740364,
      "loss": 0.4505,
      "step": 380
    },
    {
      "epoch": 0.07919183714909386,
      "grad_norm": 0.5210450291633606,
      "learning_rate": 0.0001565922920892495,
      "loss": 0.4951,
      "step": 390
    },
    {
      "epoch": 0.0812223970759937,
      "grad_norm": 0.3734821677207947,
      "learning_rate": 0.00016064908722109535,
      "loss": 0.4615,
      "step": 400
    },
    {
      "epoch": 0.08325295700289355,
      "grad_norm": 0.39340832829475403,
      "learning_rate": 0.0001647058823529412,
      "loss": 0.4786,
      "step": 410
    },
    {
      "epoch": 0.08528351692979338,
      "grad_norm": 0.3536384701728821,
      "learning_rate": 0.00016876267748478703,
      "loss": 0.4845,
      "step": 420
    },
    {
      "epoch": 0.08731407685669323,
      "grad_norm": 0.486623078584671,
      "learning_rate": 0.00017281947261663287,
      "loss": 0.4933,
      "step": 430
    },
    {
      "epoch": 0.08934463678359307,
      "grad_norm": 0.44681909680366516,
      "learning_rate": 0.0001768762677484787,
      "loss": 0.4822,
      "step": 440
    },
    {
      "epoch": 0.09137519671049292,
      "grad_norm": 0.355073481798172,
      "learning_rate": 0.00018093306288032455,
      "loss": 0.4297,
      "step": 450
    },
    {
      "epoch": 0.09340575663739276,
      "grad_norm": 0.32200875878334045,
      "learning_rate": 0.0001849898580121704,
      "loss": 0.4411,
      "step": 460
    },
    {
      "epoch": 0.0954363165642926,
      "grad_norm": 0.3892982006072998,
      "learning_rate": 0.00018904665314401623,
      "loss": 0.4153,
      "step": 470
    },
    {
      "epoch": 0.09746687649119244,
      "grad_norm": 0.44994285702705383,
      "learning_rate": 0.0001931034482758621,
      "loss": 0.4837,
      "step": 480
    },
    {
      "epoch": 0.09949743641809229,
      "grad_norm": 0.33222371339797974,
      "learning_rate": 0.0001971602434077079,
      "loss": 0.4511,
      "step": 490
    },
    {
      "epoch": 0.10152799634499213,
      "grad_norm": 0.6045421957969666,
      "learning_rate": 0.0001998645903859174,
      "loss": 0.4615,
      "step": 500
    },
    {
      "epoch": 0.10355855627189198,
      "grad_norm": 0.3552744388580322,
      "learning_rate": 0.00019941322500564209,
      "loss": 0.4154,
      "step": 510
    },
    {
      "epoch": 0.10558911619879181,
      "grad_norm": 0.4124545156955719,
      "learning_rate": 0.00019896185962536676,
      "loss": 0.4689,
      "step": 520
    },
    {
      "epoch": 0.10761967612569166,
      "grad_norm": 0.39157068729400635,
      "learning_rate": 0.0001985104942450914,
      "loss": 0.4679,
      "step": 530
    },
    {
      "epoch": 0.1096502360525915,
      "grad_norm": 0.36042892932891846,
      "learning_rate": 0.00019805912886481608,
      "loss": 0.4704,
      "step": 540
    },
    {
      "epoch": 0.11168079597949135,
      "grad_norm": 0.3177231550216675,
      "learning_rate": 0.00019760776348454076,
      "loss": 0.4084,
      "step": 550
    },
    {
      "epoch": 0.11371135590639118,
      "grad_norm": 0.3979862630367279,
      "learning_rate": 0.0001971563981042654,
      "loss": 0.4559,
      "step": 560
    },
    {
      "epoch": 0.11574191583329103,
      "grad_norm": 0.4167219400405884,
      "learning_rate": 0.00019670503272399008,
      "loss": 0.4668,
      "step": 570
    },
    {
      "epoch": 0.11777247576019087,
      "grad_norm": 0.4179900288581848,
      "learning_rate": 0.00019625366734371476,
      "loss": 0.4296,
      "step": 580
    },
    {
      "epoch": 0.11980303568709072,
      "grad_norm": 0.3873843550682068,
      "learning_rate": 0.0001958023019634394,
      "loss": 0.4307,
      "step": 590
    },
    {
      "epoch": 0.12183359561399056,
      "grad_norm": 0.40960511565208435,
      "learning_rate": 0.00019535093658316408,
      "loss": 0.5019,
      "step": 600
    },
    {
      "epoch": 0.1238641555408904,
      "grad_norm": 0.34212297201156616,
      "learning_rate": 0.00019489957120288876,
      "loss": 0.4944,
      "step": 610
    },
    {
      "epoch": 0.12589471546779024,
      "grad_norm": 0.3858202397823334,
      "learning_rate": 0.0001944482058226134,
      "loss": 0.4856,
      "step": 620
    },
    {
      "epoch": 0.1279252753946901,
      "grad_norm": 0.32418596744537354,
      "learning_rate": 0.00019399684044233808,
      "loss": 0.4681,
      "step": 630
    },
    {
      "epoch": 0.12995583532158994,
      "grad_norm": 0.4318010210990906,
      "learning_rate": 0.00019354547506206276,
      "loss": 0.4798,
      "step": 640
    },
    {
      "epoch": 0.13198639524848976,
      "grad_norm": 0.3153812289237976,
      "learning_rate": 0.0001930941096817874,
      "loss": 0.4145,
      "step": 650
    },
    {
      "epoch": 0.1340169551753896,
      "grad_norm": 0.35122668743133545,
      "learning_rate": 0.00019264274430151208,
      "loss": 0.4889,
      "step": 660
    },
    {
      "epoch": 0.13604751510228946,
      "grad_norm": 0.4174827039241791,
      "learning_rate": 0.00019219137892123676,
      "loss": 0.4927,
      "step": 670
    },
    {
      "epoch": 0.1380780750291893,
      "grad_norm": 0.36343929171562195,
      "learning_rate": 0.00019174001354096143,
      "loss": 0.4607,
      "step": 680
    },
    {
      "epoch": 0.14010863495608913,
      "grad_norm": 0.3560146987438202,
      "learning_rate": 0.00019128864816068608,
      "loss": 0.4651,
      "step": 690
    },
    {
      "epoch": 0.14213919488298898,
      "grad_norm": 0.44614145159721375,
      "learning_rate": 0.00019083728278041076,
      "loss": 0.4958,
      "step": 700
    },
    {
      "epoch": 0.14416975480988883,
      "grad_norm": 0.45843973755836487,
      "learning_rate": 0.00019038591740013543,
      "loss": 0.4828,
      "step": 710
    },
    {
      "epoch": 0.14620031473678866,
      "grad_norm": 0.34557226300239563,
      "learning_rate": 0.00018993455201986008,
      "loss": 0.4429,
      "step": 720
    },
    {
      "epoch": 0.1482308746636885,
      "grad_norm": 0.31514984369277954,
      "learning_rate": 0.00018948318663958476,
      "loss": 0.4021,
      "step": 730
    },
    {
      "epoch": 0.15026143459058836,
      "grad_norm": 0.3991875648498535,
      "learning_rate": 0.00018903182125930943,
      "loss": 0.3912,
      "step": 740
    },
    {
      "epoch": 0.1522919945174882,
      "grad_norm": 0.3204464614391327,
      "learning_rate": 0.00018858045587903408,
      "loss": 0.4466,
      "step": 750
    },
    {
      "epoch": 0.15432255444438803,
      "grad_norm": 0.34896278381347656,
      "learning_rate": 0.00018812909049875876,
      "loss": 0.4436,
      "step": 760
    },
    {
      "epoch": 0.15635311437128788,
      "grad_norm": 0.3134208619594574,
      "learning_rate": 0.00018767772511848343,
      "loss": 0.4643,
      "step": 770
    },
    {
      "epoch": 0.15838367429818773,
      "grad_norm": 0.419047474861145,
      "learning_rate": 0.00018722635973820808,
      "loss": 0.484,
      "step": 780
    },
    {
      "epoch": 0.16041423422508758,
      "grad_norm": 0.2710869312286377,
      "learning_rate": 0.00018677499435793276,
      "loss": 0.427,
      "step": 790
    },
    {
      "epoch": 0.1624447941519874,
      "grad_norm": 0.32744911313056946,
      "learning_rate": 0.00018632362897765743,
      "loss": 0.4509,
      "step": 800
    },
    {
      "epoch": 0.16447535407888725,
      "grad_norm": 0.30029457807540894,
      "learning_rate": 0.00018587226359738208,
      "loss": 0.447,
      "step": 810
    },
    {
      "epoch": 0.1665059140057871,
      "grad_norm": 0.33869436383247375,
      "learning_rate": 0.00018542089821710676,
      "loss": 0.4494,
      "step": 820
    },
    {
      "epoch": 0.16853647393268695,
      "grad_norm": 0.37689220905303955,
      "learning_rate": 0.00018496953283683143,
      "loss": 0.4833,
      "step": 830
    },
    {
      "epoch": 0.17056703385958677,
      "grad_norm": 0.28426074981689453,
      "learning_rate": 0.0001845181674565561,
      "loss": 0.4597,
      "step": 840
    },
    {
      "epoch": 0.17259759378648662,
      "grad_norm": 0.2786466181278229,
      "learning_rate": 0.00018406680207628075,
      "loss": 0.4581,
      "step": 850
    },
    {
      "epoch": 0.17462815371338647,
      "grad_norm": 0.3719555139541626,
      "learning_rate": 0.00018361543669600543,
      "loss": 0.4513,
      "step": 860
    },
    {
      "epoch": 0.17665871364028632,
      "grad_norm": 0.32406190037727356,
      "learning_rate": 0.0001831640713157301,
      "loss": 0.4205,
      "step": 870
    },
    {
      "epoch": 0.17868927356718614,
      "grad_norm": 0.30081430077552795,
      "learning_rate": 0.00018271270593545475,
      "loss": 0.4467,
      "step": 880
    },
    {
      "epoch": 0.180719833494086,
      "grad_norm": 0.4292883574962616,
      "learning_rate": 0.00018226134055517943,
      "loss": 0.4584,
      "step": 890
    },
    {
      "epoch": 0.18275039342098584,
      "grad_norm": 0.4099736511707306,
      "learning_rate": 0.0001818099751749041,
      "loss": 0.4642,
      "step": 900
    },
    {
      "epoch": 0.1847809533478857,
      "grad_norm": 0.38077083230018616,
      "learning_rate": 0.00018135860979462875,
      "loss": 0.5074,
      "step": 910
    },
    {
      "epoch": 0.1868115132747855,
      "grad_norm": 0.32568809390068054,
      "learning_rate": 0.00018090724441435343,
      "loss": 0.4915,
      "step": 920
    },
    {
      "epoch": 0.18884207320168536,
      "grad_norm": 0.3478206992149353,
      "learning_rate": 0.0001804558790340781,
      "loss": 0.4252,
      "step": 930
    },
    {
      "epoch": 0.1908726331285852,
      "grad_norm": 0.38398346304893494,
      "learning_rate": 0.00018000451365380275,
      "loss": 0.4712,
      "step": 940
    },
    {
      "epoch": 0.19290319305548506,
      "grad_norm": 0.3757167160511017,
      "learning_rate": 0.00017955314827352743,
      "loss": 0.4339,
      "step": 950
    },
    {
      "epoch": 0.19493375298238488,
      "grad_norm": 0.29979416728019714,
      "learning_rate": 0.0001791017828932521,
      "loss": 0.4258,
      "step": 960
    },
    {
      "epoch": 0.19696431290928473,
      "grad_norm": 0.3802444040775299,
      "learning_rate": 0.00017865041751297675,
      "loss": 0.4633,
      "step": 970
    },
    {
      "epoch": 0.19899487283618458,
      "grad_norm": 0.3804875910282135,
      "learning_rate": 0.00017819905213270143,
      "loss": 0.4558,
      "step": 980
    },
    {
      "epoch": 0.20102543276308443,
      "grad_norm": 0.3436937630176544,
      "learning_rate": 0.0001777476867524261,
      "loss": 0.4427,
      "step": 990
    },
    {
      "epoch": 0.20305599268998425,
      "grad_norm": 0.3563600480556488,
      "learning_rate": 0.00017729632137215078,
      "loss": 0.4837,
      "step": 1000
    },
    {
      "epoch": 0.2050865526168841,
      "grad_norm": 0.39351335167884827,
      "learning_rate": 0.00017684495599187543,
      "loss": 0.4961,
      "step": 1010
    },
    {
      "epoch": 0.20711711254378395,
      "grad_norm": 0.4033966064453125,
      "learning_rate": 0.0001763935906116001,
      "loss": 0.4481,
      "step": 1020
    },
    {
      "epoch": 0.2091476724706838,
      "grad_norm": 0.3292248547077179,
      "learning_rate": 0.00017594222523132478,
      "loss": 0.4577,
      "step": 1030
    },
    {
      "epoch": 0.21117823239758363,
      "grad_norm": 0.28795334696769714,
      "learning_rate": 0.00017549085985104943,
      "loss": 0.4279,
      "step": 1040
    },
    {
      "epoch": 0.21320879232448348,
      "grad_norm": 0.32662835717201233,
      "learning_rate": 0.0001750394944707741,
      "loss": 0.4686,
      "step": 1050
    },
    {
      "epoch": 0.21523935225138333,
      "grad_norm": 0.36720260977745056,
      "learning_rate": 0.00017458812909049878,
      "loss": 0.4466,
      "step": 1060
    },
    {
      "epoch": 0.21726991217828315,
      "grad_norm": 0.3804803490638733,
      "learning_rate": 0.00017413676371022343,
      "loss": 0.456,
      "step": 1070
    },
    {
      "epoch": 0.219300472105183,
      "grad_norm": 0.311245858669281,
      "learning_rate": 0.0001736853983299481,
      "loss": 0.4661,
      "step": 1080
    },
    {
      "epoch": 0.22133103203208285,
      "grad_norm": 0.2953924238681793,
      "learning_rate": 0.00017323403294967278,
      "loss": 0.5153,
      "step": 1090
    },
    {
      "epoch": 0.2233615919589827,
      "grad_norm": 0.3950413763523102,
      "learning_rate": 0.00017278266756939743,
      "loss": 0.4509,
      "step": 1100
    },
    {
      "epoch": 0.22539215188588252,
      "grad_norm": 0.3812825381755829,
      "learning_rate": 0.0001723313021891221,
      "loss": 0.4969,
      "step": 1110
    },
    {
      "epoch": 0.22742271181278237,
      "grad_norm": 0.2786543071269989,
      "learning_rate": 0.00017187993680884678,
      "loss": 0.4795,
      "step": 1120
    },
    {
      "epoch": 0.22945327173968222,
      "grad_norm": 0.4053232967853546,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.5029,
      "step": 1130
    },
    {
      "epoch": 0.23148383166658207,
      "grad_norm": 0.35282421112060547,
      "learning_rate": 0.0001709772060482961,
      "loss": 0.4734,
      "step": 1140
    },
    {
      "epoch": 0.2335143915934819,
      "grad_norm": 0.35181882977485657,
      "learning_rate": 0.00017052584066802078,
      "loss": 0.4745,
      "step": 1150
    },
    {
      "epoch": 0.23554495152038174,
      "grad_norm": 0.39276987314224243,
      "learning_rate": 0.00017007447528774542,
      "loss": 0.4532,
      "step": 1160
    },
    {
      "epoch": 0.2375755114472816,
      "grad_norm": 0.30671632289886475,
      "learning_rate": 0.0001696231099074701,
      "loss": 0.4281,
      "step": 1170
    },
    {
      "epoch": 0.23960607137418144,
      "grad_norm": 0.3795407712459564,
      "learning_rate": 0.00016917174452719478,
      "loss": 0.4672,
      "step": 1180
    },
    {
      "epoch": 0.24163663130108126,
      "grad_norm": 0.45240843296051025,
      "learning_rate": 0.00016872037914691945,
      "loss": 0.4624,
      "step": 1190
    },
    {
      "epoch": 0.2436671912279811,
      "grad_norm": 0.2961958348751068,
      "learning_rate": 0.0001682690137666441,
      "loss": 0.4526,
      "step": 1200
    },
    {
      "epoch": 0.24569775115488096,
      "grad_norm": 0.3619149327278137,
      "learning_rate": 0.00016781764838636878,
      "loss": 0.4633,
      "step": 1210
    },
    {
      "epoch": 0.2477283110817808,
      "grad_norm": 0.30871307849884033,
      "learning_rate": 0.00016736628300609345,
      "loss": 0.5029,
      "step": 1220
    },
    {
      "epoch": 0.24975887100868063,
      "grad_norm": 0.3374318778514862,
      "learning_rate": 0.0001669149176258181,
      "loss": 0.4942,
      "step": 1230
    },
    {
      "epoch": 0.2517894309355805,
      "grad_norm": 0.3178846538066864,
      "learning_rate": 0.00016646355224554277,
      "loss": 0.3986,
      "step": 1240
    },
    {
      "epoch": 0.2538199908624803,
      "grad_norm": 0.38935381174087524,
      "learning_rate": 0.00016601218686526745,
      "loss": 0.453,
      "step": 1250
    },
    {
      "epoch": 0.2558505507893802,
      "grad_norm": 0.28665369749069214,
      "learning_rate": 0.0001655608214849921,
      "loss": 0.3811,
      "step": 1260
    },
    {
      "epoch": 0.25788111071628,
      "grad_norm": 0.34386351704597473,
      "learning_rate": 0.00016510945610471677,
      "loss": 0.4134,
      "step": 1270
    },
    {
      "epoch": 0.2599116706431799,
      "grad_norm": 0.3086637258529663,
      "learning_rate": 0.00016465809072444145,
      "loss": 0.4342,
      "step": 1280
    },
    {
      "epoch": 0.2619422305700797,
      "grad_norm": 0.36855483055114746,
      "learning_rate": 0.0001642067253441661,
      "loss": 0.446,
      "step": 1290
    },
    {
      "epoch": 0.2639727904969795,
      "grad_norm": 0.3401552438735962,
      "learning_rate": 0.00016375535996389077,
      "loss": 0.4249,
      "step": 1300
    },
    {
      "epoch": 0.2660033504238794,
      "grad_norm": 0.3651927411556244,
      "learning_rate": 0.00016330399458361545,
      "loss": 0.453,
      "step": 1310
    },
    {
      "epoch": 0.2680339103507792,
      "grad_norm": 0.32824066281318665,
      "learning_rate": 0.0001628526292033401,
      "loss": 0.4774,
      "step": 1320
    },
    {
      "epoch": 0.27006447027767905,
      "grad_norm": 0.29458296298980713,
      "learning_rate": 0.00016240126382306477,
      "loss": 0.4501,
      "step": 1330
    },
    {
      "epoch": 0.2720950302045789,
      "grad_norm": 0.4035722315311432,
      "learning_rate": 0.00016194989844278945,
      "loss": 0.4672,
      "step": 1340
    },
    {
      "epoch": 0.27412559013147875,
      "grad_norm": 0.37012001872062683,
      "learning_rate": 0.00016149853306251412,
      "loss": 0.4327,
      "step": 1350
    },
    {
      "epoch": 0.2761561500583786,
      "grad_norm": 0.4562472701072693,
      "learning_rate": 0.00016104716768223877,
      "loss": 0.5034,
      "step": 1360
    },
    {
      "epoch": 0.27818670998527845,
      "grad_norm": 0.352560430765152,
      "learning_rate": 0.00016059580230196345,
      "loss": 0.4494,
      "step": 1370
    },
    {
      "epoch": 0.28021726991217827,
      "grad_norm": 0.3224174678325653,
      "learning_rate": 0.00016014443692168812,
      "loss": 0.4297,
      "step": 1380
    },
    {
      "epoch": 0.28224782983907815,
      "grad_norm": 0.2712392508983612,
      "learning_rate": 0.00015969307154141277,
      "loss": 0.4361,
      "step": 1390
    },
    {
      "epoch": 0.28427838976597797,
      "grad_norm": 0.27385619282722473,
      "learning_rate": 0.00015924170616113745,
      "loss": 0.4414,
      "step": 1400
    },
    {
      "epoch": 0.2863089496928778,
      "grad_norm": 0.35536691546440125,
      "learning_rate": 0.00015879034078086212,
      "loss": 0.5089,
      "step": 1410
    },
    {
      "epoch": 0.28833950961977767,
      "grad_norm": 0.3184865713119507,
      "learning_rate": 0.00015833897540058677,
      "loss": 0.4651,
      "step": 1420
    },
    {
      "epoch": 0.2903700695466775,
      "grad_norm": 0.27388712763786316,
      "learning_rate": 0.00015788761002031145,
      "loss": 0.4502,
      "step": 1430
    },
    {
      "epoch": 0.2924006294735773,
      "grad_norm": 0.3496024012565613,
      "learning_rate": 0.00015743624464003612,
      "loss": 0.449,
      "step": 1440
    },
    {
      "epoch": 0.2944311894004772,
      "grad_norm": 0.34353551268577576,
      "learning_rate": 0.00015698487925976077,
      "loss": 0.4586,
      "step": 1450
    },
    {
      "epoch": 0.296461749327377,
      "grad_norm": 0.353692889213562,
      "learning_rate": 0.00015653351387948545,
      "loss": 0.4186,
      "step": 1460
    },
    {
      "epoch": 0.2984923092542769,
      "grad_norm": 0.38692572712898254,
      "learning_rate": 0.00015608214849921012,
      "loss": 0.4551,
      "step": 1470
    },
    {
      "epoch": 0.3005228691811767,
      "grad_norm": 0.30605605244636536,
      "learning_rate": 0.00015563078311893477,
      "loss": 0.4593,
      "step": 1480
    },
    {
      "epoch": 0.30255342910807653,
      "grad_norm": 0.35500672459602356,
      "learning_rate": 0.00015517941773865945,
      "loss": 0.4471,
      "step": 1490
    },
    {
      "epoch": 0.3045839890349764,
      "grad_norm": 0.29094603657722473,
      "learning_rate": 0.00015472805235838412,
      "loss": 0.4207,
      "step": 1500
    },
    {
      "epoch": 0.30661454896187623,
      "grad_norm": 0.3216420114040375,
      "learning_rate": 0.00015427668697810877,
      "loss": 0.4128,
      "step": 1510
    },
    {
      "epoch": 0.30864510888877605,
      "grad_norm": 0.3424105644226074,
      "learning_rate": 0.00015382532159783345,
      "loss": 0.4663,
      "step": 1520
    },
    {
      "epoch": 0.31067566881567593,
      "grad_norm": 0.3684339225292206,
      "learning_rate": 0.00015337395621755812,
      "loss": 0.4576,
      "step": 1530
    },
    {
      "epoch": 0.31270622874257575,
      "grad_norm": 0.33997857570648193,
      "learning_rate": 0.0001529225908372828,
      "loss": 0.4298,
      "step": 1540
    },
    {
      "epoch": 0.31473678866947563,
      "grad_norm": 0.3647293150424957,
      "learning_rate": 0.00015247122545700744,
      "loss": 0.4423,
      "step": 1550
    },
    {
      "epoch": 0.31676734859637545,
      "grad_norm": 0.3567065894603729,
      "learning_rate": 0.00015201986007673212,
      "loss": 0.4604,
      "step": 1560
    },
    {
      "epoch": 0.3187979085232753,
      "grad_norm": 0.29263198375701904,
      "learning_rate": 0.0001515684946964568,
      "loss": 0.4416,
      "step": 1570
    },
    {
      "epoch": 0.32082846845017515,
      "grad_norm": 0.3121137320995331,
      "learning_rate": 0.00015111712931618144,
      "loss": 0.4284,
      "step": 1580
    },
    {
      "epoch": 0.322859028377075,
      "grad_norm": 0.3998958170413971,
      "learning_rate": 0.00015066576393590612,
      "loss": 0.4288,
      "step": 1590
    },
    {
      "epoch": 0.3248895883039748,
      "grad_norm": 0.36558768153190613,
      "learning_rate": 0.0001502143985556308,
      "loss": 0.4319,
      "step": 1600
    },
    {
      "epoch": 0.3269201482308747,
      "grad_norm": 0.3543969690799713,
      "learning_rate": 0.00014976303317535544,
      "loss": 0.469,
      "step": 1610
    },
    {
      "epoch": 0.3289507081577745,
      "grad_norm": 0.35302045941352844,
      "learning_rate": 0.00014931166779508012,
      "loss": 0.4465,
      "step": 1620
    },
    {
      "epoch": 0.3309812680846744,
      "grad_norm": 0.29756954312324524,
      "learning_rate": 0.0001488603024148048,
      "loss": 0.486,
      "step": 1630
    },
    {
      "epoch": 0.3330118280115742,
      "grad_norm": 0.2849356234073639,
      "learning_rate": 0.00014840893703452944,
      "loss": 0.4387,
      "step": 1640
    },
    {
      "epoch": 0.335042387938474,
      "grad_norm": 0.29978254437446594,
      "learning_rate": 0.00014795757165425412,
      "loss": 0.4574,
      "step": 1650
    },
    {
      "epoch": 0.3370729478653739,
      "grad_norm": 0.32446569204330444,
      "learning_rate": 0.0001475062062739788,
      "loss": 0.494,
      "step": 1660
    },
    {
      "epoch": 0.3391035077922737,
      "grad_norm": 0.28707224130630493,
      "learning_rate": 0.00014705484089370344,
      "loss": 0.4522,
      "step": 1670
    },
    {
      "epoch": 0.34113406771917354,
      "grad_norm": 0.3813319802284241,
      "learning_rate": 0.00014660347551342812,
      "loss": 0.4895,
      "step": 1680
    },
    {
      "epoch": 0.3431646276460734,
      "grad_norm": 0.28786545991897583,
      "learning_rate": 0.0001461521101331528,
      "loss": 0.4494,
      "step": 1690
    },
    {
      "epoch": 0.34519518757297324,
      "grad_norm": 0.26216286420822144,
      "learning_rate": 0.00014570074475287747,
      "loss": 0.427,
      "step": 1700
    },
    {
      "epoch": 0.3472257474998731,
      "grad_norm": 0.28126412630081177,
      "learning_rate": 0.00014524937937260212,
      "loss": 0.4725,
      "step": 1710
    },
    {
      "epoch": 0.34925630742677294,
      "grad_norm": 0.363444447517395,
      "learning_rate": 0.0001447980139923268,
      "loss": 0.3958,
      "step": 1720
    },
    {
      "epoch": 0.35128686735367276,
      "grad_norm": 0.2819059491157532,
      "learning_rate": 0.00014434664861205147,
      "loss": 0.4336,
      "step": 1730
    },
    {
      "epoch": 0.35331742728057264,
      "grad_norm": 0.32816624641418457,
      "learning_rate": 0.00014389528323177612,
      "loss": 0.4795,
      "step": 1740
    },
    {
      "epoch": 0.35534798720747246,
      "grad_norm": 0.33459460735321045,
      "learning_rate": 0.0001434439178515008,
      "loss": 0.4749,
      "step": 1750
    },
    {
      "epoch": 0.3573785471343723,
      "grad_norm": 0.3092108368873596,
      "learning_rate": 0.00014299255247122547,
      "loss": 0.4547,
      "step": 1760
    },
    {
      "epoch": 0.35940910706127216,
      "grad_norm": 0.3227442800998688,
      "learning_rate": 0.00014254118709095012,
      "loss": 0.4256,
      "step": 1770
    },
    {
      "epoch": 0.361439666988172,
      "grad_norm": 0.32547053694725037,
      "learning_rate": 0.0001420898217106748,
      "loss": 0.4489,
      "step": 1780
    },
    {
      "epoch": 0.3634702269150718,
      "grad_norm": 0.31304824352264404,
      "learning_rate": 0.00014163845633039947,
      "loss": 0.4471,
      "step": 1790
    },
    {
      "epoch": 0.3655007868419717,
      "grad_norm": 0.39126670360565186,
      "learning_rate": 0.00014118709095012412,
      "loss": 0.4236,
      "step": 1800
    },
    {
      "epoch": 0.3675313467688715,
      "grad_norm": 0.38422664999961853,
      "learning_rate": 0.0001407357255698488,
      "loss": 0.4921,
      "step": 1810
    },
    {
      "epoch": 0.3695619066957714,
      "grad_norm": 0.3401220440864563,
      "learning_rate": 0.00014028436018957347,
      "loss": 0.4434,
      "step": 1820
    },
    {
      "epoch": 0.3715924666226712,
      "grad_norm": 0.36034584045410156,
      "learning_rate": 0.00013983299480929812,
      "loss": 0.4995,
      "step": 1830
    },
    {
      "epoch": 0.373623026549571,
      "grad_norm": 0.2954079508781433,
      "learning_rate": 0.0001393816294290228,
      "loss": 0.4735,
      "step": 1840
    },
    {
      "epoch": 0.3756535864764709,
      "grad_norm": 0.31726452708244324,
      "learning_rate": 0.00013893026404874747,
      "loss": 0.423,
      "step": 1850
    },
    {
      "epoch": 0.3776841464033707,
      "grad_norm": 0.23939552903175354,
      "learning_rate": 0.00013847889866847214,
      "loss": 0.4703,
      "step": 1860
    },
    {
      "epoch": 0.37971470633027055,
      "grad_norm": 0.32006555795669556,
      "learning_rate": 0.0001380275332881968,
      "loss": 0.4807,
      "step": 1870
    },
    {
      "epoch": 0.3817452662571704,
      "grad_norm": 0.31448349356651306,
      "learning_rate": 0.00013757616790792147,
      "loss": 0.4182,
      "step": 1880
    },
    {
      "epoch": 0.38377582618407025,
      "grad_norm": 0.32309994101524353,
      "learning_rate": 0.00013712480252764614,
      "loss": 0.4073,
      "step": 1890
    },
    {
      "epoch": 0.3858063861109701,
      "grad_norm": 0.36044496297836304,
      "learning_rate": 0.0001366734371473708,
      "loss": 0.4323,
      "step": 1900
    },
    {
      "epoch": 0.38783694603786995,
      "grad_norm": 0.32578060030937195,
      "learning_rate": 0.00013622207176709547,
      "loss": 0.4712,
      "step": 1910
    },
    {
      "epoch": 0.38986750596476977,
      "grad_norm": 0.338265985250473,
      "learning_rate": 0.00013577070638682014,
      "loss": 0.396,
      "step": 1920
    },
    {
      "epoch": 0.39189806589166964,
      "grad_norm": 0.37245768308639526,
      "learning_rate": 0.0001353193410065448,
      "loss": 0.4397,
      "step": 1930
    },
    {
      "epoch": 0.39392862581856947,
      "grad_norm": 0.2886294424533844,
      "learning_rate": 0.00013486797562626946,
      "loss": 0.4485,
      "step": 1940
    },
    {
      "epoch": 0.3959591857454693,
      "grad_norm": 0.3775198459625244,
      "learning_rate": 0.00013441661024599414,
      "loss": 0.4449,
      "step": 1950
    },
    {
      "epoch": 0.39798974567236917,
      "grad_norm": 0.31729620695114136,
      "learning_rate": 0.0001339652448657188,
      "loss": 0.4874,
      "step": 1960
    },
    {
      "epoch": 0.400020305599269,
      "grad_norm": 0.35057467222213745,
      "learning_rate": 0.00013351387948544346,
      "loss": 0.4361,
      "step": 1970
    },
    {
      "epoch": 0.40205086552616887,
      "grad_norm": 0.2707747519016266,
      "learning_rate": 0.00013306251410516814,
      "loss": 0.4048,
      "step": 1980
    },
    {
      "epoch": 0.4040814254530687,
      "grad_norm": 0.29461321234703064,
      "learning_rate": 0.0001326111487248928,
      "loss": 0.4755,
      "step": 1990
    },
    {
      "epoch": 0.4061119853799685,
      "grad_norm": 0.34760782122612,
      "learning_rate": 0.00013215978334461746,
      "loss": 0.5037,
      "step": 2000
    },
    {
      "epoch": 0.4081425453068684,
      "grad_norm": 0.28048962354660034,
      "learning_rate": 0.00013170841796434214,
      "loss": 0.4129,
      "step": 2010
    },
    {
      "epoch": 0.4101731052337682,
      "grad_norm": 0.3896302282810211,
      "learning_rate": 0.0001312570525840668,
      "loss": 0.4751,
      "step": 2020
    },
    {
      "epoch": 0.41220366516066803,
      "grad_norm": 0.34500575065612793,
      "learning_rate": 0.00013080568720379146,
      "loss": 0.4828,
      "step": 2030
    },
    {
      "epoch": 0.4142342250875679,
      "grad_norm": 0.3171180784702301,
      "learning_rate": 0.00013035432182351614,
      "loss": 0.4308,
      "step": 2040
    },
    {
      "epoch": 0.41626478501446773,
      "grad_norm": 0.334491491317749,
      "learning_rate": 0.00012990295644324081,
      "loss": 0.4125,
      "step": 2050
    },
    {
      "epoch": 0.4182953449413676,
      "grad_norm": 0.29877784848213196,
      "learning_rate": 0.00012945159106296546,
      "loss": 0.4531,
      "step": 2060
    },
    {
      "epoch": 0.42032590486826743,
      "grad_norm": 0.33554673194885254,
      "learning_rate": 0.00012900022568269014,
      "loss": 0.4594,
      "step": 2070
    },
    {
      "epoch": 0.42235646479516725,
      "grad_norm": 0.31176984310150146,
      "learning_rate": 0.0001285488603024148,
      "loss": 0.4691,
      "step": 2080
    },
    {
      "epoch": 0.42438702472206713,
      "grad_norm": 0.39696475863456726,
      "learning_rate": 0.00012809749492213946,
      "loss": 0.4688,
      "step": 2090
    },
    {
      "epoch": 0.42641758464896695,
      "grad_norm": 0.3407701253890991,
      "learning_rate": 0.00012764612954186414,
      "loss": 0.4462,
      "step": 2100
    },
    {
      "epoch": 0.4284481445758668,
      "grad_norm": 0.30477285385131836,
      "learning_rate": 0.0001271947641615888,
      "loss": 0.4393,
      "step": 2110
    },
    {
      "epoch": 0.43047870450276665,
      "grad_norm": 0.34559500217437744,
      "learning_rate": 0.00012674339878131346,
      "loss": 0.4334,
      "step": 2120
    },
    {
      "epoch": 0.4325092644296665,
      "grad_norm": 0.28418993949890137,
      "learning_rate": 0.00012629203340103814,
      "loss": 0.4467,
      "step": 2130
    },
    {
      "epoch": 0.4345398243565663,
      "grad_norm": 0.32702818512916565,
      "learning_rate": 0.0001258406680207628,
      "loss": 0.4837,
      "step": 2140
    },
    {
      "epoch": 0.4365703842834662,
      "grad_norm": 0.3287772834300995,
      "learning_rate": 0.00012538930264048746,
      "loss": 0.4392,
      "step": 2150
    },
    {
      "epoch": 0.438600944210366,
      "grad_norm": 0.312008261680603,
      "learning_rate": 0.00012493793726021214,
      "loss": 0.4327,
      "step": 2160
    },
    {
      "epoch": 0.4406315041372659,
      "grad_norm": 0.30065345764160156,
      "learning_rate": 0.0001244865718799368,
      "loss": 0.4605,
      "step": 2170
    },
    {
      "epoch": 0.4426620640641657,
      "grad_norm": 0.30942678451538086,
      "learning_rate": 0.00012403520649966146,
      "loss": 0.4754,
      "step": 2180
    },
    {
      "epoch": 0.4446926239910655,
      "grad_norm": 0.3628658950328827,
      "learning_rate": 0.00012358384111938614,
      "loss": 0.464,
      "step": 2190
    },
    {
      "epoch": 0.4467231839179654,
      "grad_norm": 0.2923509478569031,
      "learning_rate": 0.0001231324757391108,
      "loss": 0.4532,
      "step": 2200
    },
    {
      "epoch": 0.4487537438448652,
      "grad_norm": 0.3319997191429138,
      "learning_rate": 0.0001226811103588355,
      "loss": 0.4553,
      "step": 2210
    },
    {
      "epoch": 0.45078430377176504,
      "grad_norm": 0.36106452345848083,
      "learning_rate": 0.00012222974497856014,
      "loss": 0.4857,
      "step": 2220
    },
    {
      "epoch": 0.4528148636986649,
      "grad_norm": 0.28438377380371094,
      "learning_rate": 0.00012177837959828482,
      "loss": 0.4865,
      "step": 2230
    },
    {
      "epoch": 0.45484542362556474,
      "grad_norm": 0.31037792563438416,
      "learning_rate": 0.0001213270142180095,
      "loss": 0.4606,
      "step": 2240
    },
    {
      "epoch": 0.4568759835524646,
      "grad_norm": 0.33540207147598267,
      "learning_rate": 0.00012087564883773415,
      "loss": 0.3989,
      "step": 2250
    },
    {
      "epoch": 0.45890654347936444,
      "grad_norm": 0.3565574884414673,
      "learning_rate": 0.00012042428345745882,
      "loss": 0.4373,
      "step": 2260
    },
    {
      "epoch": 0.46093710340626426,
      "grad_norm": 0.2983192205429077,
      "learning_rate": 0.0001199729180771835,
      "loss": 0.5097,
      "step": 2270
    },
    {
      "epoch": 0.46296766333316414,
      "grad_norm": 0.3353491425514221,
      "learning_rate": 0.00011952155269690815,
      "loss": 0.491,
      "step": 2280
    },
    {
      "epoch": 0.46499822326006396,
      "grad_norm": 0.32462674379348755,
      "learning_rate": 0.00011907018731663282,
      "loss": 0.4381,
      "step": 2290
    },
    {
      "epoch": 0.4670287831869638,
      "grad_norm": 0.38128891587257385,
      "learning_rate": 0.0001186188219363575,
      "loss": 0.4621,
      "step": 2300
    },
    {
      "epoch": 0.46905934311386366,
      "grad_norm": 0.2945195138454437,
      "learning_rate": 0.00011816745655608215,
      "loss": 0.4533,
      "step": 2310
    },
    {
      "epoch": 0.4710899030407635,
      "grad_norm": 0.3313511312007904,
      "learning_rate": 0.00011771609117580682,
      "loss": 0.4502,
      "step": 2320
    },
    {
      "epoch": 0.47312046296766336,
      "grad_norm": 0.3481079339981079,
      "learning_rate": 0.0001172647257955315,
      "loss": 0.4411,
      "step": 2330
    },
    {
      "epoch": 0.4751510228945632,
      "grad_norm": 0.29668107628822327,
      "learning_rate": 0.00011681336041525615,
      "loss": 0.4457,
      "step": 2340
    },
    {
      "epoch": 0.477181582821463,
      "grad_norm": 0.3552730083465576,
      "learning_rate": 0.00011636199503498082,
      "loss": 0.4679,
      "step": 2350
    },
    {
      "epoch": 0.4792121427483629,
      "grad_norm": 0.3188958466053009,
      "learning_rate": 0.0001159106296547055,
      "loss": 0.4528,
      "step": 2360
    },
    {
      "epoch": 0.4812427026752627,
      "grad_norm": 0.24433711171150208,
      "learning_rate": 0.00011545926427443017,
      "loss": 0.4506,
      "step": 2370
    },
    {
      "epoch": 0.4832732626021625,
      "grad_norm": 0.42612355947494507,
      "learning_rate": 0.00011500789889415482,
      "loss": 0.4698,
      "step": 2380
    },
    {
      "epoch": 0.4853038225290624,
      "grad_norm": 0.32569369673728943,
      "learning_rate": 0.0001145565335138795,
      "loss": 0.5035,
      "step": 2390
    },
    {
      "epoch": 0.4873343824559622,
      "grad_norm": 0.31768354773521423,
      "learning_rate": 0.00011410516813360417,
      "loss": 0.4513,
      "step": 2400
    },
    {
      "epoch": 0.4893649423828621,
      "grad_norm": 0.28624480962753296,
      "learning_rate": 0.00011365380275332882,
      "loss": 0.5085,
      "step": 2410
    },
    {
      "epoch": 0.4913955023097619,
      "grad_norm": 0.33392205834388733,
      "learning_rate": 0.0001132024373730535,
      "loss": 0.4476,
      "step": 2420
    },
    {
      "epoch": 0.49342606223666174,
      "grad_norm": 0.29283642768859863,
      "learning_rate": 0.00011275107199277817,
      "loss": 0.4466,
      "step": 2430
    },
    {
      "epoch": 0.4954566221635616,
      "grad_norm": 0.33005380630493164,
      "learning_rate": 0.00011229970661250282,
      "loss": 0.4516,
      "step": 2440
    },
    {
      "epoch": 0.49748718209046144,
      "grad_norm": 0.27331915497779846,
      "learning_rate": 0.0001118483412322275,
      "loss": 0.4064,
      "step": 2450
    },
    {
      "epoch": 0.49951774201736127,
      "grad_norm": 0.2887941300868988,
      "learning_rate": 0.00011139697585195217,
      "loss": 0.4613,
      "step": 2460
    },
    {
      "epoch": 0.5015483019442611,
      "grad_norm": 0.3040357828140259,
      "learning_rate": 0.00011094561047167682,
      "loss": 0.3966,
      "step": 2470
    },
    {
      "epoch": 0.503578861871161,
      "grad_norm": 0.3701394498348236,
      "learning_rate": 0.0001104942450914015,
      "loss": 0.4456,
      "step": 2480
    },
    {
      "epoch": 0.5056094217980608,
      "grad_norm": 0.28752270340919495,
      "learning_rate": 0.00011004287971112617,
      "loss": 0.4645,
      "step": 2490
    },
    {
      "epoch": 0.5076399817249606,
      "grad_norm": 0.35055771470069885,
      "learning_rate": 0.00010959151433085082,
      "loss": 0.4555,
      "step": 2500
    },
    {
      "epoch": 0.5096705416518605,
      "grad_norm": 0.3273162841796875,
      "learning_rate": 0.0001091401489505755,
      "loss": 0.4573,
      "step": 2510
    },
    {
      "epoch": 0.5117011015787604,
      "grad_norm": 0.3267269730567932,
      "learning_rate": 0.00010868878357030017,
      "loss": 0.435,
      "step": 2520
    },
    {
      "epoch": 0.5137316615056602,
      "grad_norm": 0.3092781901359558,
      "learning_rate": 0.00010823741819002482,
      "loss": 0.4678,
      "step": 2530
    },
    {
      "epoch": 0.51576222143256,
      "grad_norm": 0.33799588680267334,
      "learning_rate": 0.0001077860528097495,
      "loss": 0.4364,
      "step": 2540
    },
    {
      "epoch": 0.5177927813594598,
      "grad_norm": 0.2993481159210205,
      "learning_rate": 0.00010733468742947417,
      "loss": 0.4661,
      "step": 2550
    },
    {
      "epoch": 0.5198233412863598,
      "grad_norm": 0.31829118728637695,
      "learning_rate": 0.00010688332204919885,
      "loss": 0.4517,
      "step": 2560
    },
    {
      "epoch": 0.5218539012132596,
      "grad_norm": 0.30726006627082825,
      "learning_rate": 0.0001064319566689235,
      "loss": 0.4332,
      "step": 2570
    },
    {
      "epoch": 0.5238844611401594,
      "grad_norm": 0.3422594368457794,
      "learning_rate": 0.00010598059128864817,
      "loss": 0.4756,
      "step": 2580
    },
    {
      "epoch": 0.5259150210670592,
      "grad_norm": 0.3772375285625458,
      "learning_rate": 0.00010552922590837284,
      "loss": 0.4441,
      "step": 2590
    },
    {
      "epoch": 0.527945580993959,
      "grad_norm": 0.36479806900024414,
      "learning_rate": 0.0001050778605280975,
      "loss": 0.4292,
      "step": 2600
    },
    {
      "epoch": 0.5299761409208589,
      "grad_norm": 0.26694270968437195,
      "learning_rate": 0.00010462649514782217,
      "loss": 0.437,
      "step": 2610
    },
    {
      "epoch": 0.5320067008477588,
      "grad_norm": 0.31212756037712097,
      "learning_rate": 0.00010417512976754684,
      "loss": 0.4618,
      "step": 2620
    },
    {
      "epoch": 0.5340372607746586,
      "grad_norm": 0.3570683002471924,
      "learning_rate": 0.00010372376438727149,
      "loss": 0.4756,
      "step": 2630
    },
    {
      "epoch": 0.5360678207015585,
      "grad_norm": 0.34033411741256714,
      "learning_rate": 0.00010327239900699617,
      "loss": 0.4464,
      "step": 2640
    },
    {
      "epoch": 0.5380983806284583,
      "grad_norm": 0.283494234085083,
      "learning_rate": 0.00010282103362672084,
      "loss": 0.4342,
      "step": 2650
    },
    {
      "epoch": 0.5401289405553581,
      "grad_norm": 0.32951289415359497,
      "learning_rate": 0.00010236966824644549,
      "loss": 0.4428,
      "step": 2660
    },
    {
      "epoch": 0.542159500482258,
      "grad_norm": 0.3191010355949402,
      "learning_rate": 0.00010191830286617017,
      "loss": 0.5015,
      "step": 2670
    },
    {
      "epoch": 0.5441900604091578,
      "grad_norm": 0.33143115043640137,
      "learning_rate": 0.00010146693748589484,
      "loss": 0.4812,
      "step": 2680
    },
    {
      "epoch": 0.5462206203360577,
      "grad_norm": 0.3153349757194519,
      "learning_rate": 0.00010101557210561949,
      "loss": 0.4484,
      "step": 2690
    },
    {
      "epoch": 0.5482511802629575,
      "grad_norm": 0.32009074091911316,
      "learning_rate": 0.00010056420672534417,
      "loss": 0.4889,
      "step": 2700
    },
    {
      "epoch": 0.5502817401898573,
      "grad_norm": 0.3426816761493683,
      "learning_rate": 0.00010011284134506884,
      "loss": 0.4433,
      "step": 2710
    },
    {
      "epoch": 0.5523123001167572,
      "grad_norm": 0.30117106437683105,
      "learning_rate": 9.96614759647935e-05,
      "loss": 0.4271,
      "step": 2720
    },
    {
      "epoch": 0.5543428600436571,
      "grad_norm": 0.31539225578308105,
      "learning_rate": 9.921011058451817e-05,
      "loss": 0.4743,
      "step": 2730
    },
    {
      "epoch": 0.5563734199705569,
      "grad_norm": 0.3079093098640442,
      "learning_rate": 9.875874520424284e-05,
      "loss": 0.4795,
      "step": 2740
    },
    {
      "epoch": 0.5584039798974567,
      "grad_norm": 0.3536037802696228,
      "learning_rate": 9.83073798239675e-05,
      "loss": 0.4627,
      "step": 2750
    },
    {
      "epoch": 0.5604345398243565,
      "grad_norm": 0.3187549114227295,
      "learning_rate": 9.785601444369218e-05,
      "loss": 0.4952,
      "step": 2760
    },
    {
      "epoch": 0.5624650997512564,
      "grad_norm": 0.3154836595058441,
      "learning_rate": 9.740464906341684e-05,
      "loss": 0.4424,
      "step": 2770
    },
    {
      "epoch": 0.5644956596781563,
      "grad_norm": 0.34551212191581726,
      "learning_rate": 9.69532836831415e-05,
      "loss": 0.5204,
      "step": 2780
    },
    {
      "epoch": 0.5665262196050561,
      "grad_norm": 0.33295169472694397,
      "learning_rate": 9.650191830286618e-05,
      "loss": 0.4301,
      "step": 2790
    },
    {
      "epoch": 0.5685567795319559,
      "grad_norm": 0.3152225911617279,
      "learning_rate": 9.605055292259084e-05,
      "loss": 0.4538,
      "step": 2800
    },
    {
      "epoch": 0.5705873394588558,
      "grad_norm": 0.2897580862045288,
      "learning_rate": 9.55991875423155e-05,
      "loss": 0.4385,
      "step": 2810
    },
    {
      "epoch": 0.5726178993857556,
      "grad_norm": 0.3219383656978607,
      "learning_rate": 9.514782216204018e-05,
      "loss": 0.4819,
      "step": 2820
    },
    {
      "epoch": 0.5746484593126555,
      "grad_norm": 0.3159193694591522,
      "learning_rate": 9.469645678176484e-05,
      "loss": 0.4584,
      "step": 2830
    },
    {
      "epoch": 0.5766790192395553,
      "grad_norm": 0.27353182435035706,
      "learning_rate": 9.42450914014895e-05,
      "loss": 0.4102,
      "step": 2840
    },
    {
      "epoch": 0.5787095791664552,
      "grad_norm": 0.286489337682724,
      "learning_rate": 9.379372602121418e-05,
      "loss": 0.414,
      "step": 2850
    },
    {
      "epoch": 0.580740139093355,
      "grad_norm": 0.3086869418621063,
      "learning_rate": 9.334236064093884e-05,
      "loss": 0.4663,
      "step": 2860
    },
    {
      "epoch": 0.5827706990202548,
      "grad_norm": 0.2667883634567261,
      "learning_rate": 9.289099526066352e-05,
      "loss": 0.4566,
      "step": 2870
    },
    {
      "epoch": 0.5848012589471546,
      "grad_norm": 0.2517341673374176,
      "learning_rate": 9.243962988038818e-05,
      "loss": 0.3912,
      "step": 2880
    },
    {
      "epoch": 0.5868318188740546,
      "grad_norm": 0.3023528456687927,
      "learning_rate": 9.198826450011284e-05,
      "loss": 0.4673,
      "step": 2890
    },
    {
      "epoch": 0.5888623788009544,
      "grad_norm": 0.31993579864501953,
      "learning_rate": 9.153689911983751e-05,
      "loss": 0.4377,
      "step": 2900
    },
    {
      "epoch": 0.5908929387278542,
      "grad_norm": 0.3203275799751282,
      "learning_rate": 9.108553373956218e-05,
      "loss": 0.4292,
      "step": 2910
    },
    {
      "epoch": 0.592923498654754,
      "grad_norm": 0.2754373252391815,
      "learning_rate": 9.063416835928684e-05,
      "loss": 0.4119,
      "step": 2920
    },
    {
      "epoch": 0.5949540585816538,
      "grad_norm": 0.3039360046386719,
      "learning_rate": 9.018280297901151e-05,
      "loss": 0.4522,
      "step": 2930
    },
    {
      "epoch": 0.5969846185085538,
      "grad_norm": 0.27624839544296265,
      "learning_rate": 8.973143759873618e-05,
      "loss": 0.4829,
      "step": 2940
    },
    {
      "epoch": 0.5990151784354536,
      "grad_norm": 0.4014182686805725,
      "learning_rate": 8.928007221846085e-05,
      "loss": 0.4645,
      "step": 2950
    },
    {
      "epoch": 0.6010457383623534,
      "grad_norm": 0.3507515490055084,
      "learning_rate": 8.882870683818551e-05,
      "loss": 0.4608,
      "step": 2960
    },
    {
      "epoch": 0.6030762982892532,
      "grad_norm": 0.3458273410797119,
      "learning_rate": 8.837734145791018e-05,
      "loss": 0.4435,
      "step": 2970
    },
    {
      "epoch": 0.6051068582161531,
      "grad_norm": 0.30249306559562683,
      "learning_rate": 8.792597607763485e-05,
      "loss": 0.4562,
      "step": 2980
    },
    {
      "epoch": 0.607137418143053,
      "grad_norm": 0.31352686882019043,
      "learning_rate": 8.747461069735951e-05,
      "loss": 0.4473,
      "step": 2990
    },
    {
      "epoch": 0.6091679780699528,
      "grad_norm": 0.3868395686149597,
      "learning_rate": 8.702324531708418e-05,
      "loss": 0.456,
      "step": 3000
    },
    {
      "epoch": 0.6111985379968526,
      "grad_norm": 0.3381800353527069,
      "learning_rate": 8.657187993680885e-05,
      "loss": 0.4413,
      "step": 3010
    },
    {
      "epoch": 0.6132290979237525,
      "grad_norm": 0.27119091153144836,
      "learning_rate": 8.612051455653351e-05,
      "loss": 0.4323,
      "step": 3020
    },
    {
      "epoch": 0.6152596578506523,
      "grad_norm": 0.3246462643146515,
      "learning_rate": 8.566914917625819e-05,
      "loss": 0.4605,
      "step": 3030
    },
    {
      "epoch": 0.6172902177775521,
      "grad_norm": 0.2898504137992859,
      "learning_rate": 8.521778379598285e-05,
      "loss": 0.4804,
      "step": 3040
    },
    {
      "epoch": 0.619320777704452,
      "grad_norm": 0.279544472694397,
      "learning_rate": 8.476641841570751e-05,
      "loss": 0.4544,
      "step": 3050
    },
    {
      "epoch": 0.6213513376313519,
      "grad_norm": 0.22047270834445953,
      "learning_rate": 8.431505303543219e-05,
      "loss": 0.4391,
      "step": 3060
    },
    {
      "epoch": 0.6233818975582517,
      "grad_norm": 0.36862918734550476,
      "learning_rate": 8.386368765515685e-05,
      "loss": 0.4394,
      "step": 3070
    },
    {
      "epoch": 0.6254124574851515,
      "grad_norm": 0.3542881906032562,
      "learning_rate": 8.341232227488151e-05,
      "loss": 0.4811,
      "step": 3080
    },
    {
      "epoch": 0.6274430174120513,
      "grad_norm": 0.3001244068145752,
      "learning_rate": 8.296095689460619e-05,
      "loss": 0.4757,
      "step": 3090
    },
    {
      "epoch": 0.6294735773389513,
      "grad_norm": 0.3549037277698517,
      "learning_rate": 8.250959151433085e-05,
      "loss": 0.4417,
      "step": 3100
    },
    {
      "epoch": 0.6315041372658511,
      "grad_norm": 0.3466096818447113,
      "learning_rate": 8.205822613405552e-05,
      "loss": 0.4391,
      "step": 3110
    },
    {
      "epoch": 0.6335346971927509,
      "grad_norm": 0.3547740578651428,
      "learning_rate": 8.160686075378019e-05,
      "loss": 0.4558,
      "step": 3120
    },
    {
      "epoch": 0.6355652571196507,
      "grad_norm": 0.3445540964603424,
      "learning_rate": 8.115549537350485e-05,
      "loss": 0.4473,
      "step": 3130
    },
    {
      "epoch": 0.6375958170465506,
      "grad_norm": 0.29908791184425354,
      "learning_rate": 8.070412999322952e-05,
      "loss": 0.4155,
      "step": 3140
    },
    {
      "epoch": 0.6396263769734505,
      "grad_norm": 0.3640013635158539,
      "learning_rate": 8.025276461295419e-05,
      "loss": 0.478,
      "step": 3150
    },
    {
      "epoch": 0.6416569369003503,
      "grad_norm": 0.30696406960487366,
      "learning_rate": 7.980139923267885e-05,
      "loss": 0.4561,
      "step": 3160
    },
    {
      "epoch": 0.6436874968272501,
      "grad_norm": 0.42321425676345825,
      "learning_rate": 7.935003385240352e-05,
      "loss": 0.4086,
      "step": 3170
    },
    {
      "epoch": 0.64571805675415,
      "grad_norm": 0.313687801361084,
      "learning_rate": 7.889866847212819e-05,
      "loss": 0.4354,
      "step": 3180
    },
    {
      "epoch": 0.6477486166810498,
      "grad_norm": 0.31149017810821533,
      "learning_rate": 7.844730309185286e-05,
      "loss": 0.4557,
      "step": 3190
    },
    {
      "epoch": 0.6497791766079496,
      "grad_norm": 0.32916808128356934,
      "learning_rate": 7.799593771157752e-05,
      "loss": 0.438,
      "step": 3200
    },
    {
      "epoch": 0.6518097365348495,
      "grad_norm": 0.32690513134002686,
      "learning_rate": 7.754457233130218e-05,
      "loss": 0.444,
      "step": 3210
    },
    {
      "epoch": 0.6538402964617493,
      "grad_norm": 0.30711984634399414,
      "learning_rate": 7.709320695102686e-05,
      "loss": 0.442,
      "step": 3220
    },
    {
      "epoch": 0.6558708563886492,
      "grad_norm": 0.31329619884490967,
      "learning_rate": 7.664184157075152e-05,
      "loss": 0.4552,
      "step": 3230
    },
    {
      "epoch": 0.657901416315549,
      "grad_norm": 0.26039692759513855,
      "learning_rate": 7.619047619047618e-05,
      "loss": 0.4487,
      "step": 3240
    },
    {
      "epoch": 0.6599319762424488,
      "grad_norm": 0.3030105233192444,
      "learning_rate": 7.573911081020086e-05,
      "loss": 0.4619,
      "step": 3250
    },
    {
      "epoch": 0.6619625361693487,
      "grad_norm": 0.33797264099121094,
      "learning_rate": 7.528774542992552e-05,
      "loss": 0.4232,
      "step": 3260
    },
    {
      "epoch": 0.6639930960962486,
      "grad_norm": 0.33093661069869995,
      "learning_rate": 7.483638004965018e-05,
      "loss": 0.4519,
      "step": 3270
    },
    {
      "epoch": 0.6660236560231484,
      "grad_norm": 0.25176548957824707,
      "learning_rate": 7.438501466937486e-05,
      "loss": 0.4299,
      "step": 3280
    },
    {
      "epoch": 0.6680542159500482,
      "grad_norm": 0.29229700565338135,
      "learning_rate": 7.393364928909952e-05,
      "loss": 0.4326,
      "step": 3290
    },
    {
      "epoch": 0.670084775876948,
      "grad_norm": 0.33005061745643616,
      "learning_rate": 7.34822839088242e-05,
      "loss": 0.4582,
      "step": 3300
    },
    {
      "epoch": 0.6721153358038479,
      "grad_norm": 0.32569319009780884,
      "learning_rate": 7.303091852854886e-05,
      "loss": 0.4327,
      "step": 3310
    },
    {
      "epoch": 0.6741458957307478,
      "grad_norm": 0.27885472774505615,
      "learning_rate": 7.257955314827352e-05,
      "loss": 0.4068,
      "step": 3320
    },
    {
      "epoch": 0.6761764556576476,
      "grad_norm": 0.3060564696788788,
      "learning_rate": 7.21281877679982e-05,
      "loss": 0.4393,
      "step": 3330
    },
    {
      "epoch": 0.6782070155845474,
      "grad_norm": 0.30439481139183044,
      "learning_rate": 7.167682238772286e-05,
      "loss": 0.4174,
      "step": 3340
    },
    {
      "epoch": 0.6802375755114473,
      "grad_norm": 0.35958611965179443,
      "learning_rate": 7.122545700744752e-05,
      "loss": 0.431,
      "step": 3350
    },
    {
      "epoch": 0.6822681354383471,
      "grad_norm": 0.31628406047821045,
      "learning_rate": 7.07740916271722e-05,
      "loss": 0.4466,
      "step": 3360
    },
    {
      "epoch": 0.684298695365247,
      "grad_norm": 0.28758397698402405,
      "learning_rate": 7.032272624689686e-05,
      "loss": 0.4466,
      "step": 3370
    },
    {
      "epoch": 0.6863292552921468,
      "grad_norm": 0.3175029456615448,
      "learning_rate": 6.987136086662153e-05,
      "loss": 0.4284,
      "step": 3380
    },
    {
      "epoch": 0.6883598152190467,
      "grad_norm": 0.3289939761161804,
      "learning_rate": 6.94199954863462e-05,
      "loss": 0.4456,
      "step": 3390
    },
    {
      "epoch": 0.6903903751459465,
      "grad_norm": 0.36997491121292114,
      "learning_rate": 6.896863010607086e-05,
      "loss": 0.4591,
      "step": 3400
    },
    {
      "epoch": 0.6924209350728463,
      "grad_norm": 0.31562626361846924,
      "learning_rate": 6.851726472579553e-05,
      "loss": 0.4104,
      "step": 3410
    },
    {
      "epoch": 0.6944514949997462,
      "grad_norm": 0.3092348575592041,
      "learning_rate": 6.80658993455202e-05,
      "loss": 0.4219,
      "step": 3420
    },
    {
      "epoch": 0.696482054926646,
      "grad_norm": 0.2985419034957886,
      "learning_rate": 6.761453396524487e-05,
      "loss": 0.4571,
      "step": 3430
    },
    {
      "epoch": 0.6985126148535459,
      "grad_norm": 0.2949378788471222,
      "learning_rate": 6.716316858496953e-05,
      "loss": 0.4271,
      "step": 3440
    },
    {
      "epoch": 0.7005431747804457,
      "grad_norm": 0.3256753385066986,
      "learning_rate": 6.671180320469421e-05,
      "loss": 0.4396,
      "step": 3450
    },
    {
      "epoch": 0.7025737347073455,
      "grad_norm": 0.3251384198665619,
      "learning_rate": 6.626043782441887e-05,
      "loss": 0.444,
      "step": 3460
    },
    {
      "epoch": 0.7046042946342453,
      "grad_norm": 0.31145650148391724,
      "learning_rate": 6.580907244414353e-05,
      "loss": 0.4606,
      "step": 3470
    },
    {
      "epoch": 0.7066348545611453,
      "grad_norm": 0.30666106939315796,
      "learning_rate": 6.535770706386821e-05,
      "loss": 0.4405,
      "step": 3480
    },
    {
      "epoch": 0.7086654144880451,
      "grad_norm": 0.2639696002006531,
      "learning_rate": 6.490634168359287e-05,
      "loss": 0.445,
      "step": 3490
    },
    {
      "epoch": 0.7106959744149449,
      "grad_norm": 0.2844054400920868,
      "learning_rate": 6.445497630331754e-05,
      "loss": 0.4303,
      "step": 3500
    },
    {
      "epoch": 0.7127265343418447,
      "grad_norm": 0.34349071979522705,
      "learning_rate": 6.40036109230422e-05,
      "loss": 0.4256,
      "step": 3510
    },
    {
      "epoch": 0.7147570942687446,
      "grad_norm": 0.2534099519252777,
      "learning_rate": 6.355224554276687e-05,
      "loss": 0.4611,
      "step": 3520
    },
    {
      "epoch": 0.7167876541956445,
      "grad_norm": 0.33200860023498535,
      "learning_rate": 6.310088016249154e-05,
      "loss": 0.4153,
      "step": 3530
    },
    {
      "epoch": 0.7188182141225443,
      "grad_norm": 0.3540104329586029,
      "learning_rate": 6.26495147822162e-05,
      "loss": 0.4515,
      "step": 3540
    },
    {
      "epoch": 0.7208487740494441,
      "grad_norm": 0.30501240491867065,
      "learning_rate": 6.219814940194088e-05,
      "loss": 0.4715,
      "step": 3550
    },
    {
      "epoch": 0.722879333976344,
      "grad_norm": 0.2880973517894745,
      "learning_rate": 6.174678402166554e-05,
      "loss": 0.439,
      "step": 3560
    },
    {
      "epoch": 0.7249098939032438,
      "grad_norm": 0.34371182322502136,
      "learning_rate": 6.12954186413902e-05,
      "loss": 0.3978,
      "step": 3570
    },
    {
      "epoch": 0.7269404538301436,
      "grad_norm": 0.3624887466430664,
      "learning_rate": 6.0844053261114874e-05,
      "loss": 0.4129,
      "step": 3580
    },
    {
      "epoch": 0.7289710137570435,
      "grad_norm": 0.3106871247291565,
      "learning_rate": 6.0392687880839536e-05,
      "loss": 0.4303,
      "step": 3590
    },
    {
      "epoch": 0.7310015736839434,
      "grad_norm": 0.34182989597320557,
      "learning_rate": 5.994132250056421e-05,
      "loss": 0.4676,
      "step": 3600
    },
    {
      "epoch": 0.7330321336108432,
      "grad_norm": 0.3725891709327698,
      "learning_rate": 5.9489957120288874e-05,
      "loss": 0.4404,
      "step": 3610
    },
    {
      "epoch": 0.735062693537743,
      "grad_norm": 0.3169934153556824,
      "learning_rate": 5.903859174001355e-05,
      "loss": 0.4522,
      "step": 3620
    },
    {
      "epoch": 0.7370932534646428,
      "grad_norm": 0.2764187455177307,
      "learning_rate": 5.858722635973821e-05,
      "loss": 0.4286,
      "step": 3630
    },
    {
      "epoch": 0.7391238133915428,
      "grad_norm": 0.37726834416389465,
      "learning_rate": 5.813586097946287e-05,
      "loss": 0.4443,
      "step": 3640
    },
    {
      "epoch": 0.7411543733184426,
      "grad_norm": 0.31952452659606934,
      "learning_rate": 5.768449559918755e-05,
      "loss": 0.4675,
      "step": 3650
    },
    {
      "epoch": 0.7431849332453424,
      "grad_norm": 0.3780157268047333,
      "learning_rate": 5.723313021891221e-05,
      "loss": 0.4971,
      "step": 3660
    },
    {
      "epoch": 0.7452154931722422,
      "grad_norm": 0.32312580943107605,
      "learning_rate": 5.678176483863687e-05,
      "loss": 0.4534,
      "step": 3670
    },
    {
      "epoch": 0.747246053099142,
      "grad_norm": 0.41143912076950073,
      "learning_rate": 5.633039945836155e-05,
      "loss": 0.4752,
      "step": 3680
    },
    {
      "epoch": 0.749276613026042,
      "grad_norm": 0.31553158164024353,
      "learning_rate": 5.587903407808621e-05,
      "loss": 0.4339,
      "step": 3690
    },
    {
      "epoch": 0.7513071729529418,
      "grad_norm": 0.27183452248573303,
      "learning_rate": 5.542766869781087e-05,
      "loss": 0.4262,
      "step": 3700
    },
    {
      "epoch": 0.7533377328798416,
      "grad_norm": 0.32945436239242554,
      "learning_rate": 5.497630331753555e-05,
      "loss": 0.4082,
      "step": 3710
    },
    {
      "epoch": 0.7553682928067414,
      "grad_norm": 0.2611777186393738,
      "learning_rate": 5.452493793726021e-05,
      "loss": 0.4411,
      "step": 3720
    },
    {
      "epoch": 0.7573988527336413,
      "grad_norm": 0.3144536018371582,
      "learning_rate": 5.4073572556984885e-05,
      "loss": 0.4172,
      "step": 3730
    },
    {
      "epoch": 0.7594294126605411,
      "grad_norm": 0.30025094747543335,
      "learning_rate": 5.362220717670955e-05,
      "loss": 0.3991,
      "step": 3740
    },
    {
      "epoch": 0.761459972587441,
      "grad_norm": 0.3310229182243347,
      "learning_rate": 5.317084179643421e-05,
      "loss": 0.4732,
      "step": 3750
    },
    {
      "epoch": 0.7634905325143408,
      "grad_norm": 0.29071497917175293,
      "learning_rate": 5.2719476416158885e-05,
      "loss": 0.4462,
      "step": 3760
    },
    {
      "epoch": 0.7655210924412407,
      "grad_norm": 0.35301271080970764,
      "learning_rate": 5.2268111035883547e-05,
      "loss": 0.4883,
      "step": 3770
    },
    {
      "epoch": 0.7675516523681405,
      "grad_norm": 0.26512888073921204,
      "learning_rate": 5.1816745655608215e-05,
      "loss": 0.414,
      "step": 3780
    },
    {
      "epoch": 0.7695822122950403,
      "grad_norm": 0.30075064301490784,
      "learning_rate": 5.1365380275332884e-05,
      "loss": 0.4776,
      "step": 3790
    },
    {
      "epoch": 0.7716127722219402,
      "grad_norm": 0.26458650827407837,
      "learning_rate": 5.0914014895057546e-05,
      "loss": 0.4242,
      "step": 3800
    },
    {
      "epoch": 0.7736433321488401,
      "grad_norm": 0.3457193672657013,
      "learning_rate": 5.046264951478222e-05,
      "loss": 0.4748,
      "step": 3810
    },
    {
      "epoch": 0.7756738920757399,
      "grad_norm": 0.31483811140060425,
      "learning_rate": 5.0011284134506883e-05,
      "loss": 0.4507,
      "step": 3820
    },
    {
      "epoch": 0.7777044520026397,
      "grad_norm": 0.3277527987957001,
      "learning_rate": 4.955991875423155e-05,
      "loss": 0.4471,
      "step": 3830
    },
    {
      "epoch": 0.7797350119295395,
      "grad_norm": 0.3846767842769623,
      "learning_rate": 4.910855337395622e-05,
      "loss": 0.4284,
      "step": 3840
    },
    {
      "epoch": 0.7817655718564395,
      "grad_norm": 0.2813135087490082,
      "learning_rate": 4.865718799368088e-05,
      "loss": 0.4107,
      "step": 3850
    },
    {
      "epoch": 0.7837961317833393,
      "grad_norm": 0.2546825110912323,
      "learning_rate": 4.820582261340555e-05,
      "loss": 0.4049,
      "step": 3860
    },
    {
      "epoch": 0.7858266917102391,
      "grad_norm": 0.3389606177806854,
      "learning_rate": 4.775445723313022e-05,
      "loss": 0.4457,
      "step": 3870
    },
    {
      "epoch": 0.7878572516371389,
      "grad_norm": 0.2934193015098572,
      "learning_rate": 4.730309185285489e-05,
      "loss": 0.4879,
      "step": 3880
    },
    {
      "epoch": 0.7898878115640388,
      "grad_norm": 0.32167044281959534,
      "learning_rate": 4.685172647257956e-05,
      "loss": 0.447,
      "step": 3890
    },
    {
      "epoch": 0.7919183714909386,
      "grad_norm": 0.25079795718193054,
      "learning_rate": 4.640036109230422e-05,
      "loss": 0.4483,
      "step": 3900
    },
    {
      "epoch": 0.7939489314178385,
      "grad_norm": 0.31524166464805603,
      "learning_rate": 4.594899571202889e-05,
      "loss": 0.4827,
      "step": 3910
    },
    {
      "epoch": 0.7959794913447383,
      "grad_norm": 0.33652210235595703,
      "learning_rate": 4.549763033175356e-05,
      "loss": 0.4488,
      "step": 3920
    },
    {
      "epoch": 0.7980100512716382,
      "grad_norm": 0.2767811417579651,
      "learning_rate": 4.5046264951478226e-05,
      "loss": 0.3972,
      "step": 3930
    },
    {
      "epoch": 0.800040611198538,
      "grad_norm": 0.3339448571205139,
      "learning_rate": 4.4594899571202895e-05,
      "loss": 0.425,
      "step": 3940
    },
    {
      "epoch": 0.8020711711254378,
      "grad_norm": 0.3183124363422394,
      "learning_rate": 4.414353419092756e-05,
      "loss": 0.4761,
      "step": 3950
    },
    {
      "epoch": 0.8041017310523377,
      "grad_norm": 0.32019126415252686,
      "learning_rate": 4.3692168810652226e-05,
      "loss": 0.4117,
      "step": 3960
    },
    {
      "epoch": 0.8061322909792376,
      "grad_norm": 0.34191054105758667,
      "learning_rate": 4.3240803430376894e-05,
      "loss": 0.4894,
      "step": 3970
    },
    {
      "epoch": 0.8081628509061374,
      "grad_norm": 0.3038153350353241,
      "learning_rate": 4.278943805010156e-05,
      "loss": 0.4455,
      "step": 3980
    },
    {
      "epoch": 0.8101934108330372,
      "grad_norm": 0.32275843620300293,
      "learning_rate": 4.233807266982623e-05,
      "loss": 0.4186,
      "step": 3990
    },
    {
      "epoch": 0.812223970759937,
      "grad_norm": 0.30464479327201843,
      "learning_rate": 4.1886707289550894e-05,
      "loss": 0.4882,
      "step": 4000
    },
    {
      "epoch": 0.8142545306868368,
      "grad_norm": 0.2825615406036377,
      "learning_rate": 4.143534190927556e-05,
      "loss": 0.4435,
      "step": 4010
    },
    {
      "epoch": 0.8162850906137368,
      "grad_norm": 0.3474159240722656,
      "learning_rate": 4.098397652900023e-05,
      "loss": 0.4225,
      "step": 4020
    },
    {
      "epoch": 0.8183156505406366,
      "grad_norm": 0.3120651841163635,
      "learning_rate": 4.05326111487249e-05,
      "loss": 0.4065,
      "step": 4030
    },
    {
      "epoch": 0.8203462104675364,
      "grad_norm": 0.3439611792564392,
      "learning_rate": 4.008124576844956e-05,
      "loss": 0.4438,
      "step": 4040
    },
    {
      "epoch": 0.8223767703944362,
      "grad_norm": 0.28599369525909424,
      "learning_rate": 3.962988038817423e-05,
      "loss": 0.4178,
      "step": 4050
    },
    {
      "epoch": 0.8244073303213361,
      "grad_norm": 0.2905454635620117,
      "learning_rate": 3.91785150078989e-05,
      "loss": 0.4551,
      "step": 4060
    },
    {
      "epoch": 0.826437890248236,
      "grad_norm": 0.34180620312690735,
      "learning_rate": 3.872714962762357e-05,
      "loss": 0.4355,
      "step": 4070
    },
    {
      "epoch": 0.8284684501751358,
      "grad_norm": 0.2997680604457855,
      "learning_rate": 3.827578424734823e-05,
      "loss": 0.4278,
      "step": 4080
    },
    {
      "epoch": 0.8304990101020356,
      "grad_norm": 0.22145986557006836,
      "learning_rate": 3.78244188670729e-05,
      "loss": 0.4032,
      "step": 4090
    },
    {
      "epoch": 0.8325295700289355,
      "grad_norm": 0.29043060541152954,
      "learning_rate": 3.737305348679757e-05,
      "loss": 0.4593,
      "step": 4100
    },
    {
      "epoch": 0.8345601299558353,
      "grad_norm": 0.3501964807510376,
      "learning_rate": 3.6921688106522236e-05,
      "loss": 0.4795,
      "step": 4110
    },
    {
      "epoch": 0.8365906898827352,
      "grad_norm": 0.3190287947654724,
      "learning_rate": 3.64703227262469e-05,
      "loss": 0.4093,
      "step": 4120
    },
    {
      "epoch": 0.838621249809635,
      "grad_norm": 0.3023875951766968,
      "learning_rate": 3.601895734597157e-05,
      "loss": 0.4018,
      "step": 4130
    },
    {
      "epoch": 0.8406518097365349,
      "grad_norm": 0.3303237855434418,
      "learning_rate": 3.5567591965696236e-05,
      "loss": 0.4614,
      "step": 4140
    },
    {
      "epoch": 0.8426823696634347,
      "grad_norm": 0.28546127676963806,
      "learning_rate": 3.51162265854209e-05,
      "loss": 0.4137,
      "step": 4150
    },
    {
      "epoch": 0.8447129295903345,
      "grad_norm": 0.3310924172401428,
      "learning_rate": 3.4664861205145567e-05,
      "loss": 0.468,
      "step": 4160
    },
    {
      "epoch": 0.8467434895172343,
      "grad_norm": 0.2701350450515747,
      "learning_rate": 3.4213495824870235e-05,
      "loss": 0.4795,
      "step": 4170
    },
    {
      "epoch": 0.8487740494441343,
      "grad_norm": 0.29710882902145386,
      "learning_rate": 3.3762130444594904e-05,
      "loss": 0.4169,
      "step": 4180
    },
    {
      "epoch": 0.8508046093710341,
      "grad_norm": 0.31172388792037964,
      "learning_rate": 3.3310765064319566e-05,
      "loss": 0.4585,
      "step": 4190
    },
    {
      "epoch": 0.8528351692979339,
      "grad_norm": 0.3226277232170105,
      "learning_rate": 3.2859399684044235e-05,
      "loss": 0.4304,
      "step": 4200
    },
    {
      "epoch": 0.8548657292248337,
      "grad_norm": 0.3212588131427765,
      "learning_rate": 3.2408034303768903e-05,
      "loss": 0.4624,
      "step": 4210
    },
    {
      "epoch": 0.8568962891517335,
      "grad_norm": 0.3396398425102234,
      "learning_rate": 3.195666892349357e-05,
      "loss": 0.4415,
      "step": 4220
    },
    {
      "epoch": 0.8589268490786335,
      "grad_norm": 0.28461259603500366,
      "learning_rate": 3.1505303543218234e-05,
      "loss": 0.4147,
      "step": 4230
    },
    {
      "epoch": 0.8609574090055333,
      "grad_norm": 0.29925620555877686,
      "learning_rate": 3.10539381629429e-05,
      "loss": 0.4456,
      "step": 4240
    },
    {
      "epoch": 0.8629879689324331,
      "grad_norm": 0.3242776393890381,
      "learning_rate": 3.060257278266757e-05,
      "loss": 0.4071,
      "step": 4250
    },
    {
      "epoch": 0.865018528859333,
      "grad_norm": 0.31815671920776367,
      "learning_rate": 3.015120740239224e-05,
      "loss": 0.4617,
      "step": 4260
    },
    {
      "epoch": 0.8670490887862328,
      "grad_norm": 0.27352476119995117,
      "learning_rate": 2.9699842022116902e-05,
      "loss": 0.4287,
      "step": 4270
    },
    {
      "epoch": 0.8690796487131326,
      "grad_norm": 0.26769956946372986,
      "learning_rate": 2.924847664184157e-05,
      "loss": 0.4292,
      "step": 4280
    },
    {
      "epoch": 0.8711102086400325,
      "grad_norm": 0.29947715997695923,
      "learning_rate": 2.879711126156624e-05,
      "loss": 0.4489,
      "step": 4290
    },
    {
      "epoch": 0.8731407685669323,
      "grad_norm": 0.32907065749168396,
      "learning_rate": 2.834574588129091e-05,
      "loss": 0.4544,
      "step": 4300
    },
    {
      "epoch": 0.8751713284938322,
      "grad_norm": 0.3068259358406067,
      "learning_rate": 2.789438050101557e-05,
      "loss": 0.4147,
      "step": 4310
    },
    {
      "epoch": 0.877201888420732,
      "grad_norm": 0.28988584876060486,
      "learning_rate": 2.744301512074024e-05,
      "loss": 0.4445,
      "step": 4320
    },
    {
      "epoch": 0.8792324483476318,
      "grad_norm": 0.2980601489543915,
      "learning_rate": 2.6991649740464908e-05,
      "loss": 0.4471,
      "step": 4330
    },
    {
      "epoch": 0.8812630082745317,
      "grad_norm": 0.31359732151031494,
      "learning_rate": 2.6540284360189577e-05,
      "loss": 0.4473,
      "step": 4340
    },
    {
      "epoch": 0.8832935682014316,
      "grad_norm": 0.284942090511322,
      "learning_rate": 2.608891897991424e-05,
      "loss": 0.3976,
      "step": 4350
    },
    {
      "epoch": 0.8853241281283314,
      "grad_norm": 0.3209346532821655,
      "learning_rate": 2.5637553599638908e-05,
      "loss": 0.4354,
      "step": 4360
    },
    {
      "epoch": 0.8873546880552312,
      "grad_norm": 0.2957816421985626,
      "learning_rate": 2.5186188219363576e-05,
      "loss": 0.4063,
      "step": 4370
    },
    {
      "epoch": 0.889385247982131,
      "grad_norm": 0.31564807891845703,
      "learning_rate": 2.473482283908824e-05,
      "loss": 0.4322,
      "step": 4380
    },
    {
      "epoch": 0.891415807909031,
      "grad_norm": 0.34027403593063354,
      "learning_rate": 2.428345745881291e-05,
      "loss": 0.4378,
      "step": 4390
    },
    {
      "epoch": 0.8934463678359308,
      "grad_norm": 0.34483709931373596,
      "learning_rate": 2.3832092078537576e-05,
      "loss": 0.4419,
      "step": 4400
    },
    {
      "epoch": 0.8954769277628306,
      "grad_norm": 0.2521377503871918,
      "learning_rate": 2.3380726698262244e-05,
      "loss": 0.4031,
      "step": 4410
    },
    {
      "epoch": 0.8975074876897304,
      "grad_norm": 0.3047831356525421,
      "learning_rate": 2.292936131798691e-05,
      "loss": 0.4544,
      "step": 4420
    },
    {
      "epoch": 0.8995380476166303,
      "grad_norm": 0.3123186230659485,
      "learning_rate": 2.247799593771158e-05,
      "loss": 0.4688,
      "step": 4430
    },
    {
      "epoch": 0.9015686075435301,
      "grad_norm": 0.28600379824638367,
      "learning_rate": 2.2026630557436244e-05,
      "loss": 0.4043,
      "step": 4440
    },
    {
      "epoch": 0.90359916747043,
      "grad_norm": 0.3245786130428314,
      "learning_rate": 2.1575265177160913e-05,
      "loss": 0.4484,
      "step": 4450
    },
    {
      "epoch": 0.9056297273973298,
      "grad_norm": 0.3383227586746216,
      "learning_rate": 2.1123899796885578e-05,
      "loss": 0.4632,
      "step": 4460
    },
    {
      "epoch": 0.9076602873242297,
      "grad_norm": 0.32677173614501953,
      "learning_rate": 2.0672534416610247e-05,
      "loss": 0.4496,
      "step": 4470
    },
    {
      "epoch": 0.9096908472511295,
      "grad_norm": 0.3170716166496277,
      "learning_rate": 2.0221169036334912e-05,
      "loss": 0.4446,
      "step": 4480
    },
    {
      "epoch": 0.9117214071780293,
      "grad_norm": 0.2948644757270813,
      "learning_rate": 1.976980365605958e-05,
      "loss": 0.4149,
      "step": 4490
    },
    {
      "epoch": 0.9137519671049292,
      "grad_norm": 0.26148921251296997,
      "learning_rate": 1.931843827578425e-05,
      "loss": 0.4394,
      "step": 4500
    },
    {
      "epoch": 0.915782527031829,
      "grad_norm": 0.2873954474925995,
      "learning_rate": 1.8867072895508915e-05,
      "loss": 0.3915,
      "step": 4510
    },
    {
      "epoch": 0.9178130869587289,
      "grad_norm": 0.24675175547599792,
      "learning_rate": 1.8415707515233584e-05,
      "loss": 0.4379,
      "step": 4520
    },
    {
      "epoch": 0.9198436468856287,
      "grad_norm": 0.3584192395210266,
      "learning_rate": 1.7964342134958252e-05,
      "loss": 0.4315,
      "step": 4530
    },
    {
      "epoch": 0.9218742068125285,
      "grad_norm": 0.3115171492099762,
      "learning_rate": 1.7512976754682918e-05,
      "loss": 0.4571,
      "step": 4540
    },
    {
      "epoch": 0.9239047667394285,
      "grad_norm": 0.3196638226509094,
      "learning_rate": 1.7061611374407587e-05,
      "loss": 0.4337,
      "step": 4550
    },
    {
      "epoch": 0.9259353266663283,
      "grad_norm": 0.301457941532135,
      "learning_rate": 1.6610245994132252e-05,
      "loss": 0.4879,
      "step": 4560
    },
    {
      "epoch": 0.9279658865932281,
      "grad_norm": 0.32981404662132263,
      "learning_rate": 1.615888061385692e-05,
      "loss": 0.4459,
      "step": 4570
    },
    {
      "epoch": 0.9299964465201279,
      "grad_norm": 0.30771350860595703,
      "learning_rate": 1.5707515233581586e-05,
      "loss": 0.4334,
      "step": 4580
    },
    {
      "epoch": 0.9320270064470277,
      "grad_norm": 0.33611178398132324,
      "learning_rate": 1.5256149853306251e-05,
      "loss": 0.4391,
      "step": 4590
    },
    {
      "epoch": 0.9340575663739276,
      "grad_norm": 0.321346640586853,
      "learning_rate": 1.480478447303092e-05,
      "loss": 0.4255,
      "step": 4600
    },
    {
      "epoch": 0.9360881263008275,
      "grad_norm": 0.3366367220878601,
      "learning_rate": 1.4353419092755585e-05,
      "loss": 0.5176,
      "step": 4610
    },
    {
      "epoch": 0.9381186862277273,
      "grad_norm": 0.2978334128856659,
      "learning_rate": 1.3902053712480254e-05,
      "loss": 0.3937,
      "step": 4620
    },
    {
      "epoch": 0.9401492461546271,
      "grad_norm": 0.26604631543159485,
      "learning_rate": 1.345068833220492e-05,
      "loss": 0.4741,
      "step": 4630
    },
    {
      "epoch": 0.942179806081527,
      "grad_norm": 0.34372177720069885,
      "learning_rate": 1.2999322951929588e-05,
      "loss": 0.4557,
      "step": 4640
    },
    {
      "epoch": 0.9442103660084268,
      "grad_norm": 0.2596277892589569,
      "learning_rate": 1.2547957571654254e-05,
      "loss": 0.405,
      "step": 4650
    },
    {
      "epoch": 0.9462409259353267,
      "grad_norm": 0.37350940704345703,
      "learning_rate": 1.2096592191378922e-05,
      "loss": 0.4175,
      "step": 4660
    },
    {
      "epoch": 0.9482714858622265,
      "grad_norm": 0.3574278652667999,
      "learning_rate": 1.164522681110359e-05,
      "loss": 0.5287,
      "step": 4670
    },
    {
      "epoch": 0.9503020457891264,
      "grad_norm": 0.3100239932537079,
      "learning_rate": 1.1193861430828256e-05,
      "loss": 0.3954,
      "step": 4680
    },
    {
      "epoch": 0.9523326057160262,
      "grad_norm": 0.3301067650318146,
      "learning_rate": 1.0742496050552924e-05,
      "loss": 0.4455,
      "step": 4690
    },
    {
      "epoch": 0.954363165642926,
      "grad_norm": 0.28347188234329224,
      "learning_rate": 1.029113067027759e-05,
      "loss": 0.4021,
      "step": 4700
    },
    {
      "epoch": 0.9563937255698258,
      "grad_norm": 0.32528769969940186,
      "learning_rate": 9.839765290002258e-06,
      "loss": 0.3957,
      "step": 4710
    },
    {
      "epoch": 0.9584242854967258,
      "grad_norm": 0.3124363124370575,
      "learning_rate": 9.388399909726925e-06,
      "loss": 0.4258,
      "step": 4720
    },
    {
      "epoch": 0.9604548454236256,
      "grad_norm": 0.29596635699272156,
      "learning_rate": 8.937034529451592e-06,
      "loss": 0.4499,
      "step": 4730
    },
    {
      "epoch": 0.9624854053505254,
      "grad_norm": 0.3171781599521637,
      "learning_rate": 8.485669149176259e-06,
      "loss": 0.4606,
      "step": 4740
    },
    {
      "epoch": 0.9645159652774252,
      "grad_norm": 0.3069824278354645,
      "learning_rate": 8.034303768900926e-06,
      "loss": 0.4485,
      "step": 4750
    },
    {
      "epoch": 0.966546525204325,
      "grad_norm": 0.3387303948402405,
      "learning_rate": 7.582938388625594e-06,
      "loss": 0.4357,
      "step": 4760
    },
    {
      "epoch": 0.968577085131225,
      "grad_norm": 0.36905092000961304,
      "learning_rate": 7.131573008350261e-06,
      "loss": 0.4872,
      "step": 4770
    },
    {
      "epoch": 0.9706076450581248,
      "grad_norm": 0.2986646592617035,
      "learning_rate": 6.680207628074928e-06,
      "loss": 0.4592,
      "step": 4780
    },
    {
      "epoch": 0.9726382049850246,
      "grad_norm": 0.27387574315071106,
      "learning_rate": 6.228842247799594e-06,
      "loss": 0.4333,
      "step": 4790
    },
    {
      "epoch": 0.9746687649119244,
      "grad_norm": 0.2653948962688446,
      "learning_rate": 5.777476867524261e-06,
      "loss": 0.4342,
      "step": 4800
    },
    {
      "epoch": 0.9766993248388243,
      "grad_norm": 0.2777888774871826,
      "learning_rate": 5.326111487248928e-06,
      "loss": 0.4391,
      "step": 4810
    },
    {
      "epoch": 0.9787298847657242,
      "grad_norm": 0.318057119846344,
      "learning_rate": 4.874746106973595e-06,
      "loss": 0.3987,
      "step": 4820
    },
    {
      "epoch": 0.980760444692624,
      "grad_norm": 0.3141552209854126,
      "learning_rate": 4.423380726698262e-06,
      "loss": 0.4851,
      "step": 4830
    },
    {
      "epoch": 0.9827910046195238,
      "grad_norm": 0.305196613073349,
      "learning_rate": 3.972015346422929e-06,
      "loss": 0.4626,
      "step": 4840
    },
    {
      "epoch": 0.9848215645464237,
      "grad_norm": 0.2667790949344635,
      "learning_rate": 3.5206499661475968e-06,
      "loss": 0.4046,
      "step": 4850
    },
    {
      "epoch": 0.9868521244733235,
      "grad_norm": 0.30167654156684875,
      "learning_rate": 3.069284585872264e-06,
      "loss": 0.4124,
      "step": 4860
    },
    {
      "epoch": 0.9888826844002233,
      "grad_norm": 0.3127462565898895,
      "learning_rate": 2.617919205596931e-06,
      "loss": 0.4426,
      "step": 4870
    },
    {
      "epoch": 0.9909132443271232,
      "grad_norm": 0.31509798765182495,
      "learning_rate": 2.166553825321598e-06,
      "loss": 0.425,
      "step": 4880
    },
    {
      "epoch": 0.9929438042540231,
      "grad_norm": 0.32562169432640076,
      "learning_rate": 1.715188445046265e-06,
      "loss": 0.4469,
      "step": 4890
    },
    {
      "epoch": 0.9949743641809229,
      "grad_norm": 0.30297955870628357,
      "learning_rate": 1.263823064770932e-06,
      "loss": 0.4097,
      "step": 4900
    },
    {
      "epoch": 0.9970049241078227,
      "grad_norm": 0.3271823823451996,
      "learning_rate": 8.124576844955993e-07,
      "loss": 0.443,
      "step": 4910
    },
    {
      "epoch": 0.9990354840347225,
      "grad_norm": 0.2992851734161377,
      "learning_rate": 3.610923042202663e-07,
      "loss": 0.3932,
      "step": 4920
    }
  ],
  "logging_steps": 10,
  "max_steps": 4924,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.715243835568947e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
