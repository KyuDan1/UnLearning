import torch
import numpy as np
import os
import json
from scipy.linalg import svd
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
from peft import PeftModel, LoraConfig, TaskType, get_peft_model

import numpy as np

class LoRAUnlearner:
    def __init__(self, alpha=0.5, beta=1.0, lambda_reg=0.1):
        self.alpha = alpha  # Adjustment coefficient for common weights
        self.beta = beta    # Unlearning strength hyperparameter
        self.lambda_reg = lambda_reg  # Regularization coefficient

    def svd_decompose(self, W):
        """Perform Singular Value Decomposition (SVD) on a weight matrix."""
        U, Sigma, Vt = np.linalg.svd(W, full_matrices=False)
        return U, np.diag(Sigma), Vt

    def extract_common_weights(self, W_plus, W_minus):
        """Extract common components between W+ and W- using shared subspace."""
        
        # Compute the common subspace basis
        W_cross = np.dot(W_plus, W_minus.T)
        U_c, _, _ = self.svd_decompose(W_cross)

        # Extract common weights
        W_common = np.dot(U_c, np.dot(U_c.T, (W_plus + W_minus) / 2))
        return W_common

    def remove_common_weights(self, W_minus, W_common):
        """Remove common components from W- using projection."""
        projection = np.dot(W_common, np.linalg.inv(np.dot(W_common.T, W_common)))
        projection = np.dot(projection, np.dot(W_common.T, W_minus))
        W_minus_pure = W_minus - projection
        return W_minus_pure

    def adjust_weights(self, W_minus_pure, fisher_matrix):
        """Adjust weights using Fisher information matrix."""
        fisher_inv = np.linalg.inv(fisher_matrix)
        W_minus_adjusted = np.multiply(fisher_inv, W_minus_pure - self.alpha * W_minus_pure)
        return W_minus_adjusted

    def unlearn_weights(self, W_plus, W_minus):
        """Perform the final unlearning process."""
        W_common = self.extract_common_weights(W_plus, W_minus)
        W_minus_pure = self.remove_common_weights(W_minus, W_common)
        

        # Final unlearning step
        W_unlearned = W_plus - self.beta * np.sign(np.multiply(W_plus, W_minus_pure)) * np.abs(W_minus_pure)
        W_unlearned = np.clip(W_unlearned, -1, 1)  # -1ê³¼ 1 ì‚¬ì´ë¡œ í´ë¦¬í•‘
        W_unlearned = W_unlearned - self.lambda_reg * W_unlearned
        W_unlearned = 0.9 * W_unlearned + 0.1 * W_plus


        return W_unlearned


#############################################
# 2. W_newë¥¼ LoRAì˜ ë‘ í–‰ë ¬(lora_B, lora_A)ë¡œ ë¶„í•´í•˜ëŠ” í•¨ìˆ˜
#############################################
def factorize_weight(W_new: np.ndarray, r: int, scaling: float):
    """
    W_new(íš¨ê³¼ì ì¸ LoRA ì—…ë°ì´íŠ¸)ë¥¼ lora_Bì™€ lora_Aë¡œ ë¶„í•´í•©ë‹ˆë‹¤.
    LoRA ì—…ë°ì´íŠ¸ëŠ” ì›ë˜ (lora_B @ lora_A) * scaling í˜•íƒœë¡œ ì ìš©ë˜ë¯€ë¡œ,
    lora_B @ lora_A = W_new / scalingê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    
    SVDë¥¼ í†µí•´ M = W_new/scaling = U S V^T ë¡œ ë¶„í•´í•œ í›„,
    lora_B = U * sqrt(S)   (shape: [out_features, r])
    lora_A = sqrt(S) * V^T   (shape: [r, in_features])
    
    Args:
        W_new (np.ndarray): ê²°í•©ëœ effective weight update (out_features x in_features)
        r (int): LoRAì˜ rank (ì˜ˆì œì—ì„œëŠ” 4)
        scaling (float): lora_alpha / r (ì˜ˆ: 32/4 = 8)
        
    Returns:
        lora_B, lora_A: torch.Tensorë¡œ ë³€í™˜ëœ ë¶„í•´ ê²°ê³¼.
    """
    M = W_new / scaling  # (lora_B @ lora_A = M)
    U, S, Vh = np.linalg.svd(M, full_matrices=False)
    
    U_r = U[:, :r]      # (out_features x r)
    S_r = S[:r]         # (r,)
    Vh_r = Vh[:r, :]    # (r x in_features)
    
    sqrt_S = np.sqrt(S_r)
    lora_B = U_r * sqrt_S[np.newaxis, :]   # broadcasting, shape: (out_features x r)
    lora_A = sqrt_S[:, np.newaxis] * Vh_r    # shape: (r x in_features)
    
    # torch tensorë¡œ ë³€í™˜ (dtypeì€ ëª¨ë¸ê³¼ ì¼ì¹˜í•˜ë„ë¡)
    lora_B = torch.tensor(lora_B, dtype=torch.float16)
    lora_A = torch.tensor(lora_A, dtype=torch.float16)
    return lora_B, lora_A

#############################################
# 3. Qwen ëª¨ë¸ ë¡œë“œ ë° LoRA ì ìš© (PEFT ë°©ì‹)
#############################################
model_name = "Qwen/Qwen2.5-0.5B"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto",
    #attn_implementation="eager"

)
model.config.sliding_window = None

# LoRA ì„¤ì • (ì›ë˜ íŒŒì¸íŠœë‹ì— ì‚¬ìš©í–ˆë˜ target module ëª©ë¡)
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=4,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=[
        "self_attn.q_proj",
        "self_attn.k_proj", 
        "self_attn.v_proj",
        "self_attn.o_proj",
        "mlp.gate_proj",
        "mlp.up_proj", 
        "mlp.down_proj"
    ]
)

# PEFTë¥¼ í†µí•´ ëª¨ë¸ì— LoRA ì–´ëŒ‘í„° ì¶”ê°€
peft_model = get_peft_model(model, lora_config)
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, LoraConfig, get_peft_model

path_plus = "./qwen-0.5b-lora-finetuned-alpaca-gpt4"   # W+
path_minus = "./qwen-0.5b-lora-finetuned-toxic"         # W-

# ì›ë³¸ Qwen ëª¨ë¸ ë¡œë“œ
model_name = "Qwen/Qwen2.5-0.5B"
base_model = AutoModelForCausalLM.from_pretrained(
    model_name, trust_remote_code=True, torch_dtype=torch.float16, device_map="auto"
)

# Fine-tuned LoRA ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model_plus = PeftModel.from_pretrained(base_model, path_plus)
model_minus = PeftModel.from_pretrained(base_model, path_minus)

# ëª¨ë¸ì˜ state_dict ê°€ì ¸ì˜¤ê¸°
state_dict_plus = model_plus.state_dict()
state_dict_minus = model_minus.state_dict()
np.random.seed(42)
# ğŸŸ¢ LoRA Target Modules
target_modules = [
    "self_attn.q_proj",
    "self_attn.k_proj", 
    "self_attn.v_proj",
    "self_attn.o_proj",
    "mlp.gate_proj",
    "mlp.up_proj", 
    "mlp.down_proj"
]

# LoRA ì„¤ì • (ìƒˆë¡œìš´ ëª¨ë¸ì— ì ìš©)
lora_config = LoraConfig(
    task_type="CAUSAL_LM",
    r=4,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=target_modules
)

# ìƒˆë¡œìš´ PEFT ëª¨ë¸ ìƒì„±
new_peft_model = get_peft_model(base_model, lora_config)
new_state_dict = new_peft_model.state_dict()

# LoRA scaling factor
scaling = lora_config.lora_alpha / lora_config.r  # ì˜ˆ: 32/4 = 8

for layer_idx in range(24):  # Qwen-0.5BëŠ” 24ê°œì˜ Transformer layerë¥¼ ê°€ì§
    for target_module in target_modules:
        # LoRA weight í‚¤ ìƒì„± (ê° ë ˆì´ì–´ì— ëŒ€í•´)
        key_A = f"base_model.model.model.layers.{layer_idx}.{target_module}.lora_A.default.weight"
        key_B = f"base_model.model.model.layers.{layer_idx}.{target_module}.lora_B.default.weight"

        # í‚¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸ ìˆ˜í–‰
        if key_A in state_dict_plus and key_B in state_dict_plus:
            print(f"bringing W+ and W-{layer_idx}")
            # ê¸°ì¡´ W+ì™€ W- ë¶ˆëŸ¬ì˜¤ê¸° (torch.Tensor â†’ numpy ë³€í™˜)
            W_plus = (state_dict_plus[key_B] @ state_dict_plus[key_A]).cpu().numpy()
            W_minus = (state_dict_minus[key_B] @ state_dict_minus[key_A]).cpu().numpy()
            print("combining lora weights")
            # W_new ìƒì„±
            # rank_common < LoRA rank
            unlearner = LoRAUnlearner(alpha=0.5, beta=0.5, lambda_reg=0.1)
            W_new = unlearner.unlearn_weights(W_plus, W_minus)

            delta = np.linalg.norm(W_new - W_plus) / np.linalg.norm(W_plus)
            print(f"Layer {layer_idx} Î”W: {delta:.2%}")
            # W_newë¥¼ lora_A, lora_Bë¡œ ë³µêµ¬ (í•¨ìˆ˜ í™•ì¸í•´ì•¼ë¨.)
            lora_B, lora_A = factorize_weight(W_new, r=lora_config.r, scaling=scaling)

            # ìƒˆë¡œìš´ ëª¨ë¸ì˜ LoRA weight ì—…ë°ì´íŠ¸ (torch.Tensor í˜•íƒœë¡œ ë³€í™˜) (ìŒ ì§„ì§œ?)
            with torch.no_grad():
                new_state_dict[key_A].copy_(lora_A.to(new_state_dict[key_A].dtype))
                new_state_dict[key_B].copy_(lora_B.to(new_state_dict[key_B].dtype))
                
new_peft_model.load_state_dict(new_state_dict)



save_path = "./qwen-0.5b-unlearned-lora"
new_peft_model.save_pretrained(save_path)
print(f"ìƒˆë¡œìš´ ê²°í•©ëœ LoRA ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")
